{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KyrWiBM6LAAZ"
   },
   "source": [
    "## 1.线性模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFP_wK-TLH-_"
   },
   "source": [
    "最基本的线性模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "2cmevDmtK2by"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "2iYQ-XzbLP-f"
   },
   "outputs": [],
   "source": [
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "eFnvqzkULcq_"
   },
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "  return x * w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ZVjKqaR4LhjB"
   },
   "outputs": [],
   "source": [
    "def loss(x, y):\n",
    "  y_pred = forward(x)\n",
    "  return (y_pred - y)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "lnKzFXNWLyrT"
   },
   "outputs": [],
   "source": [
    "w_list = []\n",
    "mse_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "id": "ab76uBWwL5Np",
    "outputId": "fbe38f2e-425c-41fe-e6d4-f2e54145e23f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w= 0.0\n",
      "\t 1.0 2.0 0.0 4.0\n",
      "\t 2.0 4.0 0.0 16.0\n",
      "\t 3.0 6.0 0.0 36.0\n",
      "MSE= 18.666666666666668\n",
      "w= 0.1\n",
      "\t 1.0 2.0 0.1 3.61\n",
      "\t 2.0 4.0 0.2 14.44\n",
      "\t 3.0 6.0 0.30000000000000004 32.49\n",
      "MSE= 16.846666666666668\n",
      "w= 0.2\n",
      "\t 1.0 2.0 0.2 3.24\n",
      "\t 2.0 4.0 0.4 12.96\n",
      "\t 3.0 6.0 0.6000000000000001 29.160000000000004\n",
      "MSE= 15.120000000000003\n",
      "w= 0.30000000000000004\n",
      "\t 1.0 2.0 0.30000000000000004 2.8899999999999997\n",
      "\t 2.0 4.0 0.6000000000000001 11.559999999999999\n",
      "\t 3.0 6.0 0.9000000000000001 26.009999999999998\n",
      "MSE= 13.486666666666665\n",
      "w= 0.4\n",
      "\t 1.0 2.0 0.4 2.5600000000000005\n",
      "\t 2.0 4.0 0.8 10.240000000000002\n",
      "\t 3.0 6.0 1.2000000000000002 23.04\n",
      "MSE= 11.946666666666667\n",
      "w= 0.5\n",
      "\t 1.0 2.0 0.5 2.25\n",
      "\t 2.0 4.0 1.0 9.0\n",
      "\t 3.0 6.0 1.5 20.25\n",
      "MSE= 10.5\n",
      "w= 0.6000000000000001\n",
      "\t 1.0 2.0 0.6000000000000001 1.9599999999999997\n",
      "\t 2.0 4.0 1.2000000000000002 7.839999999999999\n",
      "\t 3.0 6.0 1.8000000000000003 17.639999999999993\n",
      "MSE= 9.146666666666663\n",
      "w= 0.7000000000000001\n",
      "\t 1.0 2.0 0.7000000000000001 1.6899999999999995\n",
      "\t 2.0 4.0 1.4000000000000001 6.759999999999998\n",
      "\t 3.0 6.0 2.1 15.209999999999999\n",
      "MSE= 7.886666666666666\n",
      "w= 0.8\n",
      "\t 1.0 2.0 0.8 1.44\n",
      "\t 2.0 4.0 1.6 5.76\n",
      "\t 3.0 6.0 2.4000000000000004 12.959999999999997\n",
      "MSE= 6.719999999999999\n",
      "w= 0.9\n",
      "\t 1.0 2.0 0.9 1.2100000000000002\n",
      "\t 2.0 4.0 1.8 4.840000000000001\n",
      "\t 3.0 6.0 2.7 10.889999999999999\n",
      "MSE= 5.646666666666666\n",
      "w= 1.0\n",
      "\t 1.0 2.0 1.0 1.0\n",
      "\t 2.0 4.0 2.0 4.0\n",
      "\t 3.0 6.0 3.0 9.0\n",
      "MSE= 4.666666666666667\n",
      "w= 1.1\n",
      "\t 1.0 2.0 1.1 0.8099999999999998\n",
      "\t 2.0 4.0 2.2 3.2399999999999993\n",
      "\t 3.0 6.0 3.3000000000000003 7.289999999999998\n",
      "MSE= 3.779999999999999\n",
      "w= 1.2000000000000002\n",
      "\t 1.0 2.0 1.2000000000000002 0.6399999999999997\n",
      "\t 2.0 4.0 2.4000000000000004 2.5599999999999987\n",
      "\t 3.0 6.0 3.6000000000000005 5.759999999999997\n",
      "MSE= 2.986666666666665\n",
      "w= 1.3\n",
      "\t 1.0 2.0 1.3 0.48999999999999994\n",
      "\t 2.0 4.0 2.6 1.9599999999999997\n",
      "\t 3.0 6.0 3.9000000000000004 4.409999999999998\n",
      "MSE= 2.2866666666666657\n",
      "w= 1.4000000000000001\n",
      "\t 1.0 2.0 1.4000000000000001 0.3599999999999998\n",
      "\t 2.0 4.0 2.8000000000000003 1.4399999999999993\n",
      "\t 3.0 6.0 4.2 3.2399999999999993\n",
      "MSE= 1.6799999999999995\n",
      "w= 1.5\n",
      "\t 1.0 2.0 1.5 0.25\n",
      "\t 2.0 4.0 3.0 1.0\n",
      "\t 3.0 6.0 4.5 2.25\n",
      "MSE= 1.1666666666666667\n",
      "w= 1.6\n",
      "\t 1.0 2.0 1.6 0.15999999999999992\n",
      "\t 2.0 4.0 3.2 0.6399999999999997\n",
      "\t 3.0 6.0 4.800000000000001 1.4399999999999984\n",
      "MSE= 0.746666666666666\n",
      "w= 1.7000000000000002\n",
      "\t 1.0 2.0 1.7000000000000002 0.0899999999999999\n",
      "\t 2.0 4.0 3.4000000000000004 0.3599999999999996\n",
      "\t 3.0 6.0 5.1000000000000005 0.809999999999999\n",
      "MSE= 0.4199999999999995\n",
      "w= 1.8\n",
      "\t 1.0 2.0 1.8 0.03999999999999998\n",
      "\t 2.0 4.0 3.6 0.15999999999999992\n",
      "\t 3.0 6.0 5.4 0.3599999999999996\n",
      "MSE= 0.1866666666666665\n",
      "w= 1.9000000000000001\n",
      "\t 1.0 2.0 1.9000000000000001 0.009999999999999974\n",
      "\t 2.0 4.0 3.8000000000000003 0.0399999999999999\n",
      "\t 3.0 6.0 5.7 0.0899999999999999\n",
      "MSE= 0.046666666666666586\n",
      "w= 2.0\n",
      "\t 1.0 2.0 2.0 0.0\n",
      "\t 2.0 4.0 4.0 0.0\n",
      "\t 3.0 6.0 6.0 0.0\n",
      "MSE= 0.0\n",
      "w= 2.1\n",
      "\t 1.0 2.0 2.1 0.010000000000000018\n",
      "\t 2.0 4.0 4.2 0.04000000000000007\n",
      "\t 3.0 6.0 6.300000000000001 0.09000000000000043\n",
      "MSE= 0.046666666666666835\n",
      "w= 2.2\n",
      "\t 1.0 2.0 2.2 0.04000000000000007\n",
      "\t 2.0 4.0 4.4 0.16000000000000028\n",
      "\t 3.0 6.0 6.6000000000000005 0.36000000000000065\n",
      "MSE= 0.18666666666666698\n",
      "w= 2.3000000000000003\n",
      "\t 1.0 2.0 2.3000000000000003 0.09000000000000016\n",
      "\t 2.0 4.0 4.6000000000000005 0.36000000000000065\n",
      "\t 3.0 6.0 6.9 0.8100000000000006\n",
      "MSE= 0.42000000000000054\n",
      "w= 2.4000000000000004\n",
      "\t 1.0 2.0 2.4000000000000004 0.16000000000000028\n",
      "\t 2.0 4.0 4.800000000000001 0.6400000000000011\n",
      "\t 3.0 6.0 7.200000000000001 1.4400000000000026\n",
      "MSE= 0.7466666666666679\n",
      "w= 2.5\n",
      "\t 1.0 2.0 2.5 0.25\n",
      "\t 2.0 4.0 5.0 1.0\n",
      "\t 3.0 6.0 7.5 2.25\n",
      "MSE= 1.1666666666666667\n",
      "w= 2.6\n",
      "\t 1.0 2.0 2.6 0.3600000000000001\n",
      "\t 2.0 4.0 5.2 1.4400000000000004\n",
      "\t 3.0 6.0 7.800000000000001 3.2400000000000024\n",
      "MSE= 1.6800000000000008\n",
      "w= 2.7\n",
      "\t 1.0 2.0 2.7 0.49000000000000027\n",
      "\t 2.0 4.0 5.4 1.960000000000001\n",
      "\t 3.0 6.0 8.100000000000001 4.410000000000006\n",
      "MSE= 2.2866666666666693\n",
      "w= 2.8000000000000003\n",
      "\t 1.0 2.0 2.8000000000000003 0.6400000000000005\n",
      "\t 2.0 4.0 5.6000000000000005 2.560000000000002\n",
      "\t 3.0 6.0 8.4 5.760000000000002\n",
      "MSE= 2.986666666666668\n",
      "w= 2.9000000000000004\n",
      "\t 1.0 2.0 2.9000000000000004 0.8100000000000006\n",
      "\t 2.0 4.0 5.800000000000001 3.2400000000000024\n",
      "\t 3.0 6.0 8.700000000000001 7.290000000000005\n",
      "MSE= 3.780000000000003\n",
      "w= 3.0\n",
      "\t 1.0 2.0 3.0 1.0\n",
      "\t 2.0 4.0 6.0 4.0\n",
      "\t 3.0 6.0 9.0 9.0\n",
      "MSE= 4.666666666666667\n",
      "w= 3.1\n",
      "\t 1.0 2.0 3.1 1.2100000000000002\n",
      "\t 2.0 4.0 6.2 4.840000000000001\n",
      "\t 3.0 6.0 9.3 10.890000000000004\n",
      "MSE= 5.646666666666668\n",
      "w= 3.2\n",
      "\t 1.0 2.0 3.2 1.4400000000000004\n",
      "\t 2.0 4.0 6.4 5.760000000000002\n",
      "\t 3.0 6.0 9.600000000000001 12.96000000000001\n",
      "MSE= 6.720000000000003\n",
      "w= 3.3000000000000003\n",
      "\t 1.0 2.0 3.3000000000000003 1.6900000000000006\n",
      "\t 2.0 4.0 6.6000000000000005 6.7600000000000025\n",
      "\t 3.0 6.0 9.9 15.210000000000003\n",
      "MSE= 7.886666666666668\n",
      "w= 3.4000000000000004\n",
      "\t 1.0 2.0 3.4000000000000004 1.960000000000001\n",
      "\t 2.0 4.0 6.800000000000001 7.840000000000004\n",
      "\t 3.0 6.0 10.200000000000001 17.640000000000008\n",
      "MSE= 9.14666666666667\n",
      "w= 3.5\n",
      "\t 1.0 2.0 3.5 2.25\n",
      "\t 2.0 4.0 7.0 9.0\n",
      "\t 3.0 6.0 10.5 20.25\n",
      "MSE= 10.5\n",
      "w= 3.6\n",
      "\t 1.0 2.0 3.6 2.5600000000000005\n",
      "\t 2.0 4.0 7.2 10.240000000000002\n",
      "\t 3.0 6.0 10.8 23.040000000000006\n",
      "MSE= 11.94666666666667\n",
      "w= 3.7\n",
      "\t 1.0 2.0 3.7 2.8900000000000006\n",
      "\t 2.0 4.0 7.4 11.560000000000002\n",
      "\t 3.0 6.0 11.100000000000001 26.010000000000016\n",
      "MSE= 13.486666666666673\n",
      "w= 3.8000000000000003\n",
      "\t 1.0 2.0 3.8000000000000003 3.240000000000001\n",
      "\t 2.0 4.0 7.6000000000000005 12.960000000000004\n",
      "\t 3.0 6.0 11.4 29.160000000000004\n",
      "MSE= 15.120000000000005\n",
      "w= 3.9000000000000004\n",
      "\t 1.0 2.0 3.9000000000000004 3.610000000000001\n",
      "\t 2.0 4.0 7.800000000000001 14.440000000000005\n",
      "\t 3.0 6.0 11.700000000000001 32.49000000000001\n",
      "MSE= 16.84666666666667\n",
      "w= 4.0\n",
      "\t 1.0 2.0 4.0 4.0\n",
      "\t 2.0 4.0 8.0 16.0\n",
      "\t 3.0 6.0 12.0 36.0\n",
      "MSE= 18.666666666666668\n"
     ]
    }
   ],
   "source": [
    "for w in np.arange(0.0, 4.1, 0.1):\n",
    "  print(\"w=\", w)\n",
    "  l_sum = 0\n",
    "  for x_val, y_val in zip(x_data, y_data):\n",
    "    y_pred_val = forward(x_val)\n",
    "    loss_val = loss(x_val, y_val)\n",
    "    l_sum += loss_val\n",
    "    print('\\t', x_val, y_val, y_pred_val, loss_val)\n",
    "  print(\"MSE=\", l_sum / 3)\n",
    "  w_list.append(w)\n",
    "  mse_list.append(l_sum / 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "SW8MNK0tL-qW"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(w_list, mse_list)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('w')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作业：y = x * w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# y = 2x + 3.2\n",
    "\"\"\"\n",
    "x 1 2 3 4 \n",
    "y 5 7 9 11\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "x_data = [1.0, 2.0, 3.0, 4.0]\n",
    "y_data = [5.2, 7.2, 9.2, 11.2]\n",
    "\n",
    "def forward(x):\n",
    "    y_hat = x * w + b\n",
    "    return y_hat\n",
    "\n",
    "def loss(x, y):\n",
    "  y_pred = forward(x)\n",
    "  return (y_pred - y)**2\n",
    "\n",
    "mse_list = []\n",
    "w = np.arange(0.0,4.1,0.1)\n",
    "b = np.arange(0.0,4.1,0.1)\n",
    "[w,b]=np.meshgrid(w,b)\n",
    "\n",
    "l_sum = 0\n",
    "for x_val, y_val in zip(x_data, y_data):\n",
    "    y_pred_val = forward(x_val)\n",
    "    #print(y_pred_val)\n",
    "    loss_val = loss(x_val, y_val)\n",
    "    l_sum += loss_val\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "ax.plot_surface(w, b, l_sum/3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.梯度下降算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict (before training) 4 4.0\n",
      "Epoch: 0 w =  1.0066666666666666 loss =  0.3333333333333333\n",
      "Epoch: 1 w =  1.013288888888889 loss =  0.32890370370370375\n",
      "Epoch: 2 w =  1.019866962962963 loss =  0.32453293893004115\n",
      "Epoch: 3 w =  1.0264011832098767 loss =  0.3202202567638152\n",
      "Epoch: 4 w =  1.0328918419884776 loss =  0.31596488535170936\n",
      "Epoch: 5 w =  1.0393392297085544 loss =  0.31176606309747995\n",
      "Epoch: 6 w =  1.0457436348438307 loss =  0.3076230385256512\n",
      "Epoch: 7 w =  1.0521053439448718 loss =  0.30353507014702147\n",
      "Epoch: 8 w =  1.0584246416519059 loss =  0.2995014263259566\n",
      "Epoch: 9 w =  1.0647018107075599 loss =  0.2955213851494473\n",
      "Epoch: 10 w =  1.0709371319695096 loss =  0.2915942342979057\n",
      "Epoch: 11 w =  1.0771308844230463 loss =  0.2877192709176802\n",
      "Epoch: 12 w =  1.0832833451935593 loss =  0.2838958014952629\n",
      "Epoch: 13 w =  1.0893947895589355 loss =  0.2801231417331703\n",
      "Epoch: 14 w =  1.095465490961876 loss =  0.27640061642747177\n",
      "Epoch: 15 w =  1.1014957210221301 loss =  0.2727275593469467\n",
      "Epoch: 16 w =  1.1074857495486492 loss =  0.2691033131138473\n",
      "Epoch: 17 w =  1.1134358445516581 loss =  0.26552722908624554\n",
      "Epoch: 18 w =  1.119346272254647 loss =  0.2619986672419439\n",
      "Epoch: 19 w =  1.1252172971062826 loss =  0.2585169960639288\n",
      "Epoch: 20 w =  1.1310491817922408 loss =  0.25508159242734596\n",
      "Epoch: 21 w =  1.1368421872469592 loss =  0.2516918414879781\n",
      "Epoch: 22 w =  1.142596572665313 loss =  0.2483471365722045\n",
      "Epoch: 23 w =  1.1483125955142108 loss =  0.2450468790684227\n",
      "Epoch: 24 w =  1.153990511544116 loss =  0.2417904783199134\n",
      "Epoch: 25 w =  1.1596305748004885 loss =  0.23857735151912884\n",
      "Epoch: 26 w =  1.1652330376351518 loss =  0.2354069236033858\n",
      "Epoch: 27 w =  1.1707981507175842 loss =  0.23227862715194528\n",
      "Epoch: 28 w =  1.1763261630461337 loss =  0.2291919022844594\n",
      "Epoch: 29 w =  1.1818173219591595 loss =  0.2261461965607681\n",
      "Epoch: 30 w =  1.1872718731460983 loss =  0.22314096488202725\n",
      "Epoch: 31 w =  1.1926900606584576 loss =  0.22017566939315059\n",
      "Epoch: 32 w =  1.1980721269207346 loss =  0.2172497793865483\n",
      "Epoch: 33 w =  1.203418312741263 loss =  0.21436277120714478\n",
      "Epoch: 34 w =  1.208728857322988 loss =  0.21151412815865878\n",
      "Epoch: 35 w =  1.214003998274168 loss =  0.20870334041112812\n",
      "Epoch: 36 w =  1.2192439716190069 loss =  0.20592990490966476\n",
      "Epoch: 37 w =  1.2244490118082134 loss =  0.20319332528442072\n",
      "Epoch: 38 w =  1.229619351729492 loss =  0.20049311176175222\n",
      "Epoch: 39 w =  1.2347552227179621 loss =  0.19782878107656274\n",
      "Epoch: 40 w =  1.239856854566509 loss =  0.19519985638581192\n",
      "Epoch: 41 w =  1.2449244755360656 loss =  0.19260586718317385\n",
      "Epoch: 42 w =  1.249958312365825 loss =  0.19004634921482855\n",
      "Epoch: 43 w =  1.2549585902833862 loss =  0.18752084439637373\n",
      "Epoch: 44 w =  1.2599255330148302 loss =  0.18502890073083975\n",
      "Epoch: 45 w =  1.2648593627947313 loss =  0.1825700722277944\n",
      "Epoch: 46 w =  1.2697603003760998 loss =  0.18014391882352285\n",
      "Epoch: 47 w =  1.2746285650402591 loss =  0.177750006302268\n",
      "Epoch: 48 w =  1.2794643746066574 loss =  0.17538790621851785\n",
      "Epoch: 49 w =  1.284267945442613 loss =  0.1730571958203251\n",
      "Epoch: 50 w =  1.2890394924729955 loss =  0.17075745797364617\n",
      "Epoch: 51 w =  1.2937792291898422 loss =  0.16848828108768524\n",
      "Epoch: 52 w =  1.29848736766191 loss =  0.16624925904123114\n",
      "Epoch: 53 w =  1.3031641185441638 loss =  0.16403999110997208\n",
      "Epoch: 54 w =  1.3078096910872028 loss =  0.16186008189477738\n",
      "Epoch: 55 w =  1.3124242931466215 loss =  0.1597091412509312\n",
      "Epoch: 56 w =  1.3170081311923107 loss =  0.1575867842183077\n",
      "Epoch: 57 w =  1.3215614103176954 loss =  0.15549263095247326\n",
      "Epoch: 58 w =  1.3260843342489108 loss =  0.15342630665670484\n",
      "Epoch: 59 w =  1.3305771053539182 loss =  0.15138744151491126\n",
      "Epoch: 60 w =  1.3350399246515587 loss =  0.1493756706254464\n",
      "Epoch: 61 w =  1.3394729918205484 loss =  0.14739063393580157\n",
      "Epoch: 62 w =  1.3438765052084114 loss =  0.1454319761781658\n",
      "Epoch: 63 w =  1.3482506618403554 loss =  0.14349934680584261\n",
      "Epoch: 64 w =  1.3525956574280864 loss =  0.14159239993051162\n",
      "Epoch: 65 w =  1.3569116863785657 loss =  0.13971079426032387\n",
      "Epoch: 66 w =  1.3611989418027086 loss =  0.13785419303882007\n",
      "Epoch: 67 w =  1.365457615524024 loss =  0.13602226398465975\n",
      "Epoch: 68 w =  1.3696878980871972 loss =  0.1342146792321525\n",
      "Epoch: 69 w =  1.373889978766616 loss =  0.13243111527257853\n",
      "Epoch: 70 w =  1.3780640455748385 loss =  0.13067125289628956\n",
      "Epoch: 71 w =  1.3822102852710063 loss =  0.12893477713557885\n",
      "Epoch: 72 w =  1.3863288833691996 loss =  0.12722137720831048\n",
      "Epoch: 73 w =  1.3904200241467382 loss =  0.1255307464622978\n",
      "Epoch: 74 w =  1.3944838906524266 loss =  0.12386258232042109\n",
      "Epoch: 75 w =  1.3985206647147437 loss =  0.12221658622647415\n",
      "Epoch: 76 w =  1.4025305269499788 loss =  0.12059246359173126\n",
      "Epoch: 77 w =  1.4065136567703123 loss =  0.11898992374222334\n",
      "Epoch: 78 w =  1.4104702323918434 loss =  0.11740867986671556\n",
      "Epoch: 79 w =  1.4144004308425644 loss =  0.1158484489653757\n",
      "Epoch: 80 w =  1.4183044279702806 loss =  0.11430895179912474\n",
      "Epoch: 81 w =  1.4221823984504787 loss =  0.11278991283966083\n",
      "Epoch: 82 w =  1.426034515794142 loss =  0.11129106022014713\n",
      "Epoch: 83 w =  1.4298609523555146 loss =  0.10981212568655498\n",
      "Epoch: 84 w =  1.433661879339811 loss =  0.1083528445496536\n",
      "Epoch: 85 w =  1.437437466810879 loss =  0.10691295563763825\n",
      "Epoch: 86 w =  1.4411878836988063 loss =  0.10549220124938698\n",
      "Epoch: 87 w =  1.444913297807481 loss =  0.1040903271083396\n",
      "Epoch: 88 w =  1.4486138758220979 loss =  0.10270708231698876\n",
      "Epoch: 89 w =  1.4522897833166173 loss =  0.10134221931197629\n",
      "Epoch: 90 w =  1.4559411847611732 loss =  0.09999549381978601\n",
      "Epoch: 91 w =  1.459568243529432 loss =  0.09866666481302529\n",
      "Epoch: 92 w =  1.4631711219059025 loss =  0.0973554944672878\n",
      "Epoch: 93 w =  1.4667499810931965 loss =  0.09606174811858914\n",
      "Epoch: 94 w =  1.470304981219242 loss =  0.09478519422136876\n",
      "Epoch: 95 w =  1.473836281344447 loss =  0.09352560430704922\n",
      "Epoch: 96 w =  1.4773440394688173 loss =  0.09228275294314664\n",
      "Epoch: 97 w =  1.4808284125390252 loss =  0.0910564176929244\n",
      "Epoch: 98 w =  1.4842895564554317 loss =  0.08984637907558286\n",
      "Epoch: 99 w =  1.4877276260790622 loss =  0.08865242052697846\n",
      "Epoch: 100 w =  1.4911427752385351 loss =  0.08747432836086438\n",
      "Epoch: 101 w =  1.494535156736945 loss =  0.08631189173064667\n",
      "Epoch: 102 w =  1.4979049223586987 loss =  0.08516490259164827\n",
      "Epoch: 103 w =  1.5012522228763072 loss =  0.08403315566387481\n",
      "Epoch: 104 w =  1.5045772080571318 loss =  0.08291644839527491\n",
      "Epoch: 105 w =  1.5078800266700843 loss =  0.08181458092548882\n",
      "Epoch: 106 w =  1.5111608264922838 loss =  0.080727356050079\n",
      "Epoch: 107 w =  1.5144197543156686 loss =  0.07965457918523569\n",
      "Epoch: 108 w =  1.5176569559535642 loss =  0.0785960583329519\n",
      "Epoch: 109 w =  1.520872576247207 loss =  0.07755160404666063\n",
      "Epoch: 110 w =  1.5240667590722257 loss =  0.07652102939732947\n",
      "Epoch: 111 w =  1.5272396473450776 loss =  0.07550414994000497\n",
      "Epoch: 112 w =  1.5303913830294438 loss =  0.07450078368080221\n",
      "Epoch: 113 w =  1.5335221071425809 loss =  0.07351075104433287\n",
      "Epoch: 114 w =  1.5366319597616303 loss =  0.07253387484156594\n",
      "Epoch: 115 w =  1.539721080029886 loss =  0.07156998023811582\n",
      "Epoch: 116 w =  1.54278960616302 loss =  0.07061889472295156\n",
      "Epoch: 117 w =  1.5458376754552665 loss =  0.06968044807752213\n",
      "Epoch: 118 w =  1.5488654242855648 loss =  0.06875447234529194\n",
      "Epoch: 119 w =  1.551872988123661 loss =  0.06784080180168114\n",
      "Epoch: 120 w =  1.55486050153617 loss =  0.06693927292440548\n",
      "Epoch: 121 w =  1.5578280981925956 loss =  0.06604972436421001\n",
      "Epoch: 122 w =  1.5607759108713117 loss =  0.06517199691599229\n",
      "Epoch: 123 w =  1.563704071465503 loss =  0.06430593349030862\n",
      "Epoch: 124 w =  1.5666127109890664 loss =  0.06345137908525962\n",
      "Epoch: 125 w =  1.5695019595824726 loss =  0.06260818075874883\n",
      "Epoch: 126 w =  1.5723719465185895 loss =  0.061776187601110345\n",
      "Epoch: 127 w =  1.5752228002084656 loss =  0.06095525070810004\n",
      "Epoch: 128 w =  1.5780546482070759 loss =  0.060145223154245704\n",
      "Epoch: 129 w =  1.5808676172190288 loss =  0.0593459599665515\n",
      "Epoch: 130 w =  1.5836618331042354 loss =  0.058557318098551525\n",
      "Epoch: 131 w =  1.5864374208835406 loss =  0.05777915640470852\n",
      "Epoch: 132 w =  1.589194504744317 loss =  0.05701133561515259\n",
      "Epoch: 133 w =  1.5919332080460216 loss =  0.05625371831075566\n",
      "Epoch: 134 w =  1.5946536533257147 loss =  0.05550616889853718\n",
      "Epoch: 135 w =  1.5973559623035434 loss =  0.05476855358739662\n",
      "Epoch: 136 w =  1.6000402558881863 loss =  0.05404074036416853\n",
      "Epoch: 137 w =  1.602706654182265 loss =  0.053322598969995826\n",
      "Epoch: 138 w =  1.6053552764877166 loss =  0.052614000877016794\n",
      "Epoch: 139 w =  1.6079862413111319 loss =  0.051914819265362205\n",
      "Epoch: 140 w =  1.6105996663690576 loss =  0.051224929000458046\n",
      "Epoch: 141 w =  1.6131956685932638 loss =  0.05054420661062975\n",
      "Epoch: 142 w =  1.6157743641359754 loss =  0.04987253026500407\n",
      "Epoch: 143 w =  1.6183358683750688 loss =  0.04920977975170467\n",
      "Epoch: 144 w =  1.620880295919235 loss =  0.0485558364563376\n",
      "Epoch: 145 w =  1.6234077606131068 loss =  0.04791058334076228\n",
      "Epoch: 146 w =  1.6259183755423527 loss =  0.04727390492214503\n",
      "Epoch: 147 w =  1.628412253038737 loss =  0.04664568725229074\n",
      "Epoch: 148 w =  1.6308895046851455 loss =  0.046025817897249195\n",
      "Epoch: 149 w =  1.6333502413205778 loss =  0.04541418591719241\n",
      "Epoch: 150 w =  1.6357945730451073 loss =  0.04481068184655951\n",
      "Epoch: 151 w =  1.6382226092248067 loss =  0.04421519767446522\n",
      "Epoch: 152 w =  1.6406344584966412 loss =  0.04362762682536898\n",
      "Epoch: 153 w =  1.6430302287733303 loss =  0.04304786414000075\n",
      "Epoch: 154 w =  1.6454100272481746 loss =  0.04247580585654031\n",
      "Epoch: 155 w =  1.6477739603998536 loss =  0.04191134959204675\n",
      "Epoch: 156 w =  1.6501221339971879 loss =  0.04135439432413464\n",
      "Epoch: 157 w =  1.6524546531038733 loss =  0.04080484037289392\n",
      "Epoch: 158 w =  1.6547716220831807 loss =  0.040262589383049684\n",
      "Epoch: 159 w =  1.6570731446026261 loss =  0.03972754430635939\n",
      "Epoch: 160 w =  1.6593593236386086 loss =  0.03919960938424379\n",
      "Epoch: 161 w =  1.661630261481018 loss =  0.03867869013064873\n",
      "Epoch: 162 w =  1.6638860597378111 loss =  0.03816469331513477\n",
      "Epoch: 163 w =  1.666126819339559 loss =  0.03765752694619143\n",
      "Epoch: 164 w =  1.668352640543962 loss =  0.03715710025477314\n",
      "Epoch: 165 w =  1.6705636229403356 loss =  0.03666332367805417\n",
      "Epoch: 166 w =  1.6727598654540667 loss =  0.036176108843399134\n",
      "Epoch: 167 w =  1.6749414663510396 loss =  0.03569536855254683\n",
      "Epoch: 168 w =  1.6771085232420326 loss =  0.03522101676600411\n",
      "Epoch: 169 w =  1.6792611330870857 loss =  0.03475296858764699\n",
      "Epoch: 170 w =  1.6813993921998385 loss =  0.03429114024952672\n",
      "Epoch: 171 w =  1.6835233962518394 loss =  0.03383544909687745\n",
      "Epoch: 172 w =  1.6856332402768273 loss =  0.03338581357332341\n",
      "Epoch: 173 w =  1.6877290186749818 loss =  0.03294215320628234\n",
      "Epoch: 174 w =  1.6898108252171486 loss =  0.032504388592563285\n",
      "Epoch: 175 w =  1.6918787530490342 loss =  0.03207244138415544\n",
      "Epoch: 176 w =  1.693932894695374 loss =  0.03164623427420601\n",
      "Epoch: 177 w =  1.6959733420640715 loss =  0.03122569098318434\n",
      "Epoch: 178 w =  1.698000186450311 loss =  0.030810736245230032\n",
      "Epoch: 179 w =  1.7000135185406422 loss =  0.030401295794682325\n",
      "Epoch: 180 w =  1.7020134284170378 loss =  0.02999729635278854\n",
      "Epoch: 181 w =  1.7040000055609243 loss =  0.02959866561458928\n",
      "Epoch: 182 w =  1.705973338857185 loss =  0.029205332235977603\n",
      "Epoch: 183 w =  1.707933516598137 loss =  0.0288172258209306\n",
      "Epoch: 184 w =  1.7098806264874828 loss =  0.028434276908910223\n",
      "Epoch: 185 w =  1.711814755644233 loss =  0.028056416962431815\n",
      "Epoch: 186 w =  1.7137359906066048 loss =  0.02768357835479773\n",
      "Epoch: 187 w =  1.715644417335894 loss =  0.02731569435799396\n",
      "Epoch: 188 w =  1.7175401212203214 loss =  0.02695269913074774\n",
      "Epoch: 189 w =  1.7194231870788526 loss =  0.026594527706743577\n",
      "Epoch: 190 w =  1.7212936991649936 loss =  0.02624111598299619\n",
      "Epoch: 191 w =  1.7231517411705604 loss =  0.02589240070837769\n",
      "Epoch: 192 w =  1.7249973962294234 loss =  0.02554831947229747\n",
      "Epoch: 193 w =  1.7268307469212272 loss =  0.025208810693532254\n",
      "Epoch: 194 w =  1.7286518752750857 loss =  0.02487381360920488\n",
      "Epoch: 195 w =  1.7304608627732518 loss =  0.02454326826390922\n",
      "Epoch: 196 w =  1.7322577903547636 loss =  0.024217115498979924\n",
      "Epoch: 197 w =  1.734042738419065 loss =  0.02389529694190458\n",
      "Epoch: 198 w =  1.7358157868296047 loss =  0.023577754995876613\n",
      "Epoch: 199 w =  1.7375770149174072 loss =  0.023264432829486956\n",
      "Epoch: 200 w =  1.7393265014846244 loss =  0.0229552743665529\n",
      "Epoch: 201 w =  1.7410643248080602 loss =  0.022650224276081838\n",
      "Epoch: 202 w =  1.7427905626426732 loss =  0.022349227962368587\n",
      "Epoch: 203 w =  1.7445052922250555 loss =  0.02205223155522421\n",
      "Epoch: 204 w =  1.7462085902768885 loss =  0.021759181900334768\n",
      "Epoch: 205 w =  1.747900533008376 loss =  0.021470026549748094\n",
      "Epoch: 206 w =  1.7495811961216534 loss =  0.021184713752486994\n",
      "Epoch: 207 w =  1.7512506548141757 loss =  0.02090319244528727\n",
      "Epoch: 208 w =  1.7529089837820813 loss =  0.020625412243458784\n",
      "Epoch: 209 w =  1.754556257223534 loss =  0.020351323431867924\n",
      "Epoch: 210 w =  1.7561925488420438 loss =  0.020080876956039998\n",
      "Epoch: 211 w =  1.7578179318497635 loss =  0.019814024413379736\n",
      "Epoch: 212 w =  1.7594324789707652 loss =  0.019550718044508593\n",
      "Epoch: 213 w =  1.7610362624442935 loss =  0.019290910724717118\n",
      "Epoch: 214 w =  1.7626293540279983 loss =  0.019034555955530864\n",
      "Epoch: 215 w =  1.764211825001145 loss =  0.018781607856388462\n",
      "Epoch: 216 w =  1.7657837461678039 loss =  0.018532021156430236\n",
      "Epoch: 217 w =  1.7673451878600186 loss =  0.01828575118639591\n",
      "Epoch: 218 w =  1.7688962199409517 loss =  0.01804275387063001\n",
      "Epoch: 219 w =  1.770436911808012 loss =  0.017802985719193656\n",
      "Epoch: 220 w =  1.7719673323959586 loss =  0.017566403820080803\n",
      "Epoch: 221 w =  1.7734875501799856 loss =  0.01733296583153841\n",
      "Epoch: 222 w =  1.7749976331787858 loss =  0.01710262997448818\n",
      "Epoch: 223 w =  1.7764976489575939 loss =  0.016875355025049412\n",
      "Epoch: 224 w =  1.7779876646312098 loss =  0.01665110030716098\n",
      "Epoch: 225 w =  1.7794677468670017 loss =  0.016429825685301387\n",
      "Epoch: 226 w =  1.7809379618878884 loss =  0.01621149155730561\n",
      "Epoch: 227 w =  1.7823983754753026 loss =  0.015996058847277405\n",
      "Epoch: 228 w =  1.7838490529721338 loss =  0.015783488998595805\n",
      "Epoch: 229 w =  1.7852900592856529 loss =  0.015573743967014471\n",
      "Epoch: 230 w =  1.7867214588904152 loss =  0.01536678621385282\n",
      "Epoch: 231 w =  1.7881433158311457 loss =  0.015162578699277613\n",
      "Epoch: 232 w =  1.7895556937256047 loss =  0.014961084875673892\n",
      "Epoch: 233 w =  1.790958655767434 loss =  0.014762268681103823\n",
      "Epoch: 234 w =  1.7923522647289845 loss =  0.014566094532852724\n",
      "Epoch: 235 w =  1.7937365829641245 loss =  0.014372527321060584\n",
      "Epoch: 236 w =  1.7951116724110303 loss =  0.014181532402438496\n",
      "Epoch: 237 w =  1.7964775945949567 loss =  0.013993075594068318\n",
      "Epoch: 238 w =  1.7978344106309903 loss =  0.013807123167284932\n",
      "Epoch: 239 w =  1.7991821812267836 loss =  0.013623641841639689\n",
      "Epoch: 240 w =  1.8005209666852717 loss =  0.013442598778944123\n",
      "Epoch: 241 w =  1.8018508269073699 loss =  0.013263961577392824\n",
      "Epoch: 242 w =  1.8031718213946542 loss =  0.013087698265764365\n",
      "Epoch: 243 w =  1.8044840092520231 loss =  0.01291377729769931\n",
      "Epoch: 244 w =  1.805787449190343 loss =  0.012742167546054325\n",
      "Epoch: 245 w =  1.807082199529074 loss =  0.012572838297331206\n",
      "Epoch: 246 w =  1.80836831819888 loss =  0.012405759246180004\n",
      "Epoch: 247 w =  1.809645862744221 loss =  0.01224090048997522\n",
      "Epoch: 248 w =  1.810914890325926 loss =  0.012078232523463996\n",
      "Epoch: 249 w =  1.8121754577237532 loss =  0.011917726233485531\n",
      "Epoch: 250 w =  1.8134276213389282 loss =  0.011759352893760538\n",
      "Epoch: 251 w =  1.8146714371966688 loss =  0.011603084159750117\n",
      "Epoch: 252 w =  1.815906960948691 loss =  0.01144889206358276\n",
      "Epoch: 253 w =  1.8171342478756998 loss =  0.011296749009048916\n",
      "Epoch: 254 w =  1.8183533528898617 loss =  0.011146627766662004\n",
      "Epoch: 255 w =  1.8195643305372626 loss =  0.010998501468785041\n",
      "Epoch: 256 w =  1.8207672350003477 loss =  0.010852343604822073\n",
      "Epoch: 257 w =  1.8219621201003453 loss =  0.010708128016473535\n",
      "Epoch: 258 w =  1.8231490392996763 loss =  0.010565828893054623\n",
      "Epoch: 259 w =  1.8243280457043451 loss =  0.01042542076687581\n",
      "Epoch: 260 w =  1.8254991920663162 loss =  0.010286878508684883\n",
      "Epoch: 261 w =  1.8266625307858742 loss =  0.010150177323169472\n",
      "Epoch: 262 w =  1.8278181139139684 loss =  0.010015292744519342\n",
      "Epoch: 263 w =  1.828965993154542 loss =  0.009882200632047719\n",
      "Epoch: 264 w =  1.830106219866845 loss =  0.009750877165870737\n",
      "Epoch: 265 w =  1.8312388450677326 loss =  0.009621298842644276\n",
      "Epoch: 266 w =  1.8323639194339476 loss =  0.009493442471357588\n",
      "Epoch: 267 w =  1.833481493304388 loss =  0.009367285169182669\n",
      "Epoch: 268 w =  1.8345916166823586 loss =  0.009242804357378868\n",
      "Epoch: 269 w =  1.8356943392378096 loss =  0.009119977757251931\n",
      "Epoch: 270 w =  1.8367897103095574 loss =  0.00899878338616667\n",
      "Epoch: 271 w =  1.8378777789074936 loss =  0.008879199553612731\n",
      "Epoch: 272 w =  1.838958593714777 loss =  0.00876120485732251\n",
      "Epoch: 273 w =  1.8400322030900118 loss =  0.008644778179440754\n",
      "Epoch: 274 w =  1.8410986550694117 loss =  0.008529898682745075\n",
      "Epoch: 275 w =  1.842157997368949 loss =  0.008416545806916597\n",
      "Epoch: 276 w =  1.8432102773864893 loss =  0.008304699264860235\n",
      "Epoch: 277 w =  1.8442555422039129 loss =  0.008194339039073871\n",
      "Epoch: 278 w =  1.8452938385892201 loss =  0.008085445378065723\n",
      "Epoch: 279 w =  1.8463252129986254 loss =  0.007977998792819425\n",
      "Epoch: 280 w =  1.8473497115786346 loss =  0.00787198005330595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 281 w =  1.8483673801681104 loss =  0.007767370185042016\n",
      "Epoch: 282 w =  1.849378264300323 loss =  0.007664150465694118\n",
      "Epoch: 283 w =  1.8503824092049874 loss =  0.0075623024217277915\n",
      "Epoch: 284 w =  1.8513798598102875 loss =  0.007461807825101277\n",
      "Epoch: 285 w =  1.8523706607448855 loss =  0.007362648690003263\n",
      "Epoch: 286 w =  1.8533548563399196 loss =  0.007264807269633893\n",
      "Epoch: 287 w =  1.8543324906309868 loss =  0.00716826605302854\n",
      "Epoch: 288 w =  1.8553036073601137 loss =  0.007073007761923847\n",
      "Epoch: 289 w =  1.856268249977713 loss =  0.006979015347665382\n",
      "Epoch: 290 w =  1.8572264616445282 loss =  0.0068862719881564\n",
      "Epoch: 291 w =  1.8581782852335647 loss =  0.006794761084847126\n",
      "Epoch: 292 w =  1.8591237633320075 loss =  0.006704466259764049\n",
      "Epoch: 293 w =  1.8600629382431275 loss =  0.006615371352578748\n",
      "Epoch: 294 w =  1.8609958519881733 loss =  0.0065274604177155806\n",
      "Epoch: 295 w =  1.861922546308252 loss =  0.0064407177214979424\n",
      "Epoch: 296 w =  1.862843062666197 loss =  0.006355127739332267\n",
      "Epoch: 297 w =  1.8637574422484222 loss =  0.006270675152929595\n",
      "Epoch: 298 w =  1.864665725966766 loss =  0.0061873448475640025\n",
      "Epoch: 299 w =  1.865567954460321 loss =  0.0061051219093674865\n",
      "Epoch: 300 w =  1.8664641680972522 loss =  0.006023991622660779\n",
      "Epoch: 301 w =  1.867354406976604 loss =  0.005943939467319636\n",
      "Epoch: 302 w =  1.8682387109300933 loss =  0.005864951116176139\n",
      "Epoch: 303 w =  1.8691171195238927 loss =  0.005787012432454505\n",
      "Epoch: 304 w =  1.8699896720604001 loss =  0.005710109467240996\n",
      "Epoch: 305 w =  1.8708564075799974 loss =  0.0056342284569874355\n",
      "Epoch: 306 w =  1.8717173648627974 loss =  0.005559355821047919\n",
      "Epoch: 307 w =  1.8725725824303787 loss =  0.005485478159248217\n",
      "Epoch: 308 w =  1.8734220985475096 loss =  0.005412582249487542\n",
      "Epoch: 309 w =  1.8742659512238595 loss =  0.0053406550453721216\n",
      "Epoch: 310 w =  1.8751041782157005 loss =  0.00526968367388029\n",
      "Epoch: 311 w =  1.8759368170275958 loss =  0.005199655433058499\n",
      "Epoch: 312 w =  1.8767639049140785 loss =  0.005130557789748077\n",
      "Epoch: 313 w =  1.877585478881318 loss =  0.0050623783773420935\n",
      "Epoch: 314 w =  1.878401575688776 loss =  0.004995104993572082\n",
      "Epoch: 315 w =  1.8792122318508508 loss =  0.0049287255983241594\n",
      "Epoch: 316 w =  1.8800174836385117 loss =  0.004863228311484204\n",
      "Epoch: 317 w =  1.8808173670809216 loss =  0.0047986014108116004\n",
      "Epoch: 318 w =  1.8816119179670487 loss =  0.004734833329841262\n",
      "Epoch: 319 w =  1.8824011718472684 loss =  0.004671912655813599\n",
      "Epoch: 320 w =  1.8831851640349533 loss =  0.004609828127631899\n",
      "Epoch: 321 w =  1.8839639296080537 loss =  0.004548568633846924\n",
      "Epoch: 322 w =  1.8847375034106666 loss =  0.004488123210668239\n",
      "Epoch: 323 w =  1.8855059200545954 loss =  0.004428481040002032\n",
      "Epoch: 324 w =  1.886269213920898 loss =  0.0043696314475149016\n",
      "Epoch: 325 w =  1.8870274191614254 loss =  0.004311563900723481\n",
      "Epoch: 326 w =  1.8877805697003491 loss =  0.004254268007109427\n",
      "Epoch: 327 w =  1.8885286992356802 loss =  0.004197733512259399\n",
      "Epoch: 328 w =  1.8892718412407756 loss =  0.004141950298029812\n",
      "Epoch: 329 w =  1.890010028965837 loss =  0.004086908380735999\n",
      "Epoch: 330 w =  1.8907432954393981 loss =  0.004032597909365336\n",
      "Epoch: 331 w =  1.891471673469802 loss =  0.003979009163814215\n",
      "Epoch: 332 w =  1.89219519564667 loss =  0.003926132553148423\n",
      "Epoch: 333 w =  1.892913894342359 loss =  0.003873958613886583\n",
      "Epoch: 334 w =  1.8936278017134098 loss =  0.003822478008306491\n",
      "Epoch: 335 w =  1.894336949701987 loss =  0.0037716815227738856\n",
      "Epoch: 336 w =  1.8950413700373072 loss =  0.0037215600660934697\n",
      "Epoch: 337 w =  1.8957410942370585 loss =  0.003672104667881824\n",
      "Epoch: 338 w =  1.8964361536088115 loss =  0.003623306476961972\n",
      "Epoch: 339 w =  1.8971265792514194 loss =  0.003575156759779232\n",
      "Epoch: 340 w =  1.89781240205641 loss =  0.0035276468988381673\n",
      "Epoch: 341 w =  1.8984936527093672 loss =  0.0034807683911602665\n",
      "Epoch: 342 w =  1.8991703616913047 loss =  0.0034345128467621854\n",
      "Epoch: 343 w =  1.8998425592800294 loss =  0.0033888719871541045\n",
      "Epoch: 344 w =  1.900510275551496 loss =  0.0033438376438581397\n",
      "Epoch: 345 w =  1.9011735403811527 loss =  0.003299401756946419\n",
      "Epoch: 346 w =  1.9018323834452784 loss =  0.003255556373598551\n",
      "Epoch: 347 w =  1.9024868342223098 loss =  0.003212293646678284\n",
      "Epoch: 348 w =  1.903136921994161 loss =  0.0031696058333290974\n",
      "Epoch: 349 w =  1.9037826758475331 loss =  0.00312748529358842\n",
      "Epoch: 350 w =  1.9044241246752163 loss =  0.0030859244890202946\n",
      "Epoch: 351 w =  1.9050612971773815 loss =  0.003044915981366201\n",
      "Epoch: 352 w =  1.9056942218628656 loss =  0.0030044524312138245\n",
      "Epoch: 353 w =  1.9063229270504465 loss =  0.0029645265966834703\n",
      "Epoch: 354 w =  1.9069474408701101 loss =  0.002925131332131991\n",
      "Epoch: 355 w =  1.9075677912643094 loss =  0.002886259586873883\n",
      "Epoch: 356 w =  1.9081840059892141 loss =  0.0028479044039194244\n",
      "Epoch: 357 w =  1.9087961126159527 loss =  0.0028100589187295567\n",
      "Epoch: 358 w =  1.9094041385318463 loss =  0.0027727163579873257\n",
      "Epoch: 359 w =  1.910008110941634 loss =  0.002735870038385631\n",
      "Epoch: 360 w =  1.9106080568686898 loss =  0.002699513365431083\n",
      "Epoch: 361 w =  1.9112040031562318 loss =  0.0026636398322637967\n",
      "Epoch: 362 w =  1.9117959764685237 loss =  0.002628243018492828\n",
      "Epoch: 363 w =  1.912384003292067 loss =  0.0025933165890470733\n",
      "Epoch: 364 w =  1.9129681099367866 loss =  0.002558854293041513\n",
      "Epoch: 365 w =  1.913548322537208 loss =  0.002524849962658423\n",
      "Epoch: 366 w =  1.9141246670536267 loss =  0.0024912975120435347\n",
      "Epoch: 367 w =  1.9146971692732693 loss =  0.0024581909362168214\n",
      "Epoch: 368 w =  1.9152658548114474 loss =  0.002425524309997759\n",
      "Epoch: 369 w =  1.9158307491127045 loss =  0.002393291786944904\n",
      "Epoch: 370 w =  1.916391877451953 loss =  0.0023614875983095007\n",
      "Epoch: 371 w =  1.9169492649356068 loss =  0.0023301060520030787\n",
      "Epoch: 372 w =  1.9175029365027028 loss =  0.0022991415315786767\n",
      "Epoch: 373 w =  1.918052916926018 loss =  0.002268588495225698\n",
      "Epoch: 374 w =  1.918599230813178 loss =  0.0022384414747780306\n",
      "Epoch: 375 w =  1.919141902607757 loss =  0.0022086950747354198\n",
      "Epoch: 376 w =  1.9196809565903719 loss =  0.0021793439712978198\n",
      "Epoch: 377 w =  1.9202164168797693 loss =  0.0021503829114125766\n",
      "Epoch: 378 w =  1.9207483074339042 loss =  0.002121806711834254\n",
      "Epoch: 379 w =  1.9212766520510116 loss =  0.002093610258196986\n",
      "Epoch: 380 w =  1.9218014743706715 loss =  0.0020657885040991646\n",
      "Epoch: 381 w =  1.922322797874867 loss =  0.0020383364702002485\n",
      "Epoch: 382 w =  1.9228406458890346 loss =  0.0020112492433295927\n",
      "Epoch: 383 w =  1.9233550415831078 loss =  0.0019845219756071185\n",
      "Epoch: 384 w =  1.9238660079725538 loss =  0.001958149883575713\n",
      "Epoch: 385 w =  1.9243735679194034 loss =  0.0019321282473450802\n",
      "Epoch: 386 w =  1.924877744133274 loss =  0.0019064524097470307\n",
      "Epoch: 387 w =  1.9253785591723855 loss =  0.0018811177755019466\n",
      "Epoch: 388 w =  1.9258760354445696 loss =  0.0018561198103963887\n",
      "Epoch: 389 w =  1.9263701952082726 loss =  0.001831454040471565\n",
      "Epoch: 390 w =  1.9268610605735508 loss =  0.0018071160512226289\n",
      "Epoch: 391 w =  1.9273486535030604 loss =  0.0017831014868086028\n",
      "Epoch: 392 w =  1.92783299581304 loss =  0.001759406049272793\n",
      "Epoch: 393 w =  1.9283141091742864 loss =  0.0017360254977735665\n",
      "Epoch: 394 w =  1.9287920151131244 loss =  0.0017129556478253781\n",
      "Epoch: 395 w =  1.9292667350123702 loss =  0.001690192370549836\n",
      "Epoch: 396 w =  1.9297382901122877 loss =  0.0016677315919367507\n",
      "Epoch: 397 w =  1.9302067015115392 loss =  0.001645569292115016\n",
      "Epoch: 398 w =  1.9306719901681288 loss =  0.0016237015046331295\n",
      "Epoch: 399 w =  1.9311341769003414 loss =  0.0016021243157493409\n",
      "Epoch: 400 w =  1.9315932823876725 loss =  0.0015808338637311578\n",
      "Epoch: 401 w =  1.9320493271717547 loss =  0.0015598263381642406\n",
      "Epoch: 402 w =  1.9325023316572763 loss =  0.0015390979792704112\n",
      "Epoch: 403 w =  1.9329523161128943 loss =  0.0015186450772347773\n",
      "Epoch: 404 w =  1.9333993006721417 loss =  0.0014984639715417507\n",
      "Epoch: 405 w =  1.9338433053343274 loss =  0.0014785510503199275\n",
      "Epoch: 406 w =  1.9342843499654319 loss =  0.0014589027496956797\n",
      "Epoch: 407 w =  1.9347224542989956 loss =  0.0014395155531552785\n",
      "Epoch: 408 w =  1.9351576379370024 loss =  0.0014203859909155733\n",
      "Epoch: 409 w =  1.9355899203507556 loss =  0.0014015106393029588\n",
      "Epoch: 410 w =  1.9360193208817507 loss =  0.0013828861201406687\n",
      "Epoch: 411 w =  1.936445858742539 loss =  0.001364509100144129\n",
      "Epoch: 412 w =  1.9368695530175888 loss =  0.0013463762903244334\n",
      "Epoch: 413 w =  1.937290422664138 loss =  0.0013284844453996776\n",
      "Epoch: 414 w =  1.9377084865130438 loss =  0.0013108303632141487\n",
      "Epoch: 415 w =  1.9381237632696235 loss =  0.001293410884165214\n",
      "Epoch: 416 w =  1.9385362715144927 loss =  0.0012762228906378628\n",
      "Epoch: 417 w =  1.9389460297043961 loss =  0.0012592633064467203\n",
      "Epoch: 418 w =  1.9393530561730334 loss =  0.001242529096285494\n",
      "Epoch: 419 w =  1.93975736913188 loss =  0.0012260172651837457\n",
      "Epoch: 420 w =  1.9401589866710007 loss =  0.0012097248579708588\n",
      "Epoch: 421 w =  1.9405579267598607 loss =  0.0011936489587471562\n",
      "Epoch: 422 w =  1.9409542072481283 loss =  0.0011777866903620274\n",
      "Epoch: 423 w =  1.9413478458664741 loss =  0.0011621352138989954\n",
      "Epoch: 424 w =  1.9417388602273642 loss =  0.0011466917281676252\n",
      "Epoch: 425 w =  1.9421272678258483 loss =  0.001131453469202202\n",
      "Epoch: 426 w =  1.9425130860403426 loss =  0.0011164177097670303\n",
      "Epoch: 427 w =  1.942896332133407 loss =  0.0011015817588683524\n",
      "Epoch: 428 w =  1.9432770232525176 loss =  0.0010869429612727236\n",
      "Epoch: 429 w =  1.943655176430834 loss =  0.0010724986970318093\n",
      "Epoch: 430 w =  1.9440308085879618 loss =  0.0010582463810134787\n",
      "Epoch: 431 w =  1.9444039365307089 loss =  0.001044183462439122\n",
      "Epoch: 432 w =  1.9447745769538374 loss =  0.0010303074244271493\n",
      "Epoch: 433 w =  1.9451427464408118 loss =  0.0010166157835425434\n",
      "Epoch: 434 w =  1.9455084614645397 loss =  0.001003106089352357\n",
      "Epoch: 435 w =  1.9458717383881095 loss =  0.0009897759239871848\n",
      "Epoch: 436 w =  1.946232593465522 loss =  0.0009766229017084214\n",
      "Epoch: 437 w =  1.9465910428424185 loss =  0.0009636446684812752\n",
      "Epoch: 438 w =  1.9469471025568024 loss =  0.0009508389015534601\n",
      "Epoch: 439 w =  1.947300788539757 loss =  0.0009382033090394819\n",
      "Epoch: 440 w =  1.9476521166161587 loss =  0.0009257356295104663\n",
      "Epoch: 441 w =  1.9480011025053843 loss =  0.0009134336315894171\n",
      "Epoch: 442 w =  1.948347761822015 loss =  0.0009012951135518513\n",
      "Epoch: 443 w =  1.9486921100765349 loss =  0.0008893179029317611\n",
      "Epoch: 444 w =  1.9490341626760246 loss =  0.0008774998561328051\n",
      "Epoch: 445 w =  1.9493739349248511 loss =  0.0008658388580446418\n",
      "Epoch: 446 w =  1.949711442025352 loss =  0.0008543328216644033\n",
      "Epoch: 447 w =  1.9500466990785164 loss =  0.0008429796877231754\n",
      "Epoch: 448 w =  1.9503797210846596 loss =  0.0008317774243174329\n",
      "Epoch: 449 w =  1.9507105229440953 loss =  0.0008207240265453909\n",
      "Epoch: 450 w =  1.9510391194578014 loss =  0.0008098175161481851\n",
      "Epoch: 451 w =  1.9513655253280828 loss =  0.0007990559411558139\n",
      "Epoch: 452 w =  1.951689755159229 loss =  0.0007884373755377843\n",
      "Epoch: 453 w =  1.9520118234581674 loss =  0.0007779599188584158\n",
      "Epoch: 454 w =  1.952331744635113 loss =  0.0007676216959366985\n",
      "Epoch: 455 w =  1.9526495330042122 loss =  0.0007574208565106947\n",
      "Epoch: 456 w =  1.9529652027841842 loss =  0.0007473555749063972\n",
      "Epoch: 457 w =  1.9532787680989563 loss =  0.0007374240497109722\n",
      "Epoch: 458 w =  1.9535902429782965 loss =  0.0007276245034503683\n",
      "Epoch: 459 w =  1.9538996413584413 loss =  0.0007179551822711848\n",
      "Epoch: 460 w =  1.9542069770827184 loss =  0.0007084143556267787\n",
      "Epoch: 461 w =  1.954512263902167 loss =  0.0006990003159675593\n",
      "Epoch: 462 w =  1.9548155154761526 loss =  0.0006897113784353654\n",
      "Epoch: 463 w =  1.9551167453729783 loss =  0.0006805458805619344\n",
      "Epoch: 464 w =  1.9554159670704918 loss =  0.0006715021819713548\n",
      "Epoch: 465 w =  1.9557131939566885 loss =  0.0006625786640864902\n",
      "Epoch: 466 w =  1.9560084393303105 loss =  0.0006537737298392966\n",
      "Epoch: 467 w =  1.9563017164014418 loss =  0.0006450858033849894\n",
      "Epoch: 468 w =  1.9565930382920989 loss =  0.0006365133298200071\n",
      "Epoch: 469 w =  1.9568824180368183 loss =  0.0006280547749037322\n",
      "Epoch: 470 w =  1.9571698685832395 loss =  0.0006197086247838978\n",
      "Epoch: 471 w =  1.9574554027926845 loss =  0.0006114733857256585\n",
      "Epoch: 472 w =  1.9577390334407332 loss =  0.0006033475838442397\n",
      "Epoch: 473 w =  1.958020773217795 loss =  0.0005953297648411559\n",
      "Epoch: 474 w =  1.9583006347296763 loss =  0.0005874184937439326\n",
      "Epoch: 475 w =  1.9585786304981452 loss =  0.0005796123546492926\n",
      "Epoch: 476 w =  1.958854772961491 loss =  0.0005719099504697295\n",
      "Epoch: 477 w =  1.959129074475081 loss =  0.0005643099026834865\n",
      "Epoch: 478 w =  1.9594015473119137 loss =  0.0005568108510878282\n",
      "Epoch: 479 w =  1.9596722036631677 loss =  0.0005494114535555929\n",
      "Epoch: 480 w =  1.9599410556387467 loss =  0.000542110385795007\n",
      "Epoch: 481 w =  1.9602081152678217 loss =  0.0005349063411126638\n",
      "Epoch: 482 w =  1.9604733944993695 loss =  0.0005277980301796553\n",
      "Epoch: 483 w =  1.960736905202707 loss =  0.0005207841808008237\n",
      "Epoch: 484 w =  1.9609986591680224 loss =  0.000513863537687071\n",
      "Epoch: 485 w =  1.9612586681069022 loss =  0.0005070348622306942\n",
      "Epoch: 486 w =  1.9615169436528561 loss =  0.0005002969322837191\n",
      "Epoch: 487 w =  1.961773497361837 loss =  0.0004936485419391504\n",
      "Epoch: 488 w =  1.9620283407127581 loss =  0.00048708850131516\n",
      "Epoch: 489 w =  1.9622814851080064 loss =  0.0004806156363421277\n",
      "Epoch: 490 w =  1.962532941873953 loss =  0.00047422878855251494\n",
      "Epoch: 491 w =  1.96278272226146 loss =  0.00046792681487352827\n",
      "Epoch: 492 w =  1.9630308374463836 loss =  0.00046170858742254167\n",
      "Epoch: 493 w =  1.9632772985300744 loss =  0.00045557299330523706\n",
      "Epoch: 494 w =  1.963522116539874 loss =  0.00044951893441642584\n",
      "Epoch: 495 w =  1.9637653024296082 loss =  0.00044354532724351223\n",
      "Epoch: 496 w =  1.9640068670800774 loss =  0.0004376511026725851\n",
      "Epoch: 497 w =  1.9642468212995436 loss =  0.00043183520579707116\n",
      "Epoch: 498 w =  1.9644851758242134 loss =  0.000426096595728922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 499 w =  1.9647219413187187 loss =  0.0004204342454123443\n",
      "Predict (after traing) 4 7.858887765274875\n"
     ]
    }
   ],
   "source": [
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]\n",
    "\n",
    "w = 1.0\n",
    "learning_rate = 0.01\n",
    "\n",
    "def forward(x):\n",
    "    return x *w\n",
    "\n",
    "def cost(xs, ys):\n",
    "    cost = 0\n",
    "    for x, y in zip(xs, ys):\n",
    "        y_pred = forward(x)\n",
    "        cost += (y_pred - y) **2\n",
    "        return cost / len(xs)\n",
    "\n",
    "def gradient(xs, ys):\n",
    "    grad = 0\n",
    "    for x, y in zip(xs, ys):\n",
    "        grad += 2 * x * (x * w - y)\n",
    "        return grad / len(xs)\n",
    "\n",
    "print(\"Predict (before training)\", 4, forward(4))\n",
    "cost_list = []\n",
    "epoch_list = []\n",
    "for epoch in range(500):\n",
    "    cost_val = cost(x_data, y_data)\n",
    "    grad_val = gradient(x_data, y_data)\n",
    "    w -= learning_rate * grad_val\n",
    "    cost_list.append(cost_val)\n",
    "    epoch_list.append(epoch)\n",
    "    print(\"Epoch:\", epoch, \"w = \", w, \"loss = \", cost_val)\n",
    "\n",
    "print(\"Predict (after traing)\", 4, forward(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAjvklEQVR4nO3dd3RU95338fd3Rl2IokYRHYQx2IBBxgXbMXFc17t4E9dNnDhxHuKNvZuyJfaTbHK2nCebstnNs8vGYR1nU7xxnE3s5YmJaxz3grAxxRgQzciiiCbR1L/PH3OBsTLgEejqSjOf1zlzZu7v3t/w/ekAH932u+buiIiIdBeLugAREemfFBAiIpKSAkJERFJSQIiISEoKCBERSSkn6gJ6U3l5uY8fPz7qMkREBozly5fvdveKVOsyKiDGjx9PbW1t1GWIiAwYZrb1ROt0iElERFJSQIiISEoKCBERSUkBISIiKSkgREQkJQWEiIikpIAQEZGUsj4gWjs6uffZjTy/oTHqUkRE+pWsD4i8eIzFz23ikTcaoi5FRKRfyfqAMDNqxg1j2Za9UZciItKvZH1AAMydUMo7ew+zs7kl6lJERPoNBQRw7vhSAF7brL0IEZGjFBDA9FGDKcqL6zCTiEgSBQSQE48xe+ww7UGIiCRRQATOHV/Kup0HaDrSHnUpIiL9ggIicO6EYbjD8q3aixARAQXEMeeMGUZu3Fi2ZV/UpYiI9AsKiEBhXpyzqoawTOchREQABcR7zB1fysr6JlraO6MuRUQkcgqIJOeOL6Wts4s3t+2PuhQRkcgpIJLUjB8GoPshRERQQLzH0KI8po4o4ZVNCggREQVEN+dPLKN2615aO3QeQkSymwKimwsmldHS3sWb25qiLkVEJFIKiG7On1CGGby8cU/UpYiIRCrUgDCzq8xsnZnVmdndKdYvMLOVZrbCzGrN7KJ0+4ZlSFEu00cN5qWNu/vqjxQR6ZdCCwgziwOLgKuBacAtZjat22ZPAzPdfRbwKeC+HvQNzQUTy3jjnf26H0JEslqYexBzgTp33+TubcCDwILkDdz9oLt7sFgMeLp9w3TBpDLaOrt4faum3RCR7BVmQFQB25KW64O29zCzPzazt4FHSexFpN036L8wODxV29jY2CuFnzu+lHjMeHmTzkOISPYKMyAsRZv/XoP7w+4+FbgO+Pue9A36L3b3GnevqaioONVa36OkIJezq4boRLWIZLUwA6IeGJO0PBpoONHG7v4cMMnMynvaNwwXTCpjxbb9HGrt6Ms/VkSk3wgzIJYB1WY2wczygJuBJckbmNlkM7Pg82wgD9iTTt+wXTCxjI4up1bnIUQkS+WE9cXu3mFmdwGPA3HgfndfY2Z3BOvvBT4CfNzM2oEjwE3BSeuUfcOqNZWa8YnnQ7y8cQ8fmNI7h65ERAaS0AICwN2XAku7td2b9PkbwDfS7duXivJyOGfMMF6s0/0QIpKddCf1SVxcXc7qhib2HmqLuhQRkT6ngDiJi6rLcUd7ESKSlRQQJzFj9FAGF+Tw/Ibeub9CRGQgUUCcRDxmzJtczvMbdnP8hm8RkeyggHgfF1dXsL2phY2Nh6IuRUSkTykg3sfF1eUAOswkIllHAfE+xpQWMb6siOc36ES1iGQXBUQaLq6u4JVNe2jr6Iq6FBGRPqOASMPF1eUcbuvk9Xc07YaIZA8FRBrOn1RGPGY6DyEiWUUBkYbBBbnMHjuUZ9crIEQkeygg0nTpGZWsfreZXc0tUZciItInFBBpmn9GJQC/W6e9CBHJDgqINJ05soQRgwv47du7oi5FRKRPKCDSZGbMn1rBC3W7dbmriGQFBUQPzD+jkoOtHdRu2Rt1KSIioVNA9MC8yeXkxWM8s06HmUQk8ykgeqA4P4fzJpbqPISIZAUFRA/NP6OSjY2HeGfP4ahLEREJlQKih+ZPTVzuqsNMIpLpFBA9NKG8mAnlxTytw0wikuFCDQgzu8rM1plZnZndnWL9R81sZfB6ycxmJq3bYmarzGyFmdWGWWdPXTa1klc27uFAS3vUpYiIhCa0gDCzOLAIuBqYBtxiZtO6bbYZ+IC7zwD+Hljcbf18d5/l7jVh1Xkqrpg+grbOLs3NJCIZLcw9iLlAnbtvcvc24EFgQfIG7v6Sux+dQ/sVYHSI9fSaOeOGUVqcx5Nv7Yy6FBGR0IQZEFXAtqTl+qDtRG4HfpO07MATZrbczBaeqJOZLTSzWjOrbWzsm9/o4zHjsqmV/PbtXbR36q5qEclMYQaEpWjzlBuazScREF9Kap7n7rNJHKK608wuSdXX3Re7e42711RUVJxuzWm7YvoIDrR08Oom3VUtIpkpzICoB8YkLY8GGrpvZGYzgPuABe6+52i7uzcE77uAh0kcsuo3Lq4upzA3zhNv7Yi6FBGRUIQZEMuAajObYGZ5wM3AkuQNzGws8CvgVndfn9RebGYlRz8DVwCrQ6y1xwpy41xcXc6Tb+3EPeWOkYjIgBZaQLh7B3AX8DiwFnjI3deY2R1mdkew2VeBMuDfu13OOhx4wczeBF4DHnX3x8Kq9VRdMX0E25taWP1uc9SliIj0upwwv9zdlwJLu7Xdm/T508CnU/TbBMzs3t7fXDa1kpjBE2/t4OzRQ6IuR0SkV+lO6tMwrDiPuRNKeXyNzkOISOZRQJymq88ayfqdB6nbdSDqUkREepUC4jRdfdYIzODRldqLEJHMooA4TZWDCzh3XCmPrvq9K3hFRAY0BUQv+IMZOswkIplHAdELdJhJRDKRAqIXHD3MtHTV9qhLERHpNQqIXnLN2SNYt/OADjOJSMZQQPSSq88eqcNMIpJRFBC9ZLiuZhKRDKOA6EV/ODNxNdPa7ZqbSUQGPgVEL/qDGaPIiRmPrHg36lJERE6bAqIXlRbnccmUCpasaKCrS1OAi8jApoDoZQtmjWJ7UwuvbtaT5kRkYFNA9LLLpw2nKC/O/+gwk4gMcAqIXlaUl8OV00ewdNV2Wjs6oy5HROSUKSBCcN05VTS3dPDM241RlyIicsoUECGYN6mM8kF5OswkIgOaAiIEOfEYfzhzFE+v3cX+w21RlyMickoUECG5fs5o2jq7WPKm7qwWkYEp1IAws6vMbJ2Z1ZnZ3SnWf9TMVgavl8xsZrp9+7vpo4YwbeRgHqrdFnUpIiKnJLSAMLM4sAi4GpgG3GJm07ptthn4gLvPAP4eWNyDvv3ejTWjWf1uM281aOoNERl4wtyDmAvUufsmd28DHgQWJG/g7i+5+75g8RVgdLp9B4IFs6rIi8f4xXLtRYjIwBNmQFQByf8z1gdtJ3I78Jue9jWzhWZWa2a1jY3967LSYcV5fGhaJY+88S5tHV1RlyMi0iNhBoSlaEs5QZGZzScREF/qaV93X+zuNe5eU1FRcUqFhumGmjHsO9zO02t3Rl2KiEiPhBkQ9cCYpOXRwO9d0mNmM4D7gAXuvqcnfQeCS6orGD44XyerRWTACTMglgHVZjbBzPKAm4ElyRuY2VjgV8Ct7r6+J30HinjM+Mjs0Ty7vpGG/UeiLkdEJG2hBYS7dwB3AY8Da4GH3H2Nmd1hZncEm30VKAP+3cxWmFntyfqGVWvYbpk7FgcefO2dqEsREUmbuWfOcwtqamq8trY26jJS+uQPX2NNQzMv3v1BcuO6P1FE+gczW+7uNanW6X+qPvLR88ax60ArT72lk9UiMjAoIPrI/KmVVA0t5Kevbo26FBGRtCgg+kg8Ztwydwwv1u1hU+PBqMsREXlfCog+dOO5Y8iJGQ+8qpPVItL/KSD6UGVJAVeeNYL/Xl5PS7ueNici/ZsCoo997LxxNB1p59crt0ddiojISSkg+tj5E0uZXDmI/3xpM5l0ibGIZB4FRB8zMz41bwKr323mtc17oy5HROSE0goIM/ucmQ22hB+Y2etmdkXYxWWqD8+uYlhRLve9sDnqUkRETijdPYhPuXszcAVQAXwS+MfQqspwBblxPnb+OJ5au5Mtuw9FXY6ISErpBsTR6bevAX7o7m+SekpuSdOtF4wjNxbjhy9qL0JE+qd0A2K5mT1BIiAeN7MSQE/AOQ2VJQX84cxRPFRbT9Ph9qjLERH5PekGxO3A3cC57n4YyCVxmElOw+0XTeBIeyf/pVleRaQfSjcgLgDWuft+M/sY8BWgKbyyssO0UYOZN7mMH720hfZO7ZCJSP+SbkB8DzhsZjOBvwa2Aj8OraoscvtFE9jR3MKSFQPygXkiksHSDYgOT9zVtQD4rrt/FygJr6zscemUSqaOKOHff1dHV5dunBOR/iPdgDhgZvcAtwKPmlmcxHkIOU2xmHHn/MlsbDzEY2t2RF2OiMgx6QbETUArifshdgBVwLdCqyrLXHP2SCaWF7PomTpNvyEi/UZaARGEwgPAEDO7Fmhxd52D6CXxmHHHpZNY09DM79Y1Rl2OiAiQ/lQbNwKvATcANwKvmtn1YRaWbf74nCqqhhbyb9qLEJF+It1DTF8mcQ/EJ9z948Bc4G/CKyv75MZjfOYDE1m+dR+vbNIkfiISvXQDIubuu5KW96TT18yuMrN1ZlZnZnenWD/VzF42s1Yz+8tu67aY2SozW2FmtWnWOaDdWDOG8kH5LHqmLupSRETSDojHzOxxM7vNzG4DHgWWnqxDcKXTIuBqYBpwi5lN67bZXuDPgW+f4Gvmu/ssd69Js84BrSA3zmcumcgLdbt5ddOeqMsRkSyX7knqvwIWAzOAmcBid//S+3SbC9S5+yZ3bwMeJHEfRfL37nL3ZYAmIwrcesE4hg/O59tPrNO5CBGJVNoPDHL3X7r7F939C+7+cBpdqoBtScv1QVvafyTwhJktN7OFJ9rIzBaaWa2Z1TY2DvwrgApy49z1wWqWbdnHs+sH/nhEZOA6aUCY2QEza07xOmBmze/z3ammA+/Jr8Tz3H02iUNUd5rZJak2cvfF7l7j7jUVFRU9+Pr+66aaMYweVqi9CBGJ1EkDwt1L3H1wileJuw9+n++uB8YkLY8G0p5wyN0bgvddwMMkDlllhbycGF/40BRWv9vMY6t1d7WIRCPMZ1IvA6rNbIKZ5QE3A0vS6WhmxcEzJzCzYhJPslsdWqX90HXnVDG5chD/9OR6OjVHk4hEILSAcPcO4C7gcWAt8JC7rzGzO8zsDgAzG2Fm9cAXga+YWb2ZDQaGAy+Y2ZskbtB71N0fC6vW/igeM/7i8inU7TrIw2+8G3U5IpKFLJOOcdfU1HhtbebcMuHuLFj0Io0HWvntX1xKYV486pJEJMOY2fIT3UoQ5iEmOU1mxpevOZPtTS3c9/ymqMsRkSyjgOjnzptYxlXTR/C9Zzeyq7kl6nJEJIsoIAaAe66ZSntnF99+Yl3UpYhIFlFADADjyoq57cLx/GJ5PWsa9ChwEekbCogB4q4PVjO0MJd/+PVa3TwnIn1CATFADCnM5QuXT+HlTXt085yI9AkFxADyJ3PHcubIwfzdr9/iUGtH1OWISIZTQAwgOfEY/3DdWWxvauG7T2+IuhwRyXAKiAFmzrhh3HzuGH7wwmbW7TgQdTkiksEUEAPQl66ayuCCHL7yyCq6NE+TiIREATEADSvO456rz2TZln388vX6qMsRkQylgBigrp8zmppxw/g/S9fSeKA16nJEJAMpIAaoWMz4+ofP5lBrJ19bklUzoYtIH1FADGDVw0v4/OXVLF21g0dXbo+6HBHJMAqIAW7hxROZMXoIX/2f1ew5qENNItJ7FBADXE48xreun0lzSztfW7Im6nJEJIMoIDLAGSNK+Nxl1fx65XZ+s0qHmkSkdyggMsRnPjCJs6uGcM/Dq9jRpOdGiMjpU0BkiNx4jO/ePIvW9i6+8PMVdOoGOhE5TQqIDDKxYhB/+0fTeXnTHhY/p0eUisjpUUBkmBtqRnPN2SP4pyfW8ea2/VGXIyIDWKgBYWZXmdk6M6szs7tTrJ9qZi+bWauZ/WVP+kpqZsbX/3gGlSX5fO7BNzioacFF5BSFFhBmFgcWAVcD04BbzGxat832An8OfPsU+soJDCnK5Z9vmsU7ew/zpf9eqSfQicgpCXMPYi5Q5+6b3L0NeBBYkLyBu+9y92VAe0/7ysmdN7GMv75qKo+u2s4PXtgcdTkiMgCFGRBVwLak5fqgrVf7mtlCM6s1s9rGxsZTKjRTfeaSiVw5fThf/83bvLppT9TliMgAE2ZAWIq2dI91pN3X3Re7e42711RUVKRdXDYwM751w0zGlRZx18/eYFez7o8QkfSFGRD1wJik5dFAQx/0lSSDC3K599Y5HGzp4LMPvE5rR2fUJYnIABFmQCwDqs1sgpnlATcDS/qgr3QzZXgJ37phBrVb9/G/f7VaJ61FJC05YX2xu3eY2V3A40AcuN/d15jZHcH6e81sBFALDAa6zOzzwDR3b07VN6xas8G1M0ZRt+sg//LUBiZVFvPZSydHXZKI9HOhBQSAuy8FlnZruzfp8w4Sh4/S6iun53OXVbOp8RDffGwdE8uLueqskVGXJCL9mO6kziJmxjevn8E5Y4fy+Z+vYGX9/qhLEpF+TAGRZQpy4yy+tYay4nw++cNlbN59KOqSRKSfUkBkoYqSfH58+1wcuPUHr7JTl7+KSAoKiCw1qWIQP7ztXPYeauMT979G05HuN7OLSLZTQGSxmWOG8v1b57Cx8SCf/tEyjrTpHgkROU4BkeUurq7gn2+aRe3WfXz6x8toaVdIiEiCAkK4dsYovn39TF7auIf/9eNahYSIAAoICXxkzmi++ZEZvFC3m4U/Wa6QEBEFhBx3Q80YvvHhGTy3vpGFP1mucxIiWU4BIe9x47lj+MZHzub5DY18/P5XaW7R1U0i2UoBIb/npnPH8q+3nMOKbfu5+fuvsPtga9QliUgEFBCS0rUzRvEfH69h0+6D3Hjvy9TvOxx1SSLSxxQQckKXnlHJT28/j8aDrVy36CXe3LY/6pJEpA8pIOSkasaX8ss/vZD8nBg3LX6Zx1Zvj7okEekjCgh5X1OGl/DInfOYOmIwf/rA63z/2Y166JBIFlBASFoqSvJ5cOH5XHPWSL7+m7e5+5erdK+ESIYL9YFBklkKcuP86y3nMKG8mH97po63tjfzvY/NZvSwoqhLE5EQaA9CeiQWM/7yyjNYfOsctuw+xLX/+gLPrm+MuiwRCYECQk7JFdNHsOTPLmLE4AJu++FrfPepDXR26byESCZRQMgpm1BezK8+eyHXzarin59azy3/8Qrv7j8SdVki0ktCDQgzu8rM1plZnZndnWK9mdn/DdavNLPZSeu2mNkqM1thZrVh1imnrigvh+/cOJNv3zCTNe82cdW/PMeSNxuiLktEekFoAWFmcWARcDUwDbjFzKZ12+xqoDp4LQS+1239fHef5e41YdUpp8/MuH7OaJZ+7mImVw7iz3/2Bl/4+Qo9pU5kgAtzD2IuUOfum9y9DXgQWNBtmwXAjz3hFWComY0MsSYJ0biyYn7xmQv4/IeqWfJmA5d/51keX7Mj6rJE5BSFGRBVwLak5fqgLd1tHHjCzJab2cIT/SFmttDMas2strFRV9NELSce4/MfmsIjn51H2aB8PvOT5Xz2geXsOtASdWki0kNhBoSlaOt+mcvJtpnn7rNJHIa608wuSfWHuPtid69x95qKiopTr1Z61dmjh7Dkrnn81ZVn8NTaXVz+nef42Wvv6EonkQEkzICoB8YkLY8Gup+9POE27n70fRfwMIlDVjKA5MZj3Dl/Mr/53MWcMaKEe361iusWvcjyrfuiLk1E0hBmQCwDqs1sgpnlATcDS7ptswT4eHA10/lAk7tvN7NiMysBMLNi4ApgdYi1SogmVQzi5wvP57s3z2LXgRY+8r2X+OJDK9jVrMNOIv1ZaFNtuHuHmd0FPA7EgfvdfY2Z3RGsvxdYClwD1AGHgU8G3YcDD5vZ0Rr/y90fC6tWCZ+ZsWBWFR86czj/9kwdP3h+M4+t3sGnL5rApy+ZyOCC3KhLFJFuLJNm5aypqfHaWt0yMRBs2X2Ibz2xjkdXbmdYUS53zp/Mx84fR0FuPOrSRLKKmS0/0a0ECgiJ1Kr6Jr75+Ns8v2E3o4YU8GeXVfPh2VXk5ygoRPqCAkL6vZfqdvONx9fx5rb9jBhcwMJLJnLz3DEU5WnCYZEwKSBkQHB3nt+wm0XP1PHq5r2UFufxqXnjufX88Qwp0jkKkTAoIGTAqd2yl0XP1PHMukYKc+N8eHYVt104nurhJVGXJpJRFBAyYL3V0Mx/vrSZR1Y00NbRxUWTy7ntwvHMn1pJPJbqPksR6QkFhAx4ew+18bPX3uEnL29lR3MLI4cUcP2c0dwwZwxjy/REO5FTpYCQjNHe2cWTb+3kodptPLe+kS6H8yeWcmPNGK4+aySFebr6SaQnFBCSkbY3HeGXy+t5qLaed/YepjgvzuXThnPtjFFcPKVcl8qKpEEBIRmtq8t5dfNeHnnjXR5bs4OmI+2UFORw5fQRXDtjJBdOKicvRw9PFElFASFZo62jixfrdvP/Vjbw5JqdHGjtoCQ/h0umVHDZmZVcekYlpcV5UZcp0m+cLCB0F5JklLycGPOnVjJ/aiUt7Z08v2E3T6/dydNv7+LRVduJGcweO4wPnlnJB6ZUcOaIwcR0NZRIStqDkKzQ1eWsbmjiqbW7eHrtTtY0NANQWpzHBRPLuHByGfMmlTOurIhgkkiRrKBDTCLd7Ghq4cW63by4cTcv1e1hRzD1eNXQQs6bWMqcccOoGVdKdeUg7WFIRlNAiJyEu7Np9yFeqtvNi3V7qN26l90H2wAoKcjhnLHDmDN2GHPGDePs0UMYUqhpPyRzKCBEesDdeWfvYWq37GP5O/t4fes+1u08wNF/KmNLizirajDTRw3hrKohTB81mPJB+dEWLXKKdJJapAfMjHFlxYwrK+Yjc0YD0NzSzop39rO6oYk17zazuqGJpat2HOszckgBU0eUMGV4CZMrB1EdvA/K1z8xGbj0t1ckDYMLcrlkSgWXTKk41tZ0pJ23GppZ09DEqnebWLfjAC/W7aGts+vYNlVDCxOBUTmIiRWDGFdWxNjSIkYNLdRcUtLvKSBETtGQwlwumFTGBZPKjrV1dHbxzt7DbNh1kLpdB9mw8wDrdx7klU17aO04Hhy5cWPMsCLGlhUxvqyYsaVFjCsrYuSQQkYNLWBIYa6uppLIKSBEelFOPMbEisTewpXTj7d3djk7mlvYuvsQW/ceZuuew2zdc4itexLnOg62drzne4ry4owYUsCoIYWMHFLAyKGFjAreK0vyqSjJZ1hRnvZCJFQKCJE+EI8ZVUMLqRpayIXd1rk7ew+1sXXvYbbvb2F70xEajr43tbB+fSONB1vpfj1JzKBsUD7lg/IpH5RHRUk+FYMS4VE+KJ/S4jyGFeUxtCiXoUW5DMrP0V6J9IgCQiRiZkbZoHzKBuXD2NTbtHV0sbO5hR3NLTQeaKXxQCu7D773fVPjIRoPtL7nHEiynJgxtCiXIYW5ScGRx9DCRICUFCRCZFBBDiXB+9HlwQW55OfEFDBZJtSAMLOrgO8CceA+d//HbustWH8NcBi4zd1fT6evSDbJy4kxprSIMaUnf/aFu9Pc0kHjgVb2HW5j/+F29h1uoyl433+knf1B+7v7W3iroZl9h9s50t75vjXkxOx4aOTnUFKQQ1FeDoW5cQrz4hTkxinKi79nuTBoKwjakpcLcmPk5cTIj8fJz42RF4/ppsR+JrSAMLM4sAi4HKgHlpnZEnd/K2mzq4Hq4HUe8D3gvDT7ikg3ZsaQwtwe38zX2tHJwZYODrZ2cCB4P7Z87HM7B1sS6w+0dnCgJRE229s7OdLeyZG2LlraOznc1kHXKd5elRMz8nISwZEXDwIkJ0ZeTjwIk9ix9flJ2+XEY+TEjJy4Be/Bcix2rC0eM3Ljx5ePr4sF645vc3w5RtyMWAxillgfs8Tno8tmiUOIcTMseZtYsI0d3ybRjwGzJxbmHsRcoM7dNwGY2YPAAiD5P/kFwI89cbfeK2Y21MxGAuPT6CsivSQ/J07+oHjiMNdpcnfaOrtoaeviSBAYR9o7aQlC5Ghba3sXrZ1dtLZ30tbZRVtH0itYbu1Ieu/soq0j0Xff4fdu19HldHQefXc6u5z2rq7fO2/TX5iRCJ7k8EkKkkTwGAbvCRULwunYe/BdZcX5PHTHBb1eZ5gBUQVsS1quJ7GX8H7bVKXZFwAzWwgsBBg79gQHcEWkz5hZInBy4gwh2mlJOrucjq4uOjr9WIgkwsPp7EyESGeX0360PQiXY2HT1UVnF3S509XldDl0uuOe2K6zy/Gg7T3bdAXLfnw50Yek9sTy0e/qcoK24+vdwZ1j3+P4seVj70BJSDdkhhkQqfahuuf5ibZJp2+i0X0xsBgSU230pEARyWyJ38bj6Ib2UxPmj60eGJO0PBpoSHObvDT6iohIiMJ8DuMyoNrMJphZHnAzsKTbNkuAj1vC+UCTu29Ps6+IiIQotD0Id+8ws7uAx0lcqnq/u68xszuC9fcCS0lc4lpH4jLXT56sb1i1iojI79N03yIiWexk032HeYhJREQGMAWEiIikpIAQEZGUFBAiIpJSRp2kNrNGYOspdi8HdvdiOQOBxpwdNObscKpjHufuFalWZFRAnA4zqz3RmfxMpTFnB405O4QxZh1iEhGRlBQQIiKSkgLiuMVRFxABjTk7aMzZodfHrHMQIiKSkvYgREQkJQWEiIiklPUBYWZXmdk6M6szs7ujrqe3mNn9ZrbLzFYntZWa2ZNmtiF4H5a07p7gZ7DOzK6MpurTY2ZjzOwZM1trZmvM7HNBe8aO28wKzOw1M3szGPPfBu0ZO+ajzCxuZm+Y2a+D5Ywes5ltMbNVZrbCzGqDtnDH7MHj87LxRWIq8Y3ARBIPKXoTmBZ1Xb00tkuA2cDqpLZvAncHn+8GvhF8nhaMPR+YEPxM4lGP4RTGPBKYHXwuAdYHY8vYcZN4+uKg4HMu8CpwfiaPOWnsXwT+C/h1sJzRYwa2AOXd2kIdc7bvQcwF6tx9k7u3AQ8CCyKuqVe4+3PA3m7NC4AfBZ9/BFyX1P6gu7e6+2YSz+eY2xd19iZ33+7urwefDwBrSTzfPGPH7QkHg8Xc4OVk8JgBzGw08AfAfUnNGT3mEwh1zNkeEFXAtqTl+qAtUw33xBP7CN4rg/aM+zmY2XjgHBK/UWf0uINDLSuAXcCT7p7xYwb+BfhroCupLdPH7MATZrbczBYGbaGOOdsf5W0p2rLxut+M+jmY2SDgl8Dn3b3ZLNXwEpumaBtw43b3TmCWmQ0FHjazs06y+YAfs5ldC+xy9+Vmdmk6XVK0DagxB+a5e4OZVQJPmtnbJ9m2V8ac7XsQ9cCYpOXRQENEtfSFnWY2EiB43xW0Z8zPwcxySYTDA+7+q6A548cN4O77gd8BV5HZY54H/JGZbSFxWPiDZvZTMnvMuHtD8L4LeJjEIaNQx5ztAbEMqDazCWaWB9wMLIm4pjAtAT4RfP4E8D9J7TebWb6ZTQCqgdciqO+0WGJX4QfAWnf/TtKqjB23mVUEew6YWSHwIeBtMnjM7n6Pu4929/Ek/s3+1t0/RgaP2cyKzazk6GfgCmA1YY856jPzUb+Aa0hc7bIR+HLU9fTiuH4GbAfaSfw2cTtQBjwNbAjeS5O2/3LwM1gHXB11/ac45otI7EavBFYEr2syedzADOCNYMyrga8G7Rk75m7jv5TjVzFl7JhJXGn5ZvBac/T/qrDHrKk2REQkpWw/xCQiIieggBARkZQUECIikpICQkREUlJAiIhISgoIkX7AzC49OiupSH+hgBARkZQUECI9YGYfC56/sMLMvh9MlHfQzP7JzF43s6fNrCLYdpaZvWJmK83s4aNz9ZvZZDN7KniGw+tmNin4+kFm9t9m9raZPWAnmURKpC8oIETSZGZnAjeRmDRtFtAJfBQoBl5399nAs8DXgi4/Br7k7jOAVUntDwCL3H0mcCGJO94hMfvs50nM5T+RxJxDIpHJ9tlcRXriMmAOsCz45b6QxORoXcDPg21+CvzKzIYAQ9392aD9R8Avgvl0qtz9YQB3bwEIvu81d68PllcA44EXQh+VyAkoIETSZ8CP3P2e9zSa/U237U42f83JDhu1Jn3uRP8+JWI6xCSSvqeB64P5+I8+D3gciX9H1wfb/Anwgrs3AfvM7OKg/VbgWXdvBurN7LrgO/LNrKgvByGSLv2GIpImd3/LzL5C4qleMRIz5d4JHAKmm9lyoInEeQpITL98bxAAm4BPBu23At83s78LvuOGPhyGSNo0m6vIaTKzg+4+KOo6RHqbDjGJiEhK2oMQEZGUtAchIiIpKSBERCQlBYSIiKSkgBARkZQUECIiktL/Bzn+4XAerMy6AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epoch_list, cost_list)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练前： 4 tensor([4.], grad_fn=<MulBackward0>)\n",
      "w.data tensor([1.]) w.grad.data tensor([-2.])\n",
      "w.data tensor([1.0200]) w.grad.data tensor([-7.8400])\n",
      "w.data tensor([1.0984]) w.grad.data tensor([-16.2288])\n",
      "0 7.315943717956543\n",
      "w.data tensor([1.2607]) w.grad.data tensor([-1.4786])\n",
      "w.data tensor([1.2755]) w.grad.data tensor([-5.7962])\n",
      "w.data tensor([1.3334]) w.grad.data tensor([-11.9981])\n",
      "1 3.9987640380859375\n",
      "w.data tensor([1.4534]) w.grad.data tensor([-1.0932])\n",
      "w.data tensor([1.4643]) w.grad.data tensor([-4.2852])\n",
      "w.data tensor([1.5072]) w.grad.data tensor([-8.8704])\n",
      "2 2.1856532096862793\n",
      "w.data tensor([1.5959]) w.grad.data tensor([-0.8082])\n",
      "w.data tensor([1.6040]) w.grad.data tensor([-3.1681])\n",
      "w.data tensor([1.6357]) w.grad.data tensor([-6.5580])\n",
      "3 1.1946394443511963\n",
      "w.data tensor([1.7012]) w.grad.data tensor([-0.5975])\n",
      "w.data tensor([1.7072]) w.grad.data tensor([-2.3422])\n",
      "w.data tensor([1.7306]) w.grad.data tensor([-4.8484])\n",
      "4 0.6529689431190491\n",
      "w.data tensor([1.7791]) w.grad.data tensor([-0.4417])\n",
      "w.data tensor([1.7835]) w.grad.data tensor([-1.7316])\n",
      "w.data tensor([1.8009]) w.grad.data tensor([-3.5845])\n",
      "5 0.35690122842788696\n",
      "w.data tensor([1.8367]) w.grad.data tensor([-0.3266])\n",
      "w.data tensor([1.8400]) w.grad.data tensor([-1.2802])\n",
      "w.data tensor([1.8528]) w.grad.data tensor([-2.6500])\n",
      "6 0.195076122879982\n",
      "w.data tensor([1.8793]) w.grad.data tensor([-0.2414])\n",
      "w.data tensor([1.8817]) w.grad.data tensor([-0.9465])\n",
      "w.data tensor([1.8912]) w.grad.data tensor([-1.9592])\n",
      "7 0.10662525147199631\n",
      "w.data tensor([1.9107]) w.grad.data tensor([-0.1785])\n",
      "w.data tensor([1.9125]) w.grad.data tensor([-0.6997])\n",
      "w.data tensor([1.9195]) w.grad.data tensor([-1.4485])\n",
      "8 0.0582793727517128\n",
      "w.data tensor([1.9340]) w.grad.data tensor([-0.1320])\n",
      "w.data tensor([1.9353]) w.grad.data tensor([-0.5173])\n",
      "w.data tensor([1.9405]) w.grad.data tensor([-1.0709])\n",
      "9 0.03185431286692619\n",
      "w.data tensor([1.9512]) w.grad.data tensor([-0.0976])\n",
      "w.data tensor([1.9522]) w.grad.data tensor([-0.3825])\n",
      "w.data tensor([1.9560]) w.grad.data tensor([-0.7917])\n",
      "10 0.017410902306437492\n",
      "w.data tensor([1.9639]) w.grad.data tensor([-0.0721])\n",
      "w.data tensor([1.9647]) w.grad.data tensor([-0.2828])\n",
      "w.data tensor([1.9675]) w.grad.data tensor([-0.5853])\n",
      "11 0.009516451507806778\n",
      "w.data tensor([1.9733]) w.grad.data tensor([-0.0533])\n",
      "w.data tensor([1.9739]) w.grad.data tensor([-0.2090])\n",
      "w.data tensor([1.9760]) w.grad.data tensor([-0.4327])\n",
      "12 0.005201528314501047\n",
      "w.data tensor([1.9803]) w.grad.data tensor([-0.0394])\n",
      "w.data tensor([1.9807]) w.grad.data tensor([-0.1546])\n",
      "w.data tensor([1.9822]) w.grad.data tensor([-0.3199])\n",
      "13 0.0028430151287466288\n",
      "w.data tensor([1.9854]) w.grad.data tensor([-0.0291])\n",
      "w.data tensor([1.9857]) w.grad.data tensor([-0.1143])\n",
      "w.data tensor([1.9869]) w.grad.data tensor([-0.2365])\n",
      "14 0.0015539465239271522\n",
      "w.data tensor([1.9892]) w.grad.data tensor([-0.0215])\n",
      "w.data tensor([1.9894]) w.grad.data tensor([-0.0845])\n",
      "w.data tensor([1.9903]) w.grad.data tensor([-0.1749])\n",
      "15 0.0008493617060594261\n",
      "w.data tensor([1.9920]) w.grad.data tensor([-0.0159])\n",
      "w.data tensor([1.9922]) w.grad.data tensor([-0.0625])\n",
      "w.data tensor([1.9928]) w.grad.data tensor([-0.1293])\n",
      "16 0.00046424579340964556\n",
      "w.data tensor([1.9941]) w.grad.data tensor([-0.0118])\n",
      "w.data tensor([1.9942]) w.grad.data tensor([-0.0462])\n",
      "w.data tensor([1.9947]) w.grad.data tensor([-0.0956])\n",
      "17 0.0002537401160225272\n",
      "w.data tensor([1.9956]) w.grad.data tensor([-0.0087])\n",
      "w.data tensor([1.9957]) w.grad.data tensor([-0.0341])\n",
      "w.data tensor([1.9961]) w.grad.data tensor([-0.0707])\n",
      "18 0.00013869594840798527\n",
      "w.data tensor([1.9968]) w.grad.data tensor([-0.0064])\n",
      "w.data tensor([1.9968]) w.grad.data tensor([-0.0252])\n",
      "w.data tensor([1.9971]) w.grad.data tensor([-0.0522])\n",
      "19 7.580435340059921e-05\n",
      "w.data tensor([1.9976]) w.grad.data tensor([-0.0048])\n",
      "w.data tensor([1.9977]) w.grad.data tensor([-0.0187])\n",
      "w.data tensor([1.9979]) w.grad.data tensor([-0.0386])\n",
      "20 4.143271507928148e-05\n",
      "w.data tensor([1.9982]) w.grad.data tensor([-0.0035])\n",
      "w.data tensor([1.9983]) w.grad.data tensor([-0.0138])\n",
      "w.data tensor([1.9984]) w.grad.data tensor([-0.0286])\n",
      "21 2.264650902361609e-05\n",
      "w.data tensor([1.9987]) w.grad.data tensor([-0.0026])\n",
      "w.data tensor([1.9987]) w.grad.data tensor([-0.0102])\n",
      "w.data tensor([1.9988]) w.grad.data tensor([-0.0211])\n",
      "22 1.2377059647405986e-05\n",
      "w.data tensor([1.9990]) w.grad.data tensor([-0.0019])\n",
      "w.data tensor([1.9991]) w.grad.data tensor([-0.0075])\n",
      "w.data tensor([1.9991]) w.grad.data tensor([-0.0156])\n",
      "23 6.768445018678904e-06\n",
      "w.data tensor([1.9993]) w.grad.data tensor([-0.0014])\n",
      "w.data tensor([1.9993]) w.grad.data tensor([-0.0056])\n",
      "w.data tensor([1.9994]) w.grad.data tensor([-0.0115])\n",
      "24 3.7000872907810844e-06\n",
      "w.data tensor([1.9995]) w.grad.data tensor([-0.0011])\n",
      "w.data tensor([1.9995]) w.grad.data tensor([-0.0041])\n",
      "w.data tensor([1.9995]) w.grad.data tensor([-0.0085])\n",
      "25 2.021880391112063e-06\n",
      "w.data tensor([1.9996]) w.grad.data tensor([-0.0008])\n",
      "w.data tensor([1.9996]) w.grad.data tensor([-0.0030])\n",
      "w.data tensor([1.9996]) w.grad.data tensor([-0.0063])\n",
      "26 1.1044940038118511e-06\n",
      "w.data tensor([1.9997]) w.grad.data tensor([-0.0006])\n",
      "w.data tensor([1.9997]) w.grad.data tensor([-0.0023])\n",
      "w.data tensor([1.9997]) w.grad.data tensor([-0.0047])\n",
      "27 6.041091182851233e-07\n",
      "w.data tensor([1.9998]) w.grad.data tensor([-0.0004])\n",
      "w.data tensor([1.9998]) w.grad.data tensor([-0.0017])\n",
      "w.data tensor([1.9998]) w.grad.data tensor([-0.0034])\n",
      "28 3.296045179013163e-07\n",
      "w.data tensor([1.9998]) w.grad.data tensor([-0.0003])\n",
      "w.data tensor([1.9998]) w.grad.data tensor([-0.0012])\n",
      "w.data tensor([1.9999]) w.grad.data tensor([-0.0025])\n",
      "29 1.805076408345485e-07\n",
      "w.data tensor([1.9999]) w.grad.data tensor([-0.0002])\n",
      "w.data tensor([1.9999]) w.grad.data tensor([-0.0009])\n",
      "w.data tensor([1.9999]) w.grad.data tensor([-0.0019])\n",
      "30 9.874406714516226e-08\n",
      "w.data tensor([1.9999]) w.grad.data tensor([-0.0002])\n",
      "w.data tensor([1.9999]) w.grad.data tensor([-0.0007])\n",
      "w.data tensor([1.9999]) w.grad.data tensor([-0.0014])\n",
      "31 5.4147676564753056e-08\n",
      "w.data tensor([1.9999]) w.grad.data tensor([-0.0001])\n",
      "w.data tensor([1.9999]) w.grad.data tensor([-0.0005])\n",
      "w.data tensor([1.9999]) w.grad.data tensor([-0.0010])\n",
      "32 2.9467628337442875e-08\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-9.3937e-05])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-0.0004])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-0.0008])\n",
      "33 1.6088051779661328e-08\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-6.9380e-05])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-0.0003])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-0.0006])\n",
      "34 8.734787115827203e-09\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.1260e-05])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-0.0002])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-0.0004])\n",
      "35 4.8466972657479346e-09\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-3.7909e-05])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-0.0001])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-0.0003])\n",
      "36 2.6520865503698587e-09\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8133e-05])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-0.0001])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-0.0002])\n",
      "37 1.4551915228366852e-09\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.0981e-05])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-8.2016e-05])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-0.0002])\n",
      "38 7.914877642178908e-10\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-1.5497e-05])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-6.1035e-05])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-0.0001])\n",
      "39 4.4019543565809727e-10\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-1.1444e-05])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-4.4823e-05])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-9.1553e-05])\n",
      "40 2.3283064365386963e-10\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-8.3447e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-3.2425e-05])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-6.5804e-05])\n",
      "41 1.2028067430946976e-10\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.9605e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.2888e-05])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-4.5776e-05])\n",
      "42 5.820766091346741e-11\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-4.2915e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-1.7166e-05])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-3.7193e-05])\n",
      "43 3.842615114990622e-11\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-3.3379e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-1.3351e-05])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-05])\n",
      "44 2.2737367544323206e-11\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.6226e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-1.0490e-05])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.2888e-05])\n",
      "45 1.4551915228366852e-11\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-1.9073e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.6294e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-1.4305e-05])\n",
      "46 5.6843418860808015e-12\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-1.4305e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-1.1444e-05])\n",
      "47 3.637978807091713e-12\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-1.1921e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-4.7684e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-1.1444e-05])\n",
      "48 3.637978807091713e-12\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-9.5367e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-3.8147e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-8.5831e-06])\n",
      "49 2.0463630789890885e-12\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "50 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "51 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "52 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "53 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "54 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "55 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "56 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "57 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "58 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "59 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "60 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "61 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "62 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "63 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "64 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "65 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "66 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "67 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "68 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "69 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "70 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "71 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "72 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "73 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "74 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "75 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "76 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "77 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "78 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "79 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "80 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "81 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "82 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "83 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "84 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "85 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "86 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "87 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "88 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "89 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "90 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "91 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "92 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "93 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "94 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "95 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "96 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "97 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "98 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "99 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "100 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "101 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "102 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "103 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "104 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "105 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "106 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "107 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "108 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "109 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "110 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "111 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "112 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "113 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "114 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "115 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "116 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "117 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "118 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "119 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "120 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "121 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "122 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "123 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "124 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "125 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "126 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "127 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "128 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "129 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "130 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "131 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "132 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "133 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "134 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "135 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "136 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "137 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "138 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "139 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "140 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "141 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "142 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "143 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "144 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "145 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "146 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "147 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "148 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "149 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "150 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "151 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "152 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "153 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "154 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "155 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "156 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "157 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "158 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "159 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "160 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "161 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "162 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "163 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "164 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "165 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "166 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "167 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "168 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "169 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "170 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "171 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "172 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "173 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "174 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "175 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "176 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "177 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "178 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "179 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "180 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "181 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "182 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "183 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "184 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "185 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "186 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "187 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "188 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "189 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "190 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "191 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "192 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "193 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "194 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "195 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "196 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "197 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "198 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "199 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "200 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "201 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "202 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "203 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "204 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "205 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "206 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "207 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "208 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "209 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "210 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "211 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "212 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "213 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "214 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "215 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "216 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "217 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "218 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "219 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "220 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "221 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "222 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "223 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "224 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "225 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "226 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "227 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "228 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "229 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "230 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "231 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "232 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "233 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "234 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "235 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "236 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "237 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "238 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "239 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "240 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "241 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "242 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "243 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "244 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "245 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "246 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "247 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "248 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "249 9.094947017729282e-13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "250 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "251 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "252 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "253 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "254 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "255 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "256 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "257 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "258 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "259 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "260 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "261 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "262 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "263 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "264 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "265 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "266 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "267 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "268 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "269 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "270 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "271 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "272 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "273 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "274 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "275 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "276 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "277 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "278 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "279 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "280 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "281 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "282 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "283 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "284 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "285 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "286 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "287 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "288 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "289 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "290 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "291 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "292 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "293 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "294 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "295 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "296 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "297 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "298 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "299 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "300 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "301 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "302 9.094947017729282e-13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "303 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "304 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "305 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "306 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "307 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "308 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "309 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "310 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "311 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "312 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "313 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "314 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "315 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "316 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "317 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "318 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "319 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "320 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "321 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "322 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "323 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "324 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "325 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "326 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "327 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "328 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "329 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "330 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "331 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "332 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "333 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "334 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "335 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "336 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "337 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "338 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "339 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "340 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "341 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "342 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "343 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "344 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "345 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "346 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "347 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "348 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "349 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "350 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "351 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "352 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "353 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "354 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "355 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "356 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "357 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "358 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "359 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "360 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "361 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "362 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "363 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "364 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "365 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "366 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "367 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "368 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "369 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "370 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "371 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "372 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "373 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "374 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "375 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "376 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "377 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "378 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "379 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "380 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "381 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "382 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "383 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "384 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "385 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "386 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "387 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "388 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "389 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "390 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "391 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "392 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "393 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "394 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "395 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "396 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "397 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "398 9.094947017729282e-13\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-7.1526e-07])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-2.8610e-06])\n",
      "w.data tensor([2.0000]) w.grad.data tensor([-5.7220e-06])\n",
      "399 9.094947017729282e-13\n",
      "4 7.999998569488525\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]\n",
    "w = torch.Tensor([1.0])\n",
    "w.requires_grad = True\n",
    "\n",
    "def forward(x):\n",
    "    return x * w\n",
    "\n",
    "def loss(x, y):\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred - y) ** 2\n",
    "\n",
    "print('训练前：', 4, forward(4))\n",
    "\n",
    "for epoch in range(400):\n",
    "    for x, y in zip(x_data, y_data):\n",
    "        l = loss(x, y)\n",
    "        l.backward()\n",
    "        # print(x, y, w.grad.item())\n",
    "        print(\"w.data\",w.data, \"w.grad.data\",  w.grad.data)\n",
    "        w.data = w.data - 0.01 * w.grad.data\n",
    "        w.grad.data.zero_()\n",
    "    print(epoch, l.item())\n",
    "\n",
    "print(4, forward(4).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.用Pytorch实现线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y_{}^{'} = wx + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基本流程：\n",
    "\n",
    "1. Prepare dataset\n",
    "\n",
    "2. Design model using Class\n",
    "\n",
    "3. Contruct loss and optimizer\n",
    "\n",
    "4. Training cycle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : tensor(9.8005, grad_fn=<MseLossBackward>)\n",
      "w= 0.8128864765167236 \t b= -0.2351824939250946\n",
      "y_pred= tensor([3.8292])\n",
      "1 : tensor(7.7485, grad_fn=<MseLossBackward>)\n",
      "w= 0.9330910444259644 \t b= -0.1829943060874939\n",
      "y_pred= tensor([4.4825])\n",
      "2 : tensor(6.1265, grad_fn=<MseLossBackward>)\n",
      "w= 1.0399889945983887 \t b= -0.13665805757045746\n",
      "y_pred= tensor([5.0633])\n",
      "3 : tensor(4.8443, grad_fn=<MseLossBackward>)\n",
      "w= 1.1350563764572144 \t b= -0.09552445262670517\n",
      "y_pred= tensor([5.5798])\n",
      "4 : tensor(3.8309, grad_fn=<MseLossBackward>)\n",
      "w= 1.2196054458618164 \t b= -0.05901621654629707\n",
      "y_pred= tensor([6.0390])\n",
      "5 : tensor(3.0298, grad_fn=<MseLossBackward>)\n",
      "w= 1.2948029041290283 \t b= -0.026620112359523773\n",
      "y_pred= tensor([6.4474])\n",
      "6 : tensor(2.3965, grad_fn=<MseLossBackward>)\n",
      "w= 1.361686110496521 \t b= 0.002120174467563629\n",
      "y_pred= tensor([6.8106])\n",
      "7 : tensor(1.8960, grad_fn=<MseLossBackward>)\n",
      "w= 1.4211772680282593 \t b= 0.027610329911112785\n",
      "y_pred= tensor([7.1335])\n",
      "8 : tensor(1.5003, grad_fn=<MseLossBackward>)\n",
      "w= 1.4740962982177734 \t b= 0.05021103471517563\n",
      "y_pred= tensor([7.4207])\n",
      "9 : tensor(1.1876, grad_fn=<MseLossBackward>)\n",
      "w= 1.5211721658706665 \t b= 0.07024296373128891\n",
      "y_pred= tensor([7.6761])\n",
      "10 : tensor(0.9404, grad_fn=<MseLossBackward>)\n",
      "w= 1.563053011894226 \t b= 0.08799121528863907\n",
      "y_pred= tensor([7.9033])\n",
      "11 : tensor(0.7449, grad_fn=<MseLossBackward>)\n",
      "w= 1.6003150939941406 \t b= 0.10370927304029465\n",
      "y_pred= tensor([8.1053])\n",
      "12 : tensor(0.5904, grad_fn=<MseLossBackward>)\n",
      "w= 1.6334706544876099 \t b= 0.1176224872469902\n",
      "y_pred= tensor([8.2850])\n",
      "13 : tensor(0.4683, grad_fn=<MseLossBackward>)\n",
      "w= 1.6629751920700073 \t b= 0.12993121147155762\n",
      "y_pred= tensor([8.4448])\n",
      "14 : tensor(0.3718, grad_fn=<MseLossBackward>)\n",
      "w= 1.6892335414886475 \t b= 0.14081358909606934\n",
      "y_pred= tensor([8.5870])\n",
      "15 : tensor(0.2955, grad_fn=<MseLossBackward>)\n",
      "w= 1.7126058340072632 \t b= 0.15042798221111298\n",
      "y_pred= tensor([8.7135])\n",
      "16 : tensor(0.2351, grad_fn=<MseLossBackward>)\n",
      "w= 1.7334121465682983 \t b= 0.1589151918888092\n",
      "y_pred= tensor([8.8260])\n",
      "17 : tensor(0.1875, grad_fn=<MseLossBackward>)\n",
      "w= 1.7519370317459106 \t b= 0.16640040278434753\n",
      "y_pred= tensor([8.9261])\n",
      "18 : tensor(0.1497, grad_fn=<MseLossBackward>)\n",
      "w= 1.7684335708618164 \t b= 0.17299491167068481\n",
      "y_pred= tensor([9.0152])\n",
      "19 : tensor(0.1199, grad_fn=<MseLossBackward>)\n",
      "w= 1.7831265926361084 \t b= 0.1787976771593094\n",
      "y_pred= tensor([9.0944])\n",
      "20 : tensor(0.0964, grad_fn=<MseLossBackward>)\n",
      "w= 1.7962162494659424 \t b= 0.18389666080474854\n",
      "y_pred= tensor([9.1650])\n",
      "21 : tensor(0.0777, grad_fn=<MseLossBackward>)\n",
      "w= 1.807880163192749 \t b= 0.18837007880210876\n",
      "y_pred= tensor([9.2278])\n",
      "22 : tensor(0.0630, grad_fn=<MseLossBackward>)\n",
      "w= 1.8182765245437622 \t b= 0.19228747487068176\n",
      "y_pred= tensor([9.2837])\n",
      "23 : tensor(0.0513, grad_fn=<MseLossBackward>)\n",
      "w= 1.8275458812713623 \t b= 0.1957106590270996\n",
      "y_pred= tensor([9.3334])\n",
      "24 : tensor(0.0421, grad_fn=<MseLossBackward>)\n",
      "w= 1.8358131647109985 \t b= 0.1986946165561676\n",
      "y_pred= tensor([9.3778])\n",
      "25 : tensor(0.0348, grad_fn=<MseLossBackward>)\n",
      "w= 1.8431894779205322 \t b= 0.20128819346427917\n",
      "y_pred= tensor([9.4172])\n",
      "26 : tensor(0.0290, grad_fn=<MseLossBackward>)\n",
      "w= 1.849773645401001 \t b= 0.20353484153747559\n",
      "y_pred= tensor([9.4524])\n",
      "27 : tensor(0.0244, grad_fn=<MseLossBackward>)\n",
      "w= 1.8556534051895142 \t b= 0.20547319948673248\n",
      "y_pred= tensor([9.4837])\n",
      "28 : tensor(0.0208, grad_fn=<MseLossBackward>)\n",
      "w= 1.8609068393707275 \t b= 0.2071375995874405\n",
      "y_pred= tensor([9.5117])\n",
      "29 : tensor(0.0179, grad_fn=<MseLossBackward>)\n",
      "w= 1.8656033277511597 \t b= 0.2085585743188858\n",
      "y_pred= tensor([9.5366])\n",
      "30 : tensor(0.0157, grad_fn=<MseLossBackward>)\n",
      "w= 1.8698046207427979 \t b= 0.2097632735967636\n",
      "y_pred= tensor([9.5588])\n",
      "31 : tensor(0.0139, grad_fn=<MseLossBackward>)\n",
      "w= 1.873565673828125 \t b= 0.21077582240104675\n",
      "y_pred= tensor([9.5786])\n",
      "32 : tensor(0.0124, grad_fn=<MseLossBackward>)\n",
      "w= 1.8769351243972778 \t b= 0.21161767840385437\n",
      "y_pred= tensor([9.5963])\n",
      "33 : tensor(0.0113, grad_fn=<MseLossBackward>)\n",
      "w= 1.8799564838409424 \t b= 0.2123079150915146\n",
      "y_pred= tensor([9.6121])\n",
      "34 : tensor(0.0104, grad_fn=<MseLossBackward>)\n",
      "w= 1.8826682567596436 \t b= 0.212863489985466\n",
      "y_pred= tensor([9.6262])\n",
      "35 : tensor(0.0097, grad_fn=<MseLossBackward>)\n",
      "w= 1.8851046562194824 \t b= 0.21329949796199799\n",
      "y_pred= tensor([9.6388])\n",
      "36 : tensor(0.0091, grad_fn=<MseLossBackward>)\n",
      "w= 1.887296199798584 \t b= 0.2136293202638626\n",
      "y_pred= tensor([9.6501])\n",
      "37 : tensor(0.0086, grad_fn=<MseLossBackward>)\n",
      "w= 1.8892700672149658 \t b= 0.21386489272117615\n",
      "y_pred= tensor([9.6602])\n",
      "38 : tensor(0.0082, grad_fn=<MseLossBackward>)\n",
      "w= 1.8910502195358276 \t b= 0.21401679515838623\n",
      "y_pred= tensor([9.6693])\n",
      "39 : tensor(0.0079, grad_fn=<MseLossBackward>)\n",
      "w= 1.8926582336425781 \t b= 0.21409444510936737\n",
      "y_pred= tensor([9.6774])\n",
      "40 : tensor(0.0077, grad_fn=<MseLossBackward>)\n",
      "w= 1.8941130638122559 \t b= 0.2141062319278717\n",
      "y_pred= tensor([9.6847])\n",
      "41 : tensor(0.0075, grad_fn=<MseLossBackward>)\n",
      "w= 1.895431637763977 \t b= 0.21405959129333496\n",
      "y_pred= tensor([9.6912])\n",
      "42 : tensor(0.0073, grad_fn=<MseLossBackward>)\n",
      "w= 1.896628975868225 \t b= 0.2139611393213272\n",
      "y_pred= tensor([9.6971])\n",
      "43 : tensor(0.0072, grad_fn=<MseLossBackward>)\n",
      "w= 1.8977184295654297 \t b= 0.21381676197052002\n",
      "y_pred= tensor([9.7024])\n",
      "44 : tensor(0.0071, grad_fn=<MseLossBackward>)\n",
      "w= 1.8987120389938354 \t b= 0.21363168954849243\n",
      "y_pred= tensor([9.7072])\n",
      "45 : tensor(0.0070, grad_fn=<MseLossBackward>)\n",
      "w= 1.8996202945709229 \t b= 0.21341057121753693\n",
      "y_pred= tensor([9.7115])\n",
      "46 : tensor(0.0069, grad_fn=<MseLossBackward>)\n",
      "w= 1.9004526138305664 \t b= 0.2131575495004654\n",
      "y_pred= tensor([9.7154])\n",
      "47 : tensor(0.0068, grad_fn=<MseLossBackward>)\n",
      "w= 1.9012173414230347 \t b= 0.21287629008293152\n",
      "y_pred= tensor([9.7190])\n",
      "48 : tensor(0.0067, grad_fn=<MseLossBackward>)\n",
      "w= 1.9019219875335693 \t b= 0.21257007122039795\n",
      "y_pred= tensor([9.7222])\n",
      "49 : tensor(0.0067, grad_fn=<MseLossBackward>)\n",
      "w= 1.9025731086730957 \t b= 0.2122417837381363\n",
      "y_pred= tensor([9.7251])\n",
      "50 : tensor(0.0066, grad_fn=<MseLossBackward>)\n",
      "w= 1.9031766653060913 \t b= 0.21189402043819427\n",
      "y_pred= tensor([9.7278])\n",
      "51 : tensor(0.0066, grad_fn=<MseLossBackward>)\n",
      "w= 1.9037377834320068 \t b= 0.21152907609939575\n",
      "y_pred= tensor([9.7302])\n",
      "52 : tensor(0.0065, grad_fn=<MseLossBackward>)\n",
      "w= 1.9042611122131348 \t b= 0.21114897727966309\n",
      "y_pred= tensor([9.7325])\n",
      "53 : tensor(0.0065, grad_fn=<MseLossBackward>)\n",
      "w= 1.9047508239746094 \t b= 0.21075555682182312\n",
      "y_pred= tensor([9.7345])\n",
      "54 : tensor(0.0065, grad_fn=<MseLossBackward>)\n",
      "w= 1.9052104949951172 \t b= 0.2103504091501236\n",
      "y_pred= tensor([9.7364])\n",
      "55 : tensor(0.0064, grad_fn=<MseLossBackward>)\n",
      "w= 1.9056434631347656 \t b= 0.20993497967720032\n",
      "y_pred= tensor([9.7382])\n",
      "56 : tensor(0.0064, grad_fn=<MseLossBackward>)\n",
      "w= 1.9060527086257935 \t b= 0.20951053500175476\n",
      "y_pred= tensor([9.7398])\n",
      "57 : tensor(0.0064, grad_fn=<MseLossBackward>)\n",
      "w= 1.9064407348632812 \t b= 0.20907822251319885\n",
      "y_pred= tensor([9.7413])\n",
      "58 : tensor(0.0063, grad_fn=<MseLossBackward>)\n",
      "w= 1.9068098068237305 \t b= 0.2086390256881714\n",
      "y_pred= tensor([9.7427])\n",
      "59 : tensor(0.0063, grad_fn=<MseLossBackward>)\n",
      "w= 1.9071619510650635 \t b= 0.2081938534975052\n",
      "y_pred= tensor([9.7440])\n",
      "60 : tensor(0.0063, grad_fn=<MseLossBackward>)\n",
      "w= 1.907499074935913 \t b= 0.20774349570274353\n",
      "y_pred= tensor([9.7452])\n",
      "61 : tensor(0.0062, grad_fn=<MseLossBackward>)\n",
      "w= 1.9078227281570435 \t b= 0.20728866755962372\n",
      "y_pred= tensor([9.7464])\n",
      "62 : tensor(0.0062, grad_fn=<MseLossBackward>)\n",
      "w= 1.9081343412399292 \t b= 0.2068299800157547\n",
      "y_pred= tensor([9.7475])\n",
      "63 : tensor(0.0062, grad_fn=<MseLossBackward>)\n",
      "w= 1.9084352254867554 \t b= 0.20636801421642303\n",
      "y_pred= tensor([9.7485])\n",
      "64 : tensor(0.0061, grad_fn=<MseLossBackward>)\n",
      "w= 1.9087265729904175 \t b= 0.20590324699878693\n",
      "y_pred= tensor([9.7495])\n",
      "65 : tensor(0.0061, grad_fn=<MseLossBackward>)\n",
      "w= 1.909009337425232 \t b= 0.2054361253976822\n",
      "y_pred= tensor([9.7505])\n",
      "66 : tensor(0.0061, grad_fn=<MseLossBackward>)\n",
      "w= 1.9092843532562256 \t b= 0.20496703684329987\n",
      "y_pred= tensor([9.7514])\n",
      "67 : tensor(0.0060, grad_fn=<MseLossBackward>)\n",
      "w= 1.9095524549484253 \t b= 0.2044963240623474\n",
      "y_pred= tensor([9.7523])\n",
      "68 : tensor(0.0060, grad_fn=<MseLossBackward>)\n",
      "w= 1.9098143577575684 \t b= 0.2040242999792099\n",
      "y_pred= tensor([9.7531])\n",
      "69 : tensor(0.0060, grad_fn=<MseLossBackward>)\n",
      "w= 1.9100706577301025 \t b= 0.20355123281478882\n",
      "y_pred= tensor([9.7539])\n",
      "70 : tensor(0.0060, grad_fn=<MseLossBackward>)\n",
      "w= 1.9103219509124756 \t b= 0.20307737588882446\n",
      "y_pred= tensor([9.7547])\n",
      "71 : tensor(0.0059, grad_fn=<MseLossBackward>)\n",
      "w= 1.9105688333511353 \t b= 0.20260295271873474\n",
      "y_pred= tensor([9.7554])\n",
      "72 : tensor(0.0059, grad_fn=<MseLossBackward>)\n",
      "w= 1.9108116626739502 \t b= 0.20212814211845398\n",
      "y_pred= tensor([9.7562])\n",
      "73 : tensor(0.0059, grad_fn=<MseLossBackward>)\n",
      "w= 1.911050796508789 \t b= 0.2016531080007553\n",
      "y_pred= tensor([9.7569])\n",
      "74 : tensor(0.0058, grad_fn=<MseLossBackward>)\n",
      "w= 1.9112865924835205 \t b= 0.20117801427841187\n",
      "y_pred= tensor([9.7576])\n",
      "75 : tensor(0.0058, grad_fn=<MseLossBackward>)\n",
      "w= 1.9115194082260132 \t b= 0.2007029801607132\n",
      "y_pred= tensor([9.7583])\n",
      "76 : tensor(0.0058, grad_fn=<MseLossBackward>)\n",
      "w= 1.9117494821548462 \t b= 0.20022813975811005\n",
      "y_pred= tensor([9.7590])\n",
      "77 : tensor(0.0058, grad_fn=<MseLossBackward>)\n",
      "w= 1.9119770526885986 \t b= 0.19975359737873077\n",
      "y_pred= tensor([9.7596])\n",
      "78 : tensor(0.0057, grad_fn=<MseLossBackward>)\n",
      "w= 1.9122023582458496 \t b= 0.19927944242954254\n",
      "y_pred= tensor([9.7603])\n",
      "79 : tensor(0.0057, grad_fn=<MseLossBackward>)\n",
      "w= 1.9124256372451782 \t b= 0.1988057643175125\n",
      "y_pred= tensor([9.7609])\n",
      "80 : tensor(0.0057, grad_fn=<MseLossBackward>)\n",
      "w= 1.912647008895874 \t b= 0.19833262264728546\n",
      "y_pred= tensor([9.7616])\n",
      "81 : tensor(0.0056, grad_fn=<MseLossBackward>)\n",
      "w= 1.9128665924072266 \t b= 0.19786009192466736\n",
      "y_pred= tensor([9.7622])\n",
      "82 : tensor(0.0056, grad_fn=<MseLossBackward>)\n",
      "w= 1.913084626197815 \t b= 0.19738823175430298\n",
      "y_pred= tensor([9.7628])\n",
      "83 : tensor(0.0056, grad_fn=<MseLossBackward>)\n",
      "w= 1.9133012294769287 \t b= 0.1969170868396759\n",
      "y_pred= tensor([9.7634])\n",
      "84 : tensor(0.0056, grad_fn=<MseLossBackward>)\n",
      "w= 1.9135164022445679 \t b= 0.19644670188426971\n",
      "y_pred= tensor([9.7640])\n",
      "85 : tensor(0.0055, grad_fn=<MseLossBackward>)\n",
      "w= 1.9137303829193115 \t b= 0.1959771066904068\n",
      "y_pred= tensor([9.7646])\n",
      "86 : tensor(0.0055, grad_fn=<MseLossBackward>)\n",
      "w= 1.9139431715011597 \t b= 0.19550834596157074\n",
      "y_pred= tensor([9.7652])\n",
      "87 : tensor(0.0055, grad_fn=<MseLossBackward>)\n",
      "w= 1.9141547679901123 \t b= 0.19504044950008392\n",
      "y_pred= tensor([9.7658])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 : tensor(0.0055, grad_fn=<MseLossBackward>)\n",
      "w= 1.9143654108047485 \t b= 0.19457344710826874\n",
      "y_pred= tensor([9.7664])\n",
      "89 : tensor(0.0054, grad_fn=<MseLossBackward>)\n",
      "w= 1.9145749807357788 \t b= 0.19410735368728638\n",
      "y_pred= tensor([9.7670])\n",
      "90 : tensor(0.0054, grad_fn=<MseLossBackward>)\n",
      "w= 1.9147837162017822 \t b= 0.19364221394062042\n",
      "y_pred= tensor([9.7676])\n",
      "91 : tensor(0.0054, grad_fn=<MseLossBackward>)\n",
      "w= 1.9149914979934692 \t b= 0.19317802786827087\n",
      "y_pred= tensor([9.7681])\n",
      "92 : tensor(0.0054, grad_fn=<MseLossBackward>)\n",
      "w= 1.9151984453201294 \t b= 0.19271481037139893\n",
      "y_pred= tensor([9.7687])\n",
      "93 : tensor(0.0053, grad_fn=<MseLossBackward>)\n",
      "w= 1.9154046773910522 \t b= 0.19225257635116577\n",
      "y_pred= tensor([9.7693])\n",
      "94 : tensor(0.0053, grad_fn=<MseLossBackward>)\n",
      "w= 1.9156101942062378 \t b= 0.1917913407087326\n",
      "y_pred= tensor([9.7698])\n",
      "95 : tensor(0.0053, grad_fn=<MseLossBackward>)\n",
      "w= 1.9158148765563965 \t b= 0.19133110344409943\n",
      "y_pred= tensor([9.7704])\n",
      "96 : tensor(0.0053, grad_fn=<MseLossBackward>)\n",
      "w= 1.9160189628601074 \t b= 0.19087189435958862\n",
      "y_pred= tensor([9.7710])\n",
      "97 : tensor(0.0052, grad_fn=<MseLossBackward>)\n",
      "w= 1.916222333908081 \t b= 0.190413698554039\n",
      "y_pred= tensor([9.7715])\n",
      "98 : tensor(0.0052, grad_fn=<MseLossBackward>)\n",
      "w= 1.9164249897003174 \t b= 0.18995653092861176\n",
      "y_pred= tensor([9.7721])\n",
      "99 : tensor(0.0052, grad_fn=<MseLossBackward>)\n",
      "w= 1.916627049446106 \t b= 0.18950039148330688\n",
      "y_pred= tensor([9.7726])\n",
      "100 : tensor(0.0052, grad_fn=<MseLossBackward>)\n",
      "w= 1.9168285131454468 \t b= 0.18904529511928558\n",
      "y_pred= tensor([9.7732])\n",
      "101 : tensor(0.0051, grad_fn=<MseLossBackward>)\n",
      "w= 1.9170293807983398 \t b= 0.18859124183654785\n",
      "y_pred= tensor([9.7737])\n",
      "102 : tensor(0.0051, grad_fn=<MseLossBackward>)\n",
      "w= 1.9172296524047852 \t b= 0.18813824653625488\n",
      "y_pred= tensor([9.7743])\n",
      "103 : tensor(0.0051, grad_fn=<MseLossBackward>)\n",
      "w= 1.9174293279647827 \t b= 0.18768629431724548\n",
      "y_pred= tensor([9.7748])\n",
      "104 : tensor(0.0051, grad_fn=<MseLossBackward>)\n",
      "w= 1.917628526687622 \t b= 0.18723540008068085\n",
      "y_pred= tensor([9.7754])\n",
      "105 : tensor(0.0050, grad_fn=<MseLossBackward>)\n",
      "w= 1.9178271293640137 \t b= 0.18678554892539978\n",
      "y_pred= tensor([9.7759])\n",
      "106 : tensor(0.0050, grad_fn=<MseLossBackward>)\n",
      "w= 1.9180251359939575 \t b= 0.18633675575256348\n",
      "y_pred= tensor([9.7765])\n",
      "107 : tensor(0.0050, grad_fn=<MseLossBackward>)\n",
      "w= 1.9182226657867432 \t b= 0.18588900566101074\n",
      "y_pred= tensor([9.7770])\n",
      "108 : tensor(0.0050, grad_fn=<MseLossBackward>)\n",
      "w= 1.918419599533081 \t b= 0.18544231355190277\n",
      "y_pred= tensor([9.7775])\n",
      "109 : tensor(0.0049, grad_fn=<MseLossBackward>)\n",
      "w= 1.9186160564422607 \t b= 0.18499667942523956\n",
      "y_pred= tensor([9.7781])\n",
      "110 : tensor(0.0049, grad_fn=<MseLossBackward>)\n",
      "w= 1.9188120365142822 \t b= 0.18455210328102112\n",
      "y_pred= tensor([9.7786])\n",
      "111 : tensor(0.0049, grad_fn=<MseLossBackward>)\n",
      "w= 1.9190075397491455 \t b= 0.18410857021808624\n",
      "y_pred= tensor([9.7791])\n",
      "112 : tensor(0.0049, grad_fn=<MseLossBackward>)\n",
      "w= 1.919202446937561 \t b= 0.18366609513759613\n",
      "y_pred= tensor([9.7797])\n",
      "113 : tensor(0.0048, grad_fn=<MseLossBackward>)\n",
      "w= 1.9193968772888184 \t b= 0.18322467803955078\n",
      "y_pred= tensor([9.7802])\n",
      "114 : tensor(0.0048, grad_fn=<MseLossBackward>)\n",
      "w= 1.9195908308029175 \t b= 0.182784304022789\n",
      "y_pred= tensor([9.7807])\n",
      "115 : tensor(0.0048, grad_fn=<MseLossBackward>)\n",
      "w= 1.9197843074798584 \t b= 0.18234498798847198\n",
      "y_pred= tensor([9.7813])\n",
      "116 : tensor(0.0048, grad_fn=<MseLossBackward>)\n",
      "w= 1.9199773073196411 \t b= 0.18190671503543854\n",
      "y_pred= tensor([9.7818])\n",
      "117 : tensor(0.0047, grad_fn=<MseLossBackward>)\n",
      "w= 1.9201698303222656 \t b= 0.18146948516368866\n",
      "y_pred= tensor([9.7823])\n",
      "118 : tensor(0.0047, grad_fn=<MseLossBackward>)\n",
      "w= 1.920361876487732 \t b= 0.18103329837322235\n",
      "y_pred= tensor([9.7828])\n",
      "119 : tensor(0.0047, grad_fn=<MseLossBackward>)\n",
      "w= 1.92055344581604 \t b= 0.1805981546640396\n",
      "y_pred= tensor([9.7834])\n",
      "120 : tensor(0.0047, grad_fn=<MseLossBackward>)\n",
      "w= 1.92074453830719 \t b= 0.18016405403614044\n",
      "y_pred= tensor([9.7839])\n",
      "121 : tensor(0.0047, grad_fn=<MseLossBackward>)\n",
      "w= 1.9209351539611816 \t b= 0.17973099648952484\n",
      "y_pred= tensor([9.7844])\n",
      "122 : tensor(0.0046, grad_fn=<MseLossBackward>)\n",
      "w= 1.9211252927780151 \t b= 0.17929896712303162\n",
      "y_pred= tensor([9.7849])\n",
      "123 : tensor(0.0046, grad_fn=<MseLossBackward>)\n",
      "w= 1.9213149547576904 \t b= 0.17886798083782196\n",
      "y_pred= tensor([9.7854])\n",
      "124 : tensor(0.0046, grad_fn=<MseLossBackward>)\n",
      "w= 1.9215041399002075 \t b= 0.17843802273273468\n",
      "y_pred= tensor([9.7860])\n",
      "125 : tensor(0.0046, grad_fn=<MseLossBackward>)\n",
      "w= 1.9216928482055664 \t b= 0.17800909280776978\n",
      "y_pred= tensor([9.7865])\n",
      "126 : tensor(0.0045, grad_fn=<MseLossBackward>)\n",
      "w= 1.9218811988830566 \t b= 0.17758119106292725\n",
      "y_pred= tensor([9.7870])\n",
      "127 : tensor(0.0045, grad_fn=<MseLossBackward>)\n",
      "w= 1.9220690727233887 \t b= 0.1771543174982071\n",
      "y_pred= tensor([9.7875])\n",
      "128 : tensor(0.0045, grad_fn=<MseLossBackward>)\n",
      "w= 1.9222564697265625 \t b= 0.17672847211360931\n",
      "y_pred= tensor([9.7880])\n",
      "129 : tensor(0.0045, grad_fn=<MseLossBackward>)\n",
      "w= 1.9224433898925781 \t b= 0.17630364000797272\n",
      "y_pred= tensor([9.7885])\n",
      "130 : tensor(0.0045, grad_fn=<MseLossBackward>)\n",
      "w= 1.9226298332214355 \t b= 0.1758798360824585\n",
      "y_pred= tensor([9.7890])\n",
      "131 : tensor(0.0044, grad_fn=<MseLossBackward>)\n",
      "w= 1.9228157997131348 \t b= 0.17545704543590546\n",
      "y_pred= tensor([9.7895])\n",
      "132 : tensor(0.0044, grad_fn=<MseLossBackward>)\n",
      "w= 1.9230014085769653 \t b= 0.1750352680683136\n",
      "y_pred= tensor([9.7900])\n",
      "133 : tensor(0.0044, grad_fn=<MseLossBackward>)\n",
      "w= 1.9231865406036377 \t b= 0.17461450397968292\n",
      "y_pred= tensor([9.7905])\n",
      "134 : tensor(0.0044, grad_fn=<MseLossBackward>)\n",
      "w= 1.9233711957931519 \t b= 0.17419475317001343\n",
      "y_pred= tensor([9.7910])\n",
      "135 : tensor(0.0044, grad_fn=<MseLossBackward>)\n",
      "w= 1.9235553741455078 \t b= 0.17377601563930511\n",
      "y_pred= tensor([9.7916])\n",
      "136 : tensor(0.0043, grad_fn=<MseLossBackward>)\n",
      "w= 1.9237391948699951 \t b= 0.1733582764863968\n",
      "y_pred= tensor([9.7921])\n",
      "137 : tensor(0.0043, grad_fn=<MseLossBackward>)\n",
      "w= 1.9239225387573242 \t b= 0.17294153571128845\n",
      "y_pred= tensor([9.7926])\n",
      "138 : tensor(0.0043, grad_fn=<MseLossBackward>)\n",
      "w= 1.9241054058074951 \t b= 0.1725258082151413\n",
      "y_pred= tensor([9.7931])\n",
      "139 : tensor(0.0043, grad_fn=<MseLossBackward>)\n",
      "w= 1.9242879152297974 \t b= 0.17211107909679413\n",
      "y_pred= tensor([9.7936])\n",
      "140 : tensor(0.0042, grad_fn=<MseLossBackward>)\n",
      "w= 1.9244699478149414 \t b= 0.17169733345508575\n",
      "y_pred= tensor([9.7940])\n",
      "141 : tensor(0.0042, grad_fn=<MseLossBackward>)\n",
      "w= 1.9246515035629272 \t b= 0.17128458619117737\n",
      "y_pred= tensor([9.7945])\n",
      "142 : tensor(0.0042, grad_fn=<MseLossBackward>)\n",
      "w= 1.9248327016830444 \t b= 0.17087283730506897\n",
      "y_pred= tensor([9.7950])\n",
      "143 : tensor(0.0042, grad_fn=<MseLossBackward>)\n",
      "w= 1.9250134229660034 \t b= 0.17046207189559937\n",
      "y_pred= tensor([9.7955])\n",
      "144 : tensor(0.0042, grad_fn=<MseLossBackward>)\n",
      "w= 1.9251936674118042 \t b= 0.17005228996276855\n",
      "y_pred= tensor([9.7960])\n",
      "145 : tensor(0.0041, grad_fn=<MseLossBackward>)\n",
      "w= 1.9253735542297363 \t b= 0.16964350640773773\n",
      "y_pred= tensor([9.7965])\n",
      "146 : tensor(0.0041, grad_fn=<MseLossBackward>)\n",
      "w= 1.9255529642105103 \t b= 0.1692356914281845\n",
      "y_pred= tensor([9.7970])\n",
      "147 : tensor(0.0041, grad_fn=<MseLossBackward>)\n",
      "w= 1.925731897354126 \t b= 0.16882885992527008\n",
      "y_pred= tensor([9.7975])\n",
      "148 : tensor(0.0041, grad_fn=<MseLossBackward>)\n",
      "w= 1.925910472869873 \t b= 0.16842299699783325\n",
      "y_pred= tensor([9.7980])\n",
      "149 : tensor(0.0041, grad_fn=<MseLossBackward>)\n",
      "w= 1.926088571548462 \t b= 0.16801811754703522\n",
      "y_pred= tensor([9.7985])\n",
      "150 : tensor(0.0040, grad_fn=<MseLossBackward>)\n",
      "w= 1.9262661933898926 \t b= 0.16761420667171478\n",
      "y_pred= tensor([9.7989])\n",
      "151 : tensor(0.0040, grad_fn=<MseLossBackward>)\n",
      "w= 1.9264434576034546 \t b= 0.16721127927303314\n",
      "y_pred= tensor([9.7994])\n",
      "152 : tensor(0.0040, grad_fn=<MseLossBackward>)\n",
      "w= 1.9266202449798584 \t b= 0.1668093204498291\n",
      "y_pred= tensor([9.7999])\n",
      "153 : tensor(0.0040, grad_fn=<MseLossBackward>)\n",
      "w= 1.9267966747283936 \t b= 0.16640833020210266\n",
      "y_pred= tensor([9.8004])\n",
      "154 : tensor(0.0040, grad_fn=<MseLossBackward>)\n",
      "w= 1.9269726276397705 \t b= 0.16600829362869263\n",
      "y_pred= tensor([9.8009])\n",
      "155 : tensor(0.0040, grad_fn=<MseLossBackward>)\n",
      "w= 1.9271482229232788 \t b= 0.1656092256307602\n",
      "y_pred= tensor([9.8014])\n",
      "156 : tensor(0.0039, grad_fn=<MseLossBackward>)\n",
      "w= 1.927323341369629 \t b= 0.16521111130714417\n",
      "y_pred= tensor([9.8018])\n",
      "157 : tensor(0.0039, grad_fn=<MseLossBackward>)\n",
      "w= 1.9274981021881104 \t b= 0.16481395065784454\n",
      "y_pred= tensor([9.8023])\n",
      "158 : tensor(0.0039, grad_fn=<MseLossBackward>)\n",
      "w= 1.9276723861694336 \t b= 0.16441774368286133\n",
      "y_pred= tensor([9.8028])\n",
      "159 : tensor(0.0039, grad_fn=<MseLossBackward>)\n",
      "w= 1.9278463125228882 \t b= 0.16402249038219452\n",
      "y_pred= tensor([9.8033])\n",
      "160 : tensor(0.0039, grad_fn=<MseLossBackward>)\n",
      "w= 1.9280197620391846 \t b= 0.16362819075584412\n",
      "y_pred= tensor([9.8037])\n",
      "161 : tensor(0.0038, grad_fn=<MseLossBackward>)\n",
      "w= 1.9281927347183228 \t b= 0.16323482990264893\n",
      "y_pred= tensor([9.8042])\n",
      "162 : tensor(0.0038, grad_fn=<MseLossBackward>)\n",
      "w= 1.9283653497695923 \t b= 0.16284242272377014\n",
      "y_pred= tensor([9.8047])\n",
      "163 : tensor(0.0038, grad_fn=<MseLossBackward>)\n",
      "w= 1.9285376071929932 \t b= 0.16245095431804657\n",
      "y_pred= tensor([9.8051])\n",
      "164 : tensor(0.0038, grad_fn=<MseLossBackward>)\n",
      "w= 1.9287093877792358 \t b= 0.1620604246854782\n",
      "y_pred= tensor([9.8056])\n",
      "165 : tensor(0.0038, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w= 1.9288808107376099 \t b= 0.16167084872722626\n",
      "y_pred= tensor([9.8061])\n",
      "166 : tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "w= 1.9290517568588257 \t b= 0.16128219664096832\n",
      "y_pred= tensor([9.8065])\n",
      "167 : tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "w= 1.9292223453521729 \t b= 0.1608944833278656\n",
      "y_pred= tensor([9.8070])\n",
      "168 : tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "w= 1.9293924570083618 \t b= 0.1605076938867569\n",
      "y_pred= tensor([9.8075])\n",
      "169 : tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "w= 1.9295622110366821 \t b= 0.1601218432188034\n",
      "y_pred= tensor([9.8079])\n",
      "170 : tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "w= 1.9297314882278442 \t b= 0.15973691642284393\n",
      "y_pred= tensor([9.8084])\n",
      "171 : tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "w= 1.9299004077911377 \t b= 0.15935291349887848\n",
      "y_pred= tensor([9.8089])\n",
      "172 : tensor(0.0036, grad_fn=<MseLossBackward>)\n",
      "w= 1.9300689697265625 \t b= 0.15896983444690704\n",
      "y_pred= tensor([9.8093])\n",
      "173 : tensor(0.0036, grad_fn=<MseLossBackward>)\n",
      "w= 1.930237054824829 \t b= 0.15858767926692963\n",
      "y_pred= tensor([9.8098])\n",
      "174 : tensor(0.0036, grad_fn=<MseLossBackward>)\n",
      "w= 1.930404782295227 \t b= 0.15820644795894623\n",
      "y_pred= tensor([9.8102])\n",
      "175 : tensor(0.0036, grad_fn=<MseLossBackward>)\n",
      "w= 1.9305720329284668 \t b= 0.15782612562179565\n",
      "y_pred= tensor([9.8107])\n",
      "176 : tensor(0.0036, grad_fn=<MseLossBackward>)\n",
      "w= 1.930738925933838 \t b= 0.1574467271566391\n",
      "y_pred= tensor([9.8111])\n",
      "177 : tensor(0.0036, grad_fn=<MseLossBackward>)\n",
      "w= 1.9309054613113403 \t b= 0.15706823766231537\n",
      "y_pred= tensor([9.8116])\n",
      "178 : tensor(0.0035, grad_fn=<MseLossBackward>)\n",
      "w= 1.9310715198516846 \t b= 0.15669065713882446\n",
      "y_pred= tensor([9.8120])\n",
      "179 : tensor(0.0035, grad_fn=<MseLossBackward>)\n",
      "w= 1.9312372207641602 \t b= 0.15631398558616638\n",
      "y_pred= tensor([9.8125])\n",
      "180 : tensor(0.0035, grad_fn=<MseLossBackward>)\n",
      "w= 1.931402564048767 \t b= 0.15593822300434113\n",
      "y_pred= tensor([9.8130])\n",
      "181 : tensor(0.0035, grad_fn=<MseLossBackward>)\n",
      "w= 1.9315674304962158 \t b= 0.1555633544921875\n",
      "y_pred= tensor([9.8134])\n",
      "182 : tensor(0.0035, grad_fn=<MseLossBackward>)\n",
      "w= 1.931731939315796 \t b= 0.1551893949508667\n",
      "y_pred= tensor([9.8138])\n",
      "183 : tensor(0.0035, grad_fn=<MseLossBackward>)\n",
      "w= 1.9318960905075073 \t b= 0.15481632947921753\n",
      "y_pred= tensor([9.8143])\n",
      "184 : tensor(0.0034, grad_fn=<MseLossBackward>)\n",
      "w= 1.9320597648620605 \t b= 0.15444415807724\n",
      "y_pred= tensor([9.8147])\n",
      "185 : tensor(0.0034, grad_fn=<MseLossBackward>)\n",
      "w= 1.9322230815887451 \t b= 0.15407288074493408\n",
      "y_pred= tensor([9.8152])\n",
      "186 : tensor(0.0034, grad_fn=<MseLossBackward>)\n",
      "w= 1.932386040687561 \t b= 0.1537024974822998\n",
      "y_pred= tensor([9.8156])\n",
      "187 : tensor(0.0034, grad_fn=<MseLossBackward>)\n",
      "w= 1.9325485229492188 \t b= 0.15333300828933716\n",
      "y_pred= tensor([9.8161])\n",
      "188 : tensor(0.0034, grad_fn=<MseLossBackward>)\n",
      "w= 1.9327106475830078 \t b= 0.15296439826488495\n",
      "y_pred= tensor([9.8165])\n",
      "189 : tensor(0.0034, grad_fn=<MseLossBackward>)\n",
      "w= 1.9328724145889282 \t b= 0.15259668231010437\n",
      "y_pred= tensor([9.8170])\n",
      "190 : tensor(0.0033, grad_fn=<MseLossBackward>)\n",
      "w= 1.93303382396698 \t b= 0.15222986042499542\n",
      "y_pred= tensor([9.8174])\n",
      "191 : tensor(0.0033, grad_fn=<MseLossBackward>)\n",
      "w= 1.9331947565078735 \t b= 0.15186390280723572\n",
      "y_pred= tensor([9.8178])\n",
      "192 : tensor(0.0033, grad_fn=<MseLossBackward>)\n",
      "w= 1.9333553314208984 \t b= 0.15149883925914764\n",
      "y_pred= tensor([9.8183])\n",
      "193 : tensor(0.0033, grad_fn=<MseLossBackward>)\n",
      "w= 1.9335155487060547 \t b= 0.15113465487957\n",
      "y_pred= tensor([9.8187])\n",
      "194 : tensor(0.0033, grad_fn=<MseLossBackward>)\n",
      "w= 1.9336754083633423 \t b= 0.1507713347673416\n",
      "y_pred= tensor([9.8191])\n",
      "195 : tensor(0.0033, grad_fn=<MseLossBackward>)\n",
      "w= 1.9338349103927612 \t b= 0.15040889382362366\n",
      "y_pred= tensor([9.8196])\n",
      "196 : tensor(0.0032, grad_fn=<MseLossBackward>)\n",
      "w= 1.933993935585022 \t b= 0.15004731714725494\n",
      "y_pred= tensor([9.8200])\n",
      "197 : tensor(0.0032, grad_fn=<MseLossBackward>)\n",
      "w= 1.934152603149414 \t b= 0.14968660473823547\n",
      "y_pred= tensor([9.8204])\n",
      "198 : tensor(0.0032, grad_fn=<MseLossBackward>)\n",
      "w= 1.9343109130859375 \t b= 0.14932677149772644\n",
      "y_pred= tensor([9.8209])\n",
      "199 : tensor(0.0032, grad_fn=<MseLossBackward>)\n",
      "w= 1.9344688653945923 \t b= 0.14896780252456665\n",
      "y_pred= tensor([9.8213])\n",
      "200 : tensor(0.0032, grad_fn=<MseLossBackward>)\n",
      "w= 1.9346263408660889 \t b= 0.1486096829175949\n",
      "y_pred= tensor([9.8217])\n",
      "201 : tensor(0.0032, grad_fn=<MseLossBackward>)\n",
      "w= 1.9347834587097168 \t b= 0.1482524424791336\n",
      "y_pred= tensor([9.8222])\n",
      "202 : tensor(0.0032, grad_fn=<MseLossBackward>)\n",
      "w= 1.934940218925476 \t b= 0.14789605140686035\n",
      "y_pred= tensor([9.8226])\n",
      "203 : tensor(0.0031, grad_fn=<MseLossBackward>)\n",
      "w= 1.9350966215133667 \t b= 0.14754052460193634\n",
      "y_pred= tensor([9.8230])\n",
      "204 : tensor(0.0031, grad_fn=<MseLossBackward>)\n",
      "w= 1.9352526664733887 \t b= 0.14718584716320038\n",
      "y_pred= tensor([9.8235])\n",
      "205 : tensor(0.0031, grad_fn=<MseLossBackward>)\n",
      "w= 1.935408353805542 \t b= 0.14683201909065247\n",
      "y_pred= tensor([9.8239])\n",
      "206 : tensor(0.0031, grad_fn=<MseLossBackward>)\n",
      "w= 1.9355636835098267 \t b= 0.1464790403842926\n",
      "y_pred= tensor([9.8243])\n",
      "207 : tensor(0.0031, grad_fn=<MseLossBackward>)\n",
      "w= 1.9357185363769531 \t b= 0.1461269110441208\n",
      "y_pred= tensor([9.8247])\n",
      "208 : tensor(0.0031, grad_fn=<MseLossBackward>)\n",
      "w= 1.935873031616211 \t b= 0.14577563107013702\n",
      "y_pred= tensor([9.8251])\n",
      "209 : tensor(0.0030, grad_fn=<MseLossBackward>)\n",
      "w= 1.9360271692276 \t b= 0.1454252004623413\n",
      "y_pred= tensor([9.8256])\n",
      "210 : tensor(0.0030, grad_fn=<MseLossBackward>)\n",
      "w= 1.9361809492111206 \t b= 0.14507560431957245\n",
      "y_pred= tensor([9.8260])\n",
      "211 : tensor(0.0030, grad_fn=<MseLossBackward>)\n",
      "w= 1.9363343715667725 \t b= 0.14472685754299164\n",
      "y_pred= tensor([9.8264])\n",
      "212 : tensor(0.0030, grad_fn=<MseLossBackward>)\n",
      "w= 1.9364874362945557 \t b= 0.14437894523143768\n",
      "y_pred= tensor([9.8268])\n",
      "213 : tensor(0.0030, grad_fn=<MseLossBackward>)\n",
      "w= 1.9366401433944702 \t b= 0.14403186738491058\n",
      "y_pred= tensor([9.8272])\n",
      "214 : tensor(0.0030, grad_fn=<MseLossBackward>)\n",
      "w= 1.9367924928665161 \t b= 0.14368562400341034\n",
      "y_pred= tensor([9.8276])\n",
      "215 : tensor(0.0030, grad_fn=<MseLossBackward>)\n",
      "w= 1.9369444847106934 \t b= 0.14334021508693695\n",
      "y_pred= tensor([9.8281])\n",
      "216 : tensor(0.0029, grad_fn=<MseLossBackward>)\n",
      "w= 1.937096118927002 \t b= 0.14299564063549042\n",
      "y_pred= tensor([9.8285])\n",
      "217 : tensor(0.0029, grad_fn=<MseLossBackward>)\n",
      "w= 1.9372472763061523 \t b= 0.14265188574790955\n",
      "y_pred= tensor([9.8289])\n",
      "218 : tensor(0.0029, grad_fn=<MseLossBackward>)\n",
      "w= 1.937398076057434 \t b= 0.14230895042419434\n",
      "y_pred= tensor([9.8293])\n",
      "219 : tensor(0.0029, grad_fn=<MseLossBackward>)\n",
      "w= 1.9375485181808472 \t b= 0.14196684956550598\n",
      "y_pred= tensor([9.8297])\n",
      "220 : tensor(0.0029, grad_fn=<MseLossBackward>)\n",
      "w= 1.9376986026763916 \t b= 0.1416255682706833\n",
      "y_pred= tensor([9.8301])\n",
      "221 : tensor(0.0029, grad_fn=<MseLossBackward>)\n",
      "w= 1.9378483295440674 \t b= 0.14128510653972626\n",
      "y_pred= tensor([9.8305])\n",
      "222 : tensor(0.0029, grad_fn=<MseLossBackward>)\n",
      "w= 1.9379976987838745 \t b= 0.1409454643726349\n",
      "y_pred= tensor([9.8309])\n",
      "223 : tensor(0.0028, grad_fn=<MseLossBackward>)\n",
      "w= 1.938146710395813 \t b= 0.14060664176940918\n",
      "y_pred= tensor([9.8313])\n",
      "224 : tensor(0.0028, grad_fn=<MseLossBackward>)\n",
      "w= 1.9382953643798828 \t b= 0.14026863873004913\n",
      "y_pred= tensor([9.8317])\n",
      "225 : tensor(0.0028, grad_fn=<MseLossBackward>)\n",
      "w= 1.938443660736084 \t b= 0.13993145525455475\n",
      "y_pred= tensor([9.8322])\n",
      "226 : tensor(0.0028, grad_fn=<MseLossBackward>)\n",
      "w= 1.9385915994644165 \t b= 0.13959507644176483\n",
      "y_pred= tensor([9.8326])\n",
      "227 : tensor(0.0028, grad_fn=<MseLossBackward>)\n",
      "w= 1.93873929977417 \t b= 0.13925950229167938\n",
      "y_pred= tensor([9.8330])\n",
      "228 : tensor(0.0028, grad_fn=<MseLossBackward>)\n",
      "w= 1.9388866424560547 \t b= 0.1389247477054596\n",
      "y_pred= tensor([9.8334])\n",
      "229 : tensor(0.0028, grad_fn=<MseLossBackward>)\n",
      "w= 1.9390336275100708 \t b= 0.13859078288078308\n",
      "y_pred= tensor([9.8338])\n",
      "230 : tensor(0.0028, grad_fn=<MseLossBackward>)\n",
      "w= 1.9391801357269287 \t b= 0.13825762271881104\n",
      "y_pred= tensor([9.8342])\n",
      "231 : tensor(0.0027, grad_fn=<MseLossBackward>)\n",
      "w= 1.9393264055252075 \t b= 0.13792526721954346\n",
      "y_pred= tensor([9.8346])\n",
      "232 : tensor(0.0027, grad_fn=<MseLossBackward>)\n",
      "w= 1.9394723176956177 \t b= 0.13759370148181915\n",
      "y_pred= tensor([9.8350])\n",
      "233 : tensor(0.0027, grad_fn=<MseLossBackward>)\n",
      "w= 1.9396178722381592 \t b= 0.13726294040679932\n",
      "y_pred= tensor([9.8354])\n",
      "234 : tensor(0.0027, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w= 1.939763069152832 \t b= 0.13693296909332275\n",
      "y_pred= tensor([9.8357])\n",
      "235 : tensor(0.0027, grad_fn=<MseLossBackward>)\n",
      "w= 1.9399079084396362 \t b= 0.13660378754138947\n",
      "y_pred= tensor([9.8361])\n",
      "236 : tensor(0.0027, grad_fn=<MseLossBackward>)\n",
      "w= 1.9400523900985718 \t b= 0.13627539575099945\n",
      "y_pred= tensor([9.8365])\n",
      "237 : tensor(0.0027, grad_fn=<MseLossBackward>)\n",
      "w= 1.9401965141296387 \t b= 0.1359477937221527\n",
      "y_pred= tensor([9.8369])\n",
      "238 : tensor(0.0027, grad_fn=<MseLossBackward>)\n",
      "w= 1.940340280532837 \t b= 0.13562098145484924\n",
      "y_pred= tensor([9.8373])\n",
      "239 : tensor(0.0026, grad_fn=<MseLossBackward>)\n",
      "w= 1.9404836893081665 \t b= 0.13529494404792786\n",
      "y_pred= tensor([9.8377])\n",
      "240 : tensor(0.0026, grad_fn=<MseLossBackward>)\n",
      "w= 1.9406267404556274 \t b= 0.13496969640254974\n",
      "y_pred= tensor([9.8381])\n",
      "241 : tensor(0.0026, grad_fn=<MseLossBackward>)\n",
      "w= 1.9407694339752197 \t b= 0.1346452385187149\n",
      "y_pred= tensor([9.8385])\n",
      "242 : tensor(0.0026, grad_fn=<MseLossBackward>)\n",
      "w= 1.9409117698669434 \t b= 0.13432155549526215\n",
      "y_pred= tensor([9.8389])\n",
      "243 : tensor(0.0026, grad_fn=<MseLossBackward>)\n",
      "w= 1.9410537481307983 \t b= 0.13399864733219147\n",
      "y_pred= tensor([9.8393])\n",
      "244 : tensor(0.0026, grad_fn=<MseLossBackward>)\n",
      "w= 1.9411954879760742 \t b= 0.13367652893066406\n",
      "y_pred= tensor([9.8397])\n",
      "245 : tensor(0.0026, grad_fn=<MseLossBackward>)\n",
      "w= 1.9413368701934814 \t b= 0.13335518538951874\n",
      "y_pred= tensor([9.8400])\n",
      "246 : tensor(0.0026, grad_fn=<MseLossBackward>)\n",
      "w= 1.94147789478302 \t b= 0.1330346018075943\n",
      "y_pred= tensor([9.8404])\n",
      "247 : tensor(0.0025, grad_fn=<MseLossBackward>)\n",
      "w= 1.94161856174469 \t b= 0.13271479308605194\n",
      "y_pred= tensor([9.8408])\n",
      "248 : tensor(0.0025, grad_fn=<MseLossBackward>)\n",
      "w= 1.9417588710784912 \t b= 0.13239575922489166\n",
      "y_pred= tensor([9.8412])\n",
      "249 : tensor(0.0025, grad_fn=<MseLossBackward>)\n",
      "w= 1.9418989419937134 \t b= 0.13207748532295227\n",
      "y_pred= tensor([9.8416])\n",
      "250 : tensor(0.0025, grad_fn=<MseLossBackward>)\n",
      "w= 1.942038655281067 \t b= 0.13175997138023376\n",
      "y_pred= tensor([9.8420])\n",
      "251 : tensor(0.0025, grad_fn=<MseLossBackward>)\n",
      "w= 1.9421780109405518 \t b= 0.13144321739673615\n",
      "y_pred= tensor([9.8423])\n",
      "252 : tensor(0.0025, grad_fn=<MseLossBackward>)\n",
      "w= 1.942317008972168 \t b= 0.1311272382736206\n",
      "y_pred= tensor([9.8427])\n",
      "253 : tensor(0.0025, grad_fn=<MseLossBackward>)\n",
      "w= 1.9424556493759155 \t b= 0.13081200420856476\n",
      "y_pred= tensor([9.8431])\n",
      "254 : tensor(0.0025, grad_fn=<MseLossBackward>)\n",
      "w= 1.9425939321517944 \t b= 0.130497545003891\n",
      "y_pred= tensor([9.8435])\n",
      "255 : tensor(0.0024, grad_fn=<MseLossBackward>)\n",
      "w= 1.9427319765090942 \t b= 0.13018383085727692\n",
      "y_pred= tensor([9.8438])\n",
      "256 : tensor(0.0024, grad_fn=<MseLossBackward>)\n",
      "w= 1.9428696632385254 \t b= 0.12987087666988373\n",
      "y_pred= tensor([9.8442])\n",
      "257 : tensor(0.0024, grad_fn=<MseLossBackward>)\n",
      "w= 1.943006992340088 \t b= 0.12955866754055023\n",
      "y_pred= tensor([9.8446])\n",
      "258 : tensor(0.0024, grad_fn=<MseLossBackward>)\n",
      "w= 1.9431439638137817 \t b= 0.12924721837043762\n",
      "y_pred= tensor([9.8450])\n",
      "259 : tensor(0.0024, grad_fn=<MseLossBackward>)\n",
      "w= 1.943280577659607 \t b= 0.1289365142583847\n",
      "y_pred= tensor([9.8453])\n",
      "260 : tensor(0.0024, grad_fn=<MseLossBackward>)\n",
      "w= 1.943416953086853 \t b= 0.12862657010555267\n",
      "y_pred= tensor([9.8457])\n",
      "261 : tensor(0.0024, grad_fn=<MseLossBackward>)\n",
      "w= 1.9435529708862305 \t b= 0.12831735610961914\n",
      "y_pred= tensor([9.8461])\n",
      "262 : tensor(0.0024, grad_fn=<MseLossBackward>)\n",
      "w= 1.9436886310577393 \t b= 0.1280088871717453\n",
      "y_pred= tensor([9.8465])\n",
      "263 : tensor(0.0024, grad_fn=<MseLossBackward>)\n",
      "w= 1.943824052810669 \t b= 0.12770116329193115\n",
      "y_pred= tensor([9.8468])\n",
      "264 : tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "w= 1.94395911693573 \t b= 0.1273941695690155\n",
      "y_pred= tensor([9.8472])\n",
      "265 : tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "w= 1.9440938234329224 \t b= 0.12708792090415955\n",
      "y_pred= tensor([9.8476])\n",
      "266 : tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "w= 1.944228172302246 \t b= 0.1267824023962021\n",
      "y_pred= tensor([9.8479])\n",
      "267 : tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "w= 1.9443622827529907 \t b= 0.12647762894630432\n",
      "y_pred= tensor([9.8483])\n",
      "268 : tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "w= 1.9444960355758667 \t b= 0.12617358565330505\n",
      "y_pred= tensor([9.8487])\n",
      "269 : tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "w= 1.944629430770874 \t b= 0.12587027251720428\n",
      "y_pred= tensor([9.8490])\n",
      "270 : tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "w= 1.9447625875473022 \t b= 0.12556768953800201\n",
      "y_pred= tensor([9.8494])\n",
      "271 : tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "w= 1.9448953866958618 \t b= 0.12526583671569824\n",
      "y_pred= tensor([9.8497])\n",
      "272 : tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "w= 1.9450278282165527 \t b= 0.12496469914913177\n",
      "y_pred= tensor([9.8501])\n",
      "273 : tensor(0.0022, grad_fn=<MseLossBackward>)\n",
      "w= 1.9451600313186646 \t b= 0.1246642917394638\n",
      "y_pred= tensor([9.8505])\n",
      "274 : tensor(0.0022, grad_fn=<MseLossBackward>)\n",
      "w= 1.9452918767929077 \t b= 0.12436460703611374\n",
      "y_pred= tensor([9.8508])\n",
      "275 : tensor(0.0022, grad_fn=<MseLossBackward>)\n",
      "w= 1.9454233646392822 \t b= 0.12406564503908157\n",
      "y_pred= tensor([9.8512])\n",
      "276 : tensor(0.0022, grad_fn=<MseLossBackward>)\n",
      "w= 1.945554494857788 \t b= 0.12376739084720612\n",
      "y_pred= tensor([9.8515])\n",
      "277 : tensor(0.0022, grad_fn=<MseLossBackward>)\n",
      "w= 1.9456853866577148 \t b= 0.12346986681222916\n",
      "y_pred= tensor([9.8519])\n",
      "278 : tensor(0.0022, grad_fn=<MseLossBackward>)\n",
      "w= 1.945815920829773 \t b= 0.1231730580329895\n",
      "y_pred= tensor([9.8523])\n",
      "279 : tensor(0.0022, grad_fn=<MseLossBackward>)\n",
      "w= 1.945946216583252 \t b= 0.12287695705890656\n",
      "y_pred= tensor([9.8526])\n",
      "280 : tensor(0.0022, grad_fn=<MseLossBackward>)\n",
      "w= 1.9460761547088623 \t b= 0.12258156388998032\n",
      "y_pred= tensor([9.8530])\n",
      "281 : tensor(0.0022, grad_fn=<MseLossBackward>)\n",
      "w= 1.946205735206604 \t b= 0.12228688597679138\n",
      "y_pred= tensor([9.8533])\n",
      "282 : tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "w= 1.9463350772857666 \t b= 0.12199291586875916\n",
      "y_pred= tensor([9.8537])\n",
      "283 : tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "w= 1.9464640617370605 \t b= 0.12169965356588364\n",
      "y_pred= tensor([9.8540])\n",
      "284 : tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "w= 1.9465928077697754 \t b= 0.12140709906816483\n",
      "y_pred= tensor([9.8544])\n",
      "285 : tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "w= 1.9467211961746216 \t b= 0.12111524492502213\n",
      "y_pred= tensor([9.8547])\n",
      "286 : tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "w= 1.9468492269515991 \t b= 0.12082409113645554\n",
      "y_pred= tensor([9.8551])\n",
      "287 : tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "w= 1.9469770193099976 \t b= 0.12053364515304565\n",
      "y_pred= tensor([9.8554])\n",
      "288 : tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "w= 1.9471044540405273 \t b= 0.12024389207363129\n",
      "y_pred= tensor([9.8558])\n",
      "289 : tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "w= 1.947231650352478 \t b= 0.11995483189821243\n",
      "y_pred= tensor([9.8561])\n",
      "290 : tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "w= 1.94735848903656 \t b= 0.11966646462678909\n",
      "y_pred= tensor([9.8565])\n",
      "291 : tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "w= 1.947485089302063 \t b= 0.11937879770994186\n",
      "y_pred= tensor([9.8568])\n",
      "292 : tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "w= 1.9476113319396973 \t b= 0.11909181624650955\n",
      "y_pred= tensor([9.8571])\n",
      "293 : tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "w= 1.947737216949463 \t b= 0.11880552768707275\n",
      "y_pred= tensor([9.8575])\n",
      "294 : tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "w= 1.9478628635406494 \t b= 0.11851993203163147\n",
      "y_pred= tensor([9.8578])\n",
      "295 : tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "w= 1.9479881525039673 \t b= 0.1182350218296051\n",
      "y_pred= tensor([9.8582])\n",
      "296 : tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "w= 1.948113203048706 \t b= 0.11795079708099365\n",
      "y_pred= tensor([9.8585])\n",
      "297 : tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "w= 1.9482378959655762 \t b= 0.11766725033521652\n",
      "y_pred= tensor([9.8589])\n",
      "298 : tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "w= 1.9483623504638672 \t b= 0.11738438904285431\n",
      "y_pred= tensor([9.8592])\n",
      "299 : tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "w= 1.9484864473342896 \t b= 0.11710220575332642\n",
      "y_pred= tensor([9.8595])\n",
      "300 : tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "w= 1.9486103057861328 \t b= 0.11682070046663284\n",
      "y_pred= tensor([9.8599])\n",
      "301 : tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "w= 1.9487338066101074 \t b= 0.11653987318277359\n",
      "y_pred= tensor([9.8602])\n",
      "302 : tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "w= 1.948857069015503 \t b= 0.11625972390174866\n",
      "y_pred= tensor([9.8605])\n",
      "303 : tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "w= 1.9489799737930298 \t b= 0.11598025262355804\n",
      "y_pred= tensor([9.8609])\n",
      "304 : tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "w= 1.9491026401519775 \t b= 0.11570144444704056\n",
      "y_pred= tensor([9.8612])\n",
      "305 : tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "w= 1.9492249488830566 \t b= 0.1154233068227768\n",
      "y_pred= tensor([9.8615])\n",
      "306 : tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "w= 1.9493470191955566 \t b= 0.11514584720134735\n",
      "y_pred= tensor([9.8619])\n",
      "307 : tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "w= 1.9494688510894775 \t b= 0.11486905068159103\n",
      "y_pred= tensor([9.8622])\n",
      "308 : tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "w= 1.9495903253555298 \t b= 0.11459290981292725\n",
      "y_pred= tensor([9.8625])\n",
      "309 : tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "w= 1.949711561203003 \t b= 0.11431743949651718\n",
      "y_pred= tensor([9.8629])\n",
      "310 : tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "w= 1.9498324394226074 \t b= 0.11404263228178024\n",
      "y_pred= tensor([9.8632])\n",
      "311 : tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "w= 1.9499530792236328 \t b= 0.11376848071813583\n",
      "y_pred= tensor([9.8635])\n",
      "312 : tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "w= 1.9500733613967896 \t b= 0.11349498480558395\n",
      "y_pred= tensor([9.8639])\n",
      "313 : tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "w= 1.9501934051513672 \t b= 0.1132221519947052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred= tensor([9.8642])\n",
      "314 : tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "w= 1.9503130912780762 \t b= 0.11294997483491898\n",
      "y_pred= tensor([9.8645])\n",
      "315 : tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "w= 1.950432538986206 \t b= 0.11267845332622528\n",
      "y_pred= tensor([9.8648])\n",
      "316 : tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "w= 1.9505517482757568 \t b= 0.11240758001804352\n",
      "y_pred= tensor([9.8652])\n",
      "317 : tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "w= 1.950670599937439 \t b= 0.11213735491037369\n",
      "y_pred= tensor([9.8655])\n",
      "318 : tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "w= 1.950789213180542 \t b= 0.11186778545379639\n",
      "y_pred= tensor([9.8658])\n",
      "319 : tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "w= 1.9509074687957764 \t b= 0.11159886419773102\n",
      "y_pred= tensor([9.8661])\n",
      "320 : tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "w= 1.9510254859924316 \t b= 0.11133059114217758\n",
      "y_pred= tensor([9.8665])\n",
      "321 : tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "w= 1.9511432647705078 \t b= 0.11106295883655548\n",
      "y_pred= tensor([9.8668])\n",
      "322 : tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "w= 1.9512606859207153 \t b= 0.11079596728086472\n",
      "y_pred= tensor([9.8671])\n",
      "323 : tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "w= 1.9513778686523438 \t b= 0.11052962392568588\n",
      "y_pred= tensor([9.8674])\n",
      "324 : tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "w= 1.951494812965393 \t b= 0.11026392132043839\n",
      "y_pred= tensor([9.8677])\n",
      "325 : tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "w= 1.9516113996505737 \t b= 0.10999885201454163\n",
      "y_pred= tensor([9.8681])\n",
      "326 : tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "w= 1.9517277479171753 \t b= 0.1097344234585762\n",
      "y_pred= tensor([9.8684])\n",
      "327 : tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "w= 1.9518437385559082 \t b= 0.10947062820196152\n",
      "y_pred= tensor([9.8687])\n",
      "328 : tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "w= 1.951959490776062 \t b= 0.10920746624469757\n",
      "y_pred= tensor([9.8690])\n",
      "329 : tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "w= 1.9520750045776367 \t b= 0.10894493758678436\n",
      "y_pred= tensor([9.8693])\n",
      "330 : tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "w= 1.9521901607513428 \t b= 0.1086830422282219\n",
      "y_pred= tensor([9.8696])\n",
      "331 : tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "w= 1.9523050785064697 \t b= 0.10842177271842957\n",
      "y_pred= tensor([9.8699])\n",
      "332 : tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "w= 1.9524197578430176 \t b= 0.10816112905740738\n",
      "y_pred= tensor([9.8703])\n",
      "333 : tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "w= 1.9525341987609863 \t b= 0.10790111869573593\n",
      "y_pred= tensor([9.8706])\n",
      "334 : tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "w= 1.9526482820510864 \t b= 0.10764172673225403\n",
      "y_pred= tensor([9.8709])\n",
      "335 : tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "w= 1.9527621269226074 \t b= 0.10738296061754227\n",
      "y_pred= tensor([9.8712])\n",
      "336 : tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "w= 1.9528757333755493 \t b= 0.10712482035160065\n",
      "y_pred= tensor([9.8715])\n",
      "337 : tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "w= 1.9529889822006226 \t b= 0.10686729103326797\n",
      "y_pred= tensor([9.8718])\n",
      "338 : tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "w= 1.9531019926071167 \t b= 0.10661038756370544\n",
      "y_pred= tensor([9.8721])\n",
      "339 : tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "w= 1.9532147645950317 \t b= 0.10635410249233246\n",
      "y_pred= tensor([9.8724])\n",
      "340 : tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "w= 1.9533271789550781 \t b= 0.10609842836856842\n",
      "y_pred= tensor([9.8727])\n",
      "341 : tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "w= 1.9534393548965454 \t b= 0.10584337264299393\n",
      "y_pred= tensor([9.8730])\n",
      "342 : tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "w= 1.9535512924194336 \t b= 0.10558892786502838\n",
      "y_pred= tensor([9.8733])\n",
      "343 : tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "w= 1.9536629915237427 \t b= 0.10533510148525238\n",
      "y_pred= tensor([9.8737])\n",
      "344 : tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "w= 1.953774333000183 \t b= 0.10508187860250473\n",
      "y_pred= tensor([9.8740])\n",
      "345 : tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "w= 1.9538854360580444 \t b= 0.10482926666736603\n",
      "y_pred= tensor([9.8743])\n",
      "346 : tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "w= 1.9539963006973267 \t b= 0.10457726567983627\n",
      "y_pred= tensor([9.8746])\n",
      "347 : tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "w= 1.9541069269180298 \t b= 0.10432586818933487\n",
      "y_pred= tensor([9.8749])\n",
      "348 : tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "w= 1.9542171955108643 \t b= 0.10407507419586182\n",
      "y_pred= tensor([9.8752])\n",
      "349 : tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "w= 1.9543272256851196 \t b= 0.10382488369941711\n",
      "y_pred= tensor([9.8755])\n",
      "350 : tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "w= 1.954437017440796 \t b= 0.10357529670000076\n",
      "y_pred= tensor([9.8758])\n",
      "351 : tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "w= 1.954546570777893 \t b= 0.10332631319761276\n",
      "y_pred= tensor([9.8761])\n",
      "352 : tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "w= 1.9546558856964111 \t b= 0.10307792574167252\n",
      "y_pred= tensor([9.8764])\n",
      "353 : tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "w= 1.9547648429870605 \t b= 0.10283013433218002\n",
      "y_pred= tensor([9.8767])\n",
      "354 : tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "w= 1.9548735618591309 \t b= 0.10258293896913528\n",
      "y_pred= tensor([9.8770])\n",
      "355 : tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "w= 1.954982042312622 \t b= 0.1023363396525383\n",
      "y_pred= tensor([9.8772])\n",
      "356 : tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "w= 1.9550902843475342 \t b= 0.10209032893180847\n",
      "y_pred= tensor([9.8775])\n",
      "357 : tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "w= 1.9551982879638672 \t b= 0.1018449142575264\n",
      "y_pred= tensor([9.8778])\n",
      "358 : tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "w= 1.9553059339523315 \t b= 0.10160008817911148\n",
      "y_pred= tensor([9.8781])\n",
      "359 : tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "w= 1.9554133415222168 \t b= 0.10135585069656372\n",
      "y_pred= tensor([9.8784])\n",
      "360 : tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "w= 1.955520510673523 \t b= 0.10111219435930252\n",
      "y_pred= tensor([9.8787])\n",
      "361 : tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "w= 1.95562744140625 \t b= 0.10086912661790848\n",
      "y_pred= tensor([9.8790])\n",
      "362 : tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "w= 1.955734133720398 \t b= 0.10062664747238159\n",
      "y_pred= tensor([9.8793])\n",
      "363 : tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "w= 1.9558405876159668 \t b= 0.10038474947214127\n",
      "y_pred= tensor([9.8796])\n",
      "364 : tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "w= 1.9559468030929565 \t b= 0.1001434326171875\n",
      "y_pred= tensor([9.8799])\n",
      "365 : tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "w= 1.9560526609420776 \t b= 0.0999026894569397\n",
      "y_pred= tensor([9.8802])\n",
      "366 : tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "w= 1.9561582803726196 \t b= 0.09966252744197845\n",
      "y_pred= tensor([9.8805])\n",
      "367 : tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "w= 1.9562636613845825 \t b= 0.09942294657230377\n",
      "y_pred= tensor([9.8807])\n",
      "368 : tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "w= 1.9563688039779663 \t b= 0.09918393939733505\n",
      "y_pred= tensor([9.8810])\n",
      "369 : tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "w= 1.956473708152771 \t b= 0.0989455133676529\n",
      "y_pred= tensor([9.8813])\n",
      "370 : tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "w= 1.9565783739089966 \t b= 0.0987076535820961\n",
      "y_pred= tensor([9.8816])\n",
      "371 : tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "w= 1.956682801246643 \t b= 0.09847036749124527\n",
      "y_pred= tensor([9.8819])\n",
      "372 : tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "w= 1.956786870956421 \t b= 0.0982336476445198\n",
      "y_pred= tensor([9.8822])\n",
      "373 : tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "w= 1.9568907022476196 \t b= 0.0979975014925003\n",
      "y_pred= tensor([9.8825])\n",
      "374 : tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "w= 1.9569942951202393 \t b= 0.09776192158460617\n",
      "y_pred= tensor([9.8827])\n",
      "375 : tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "w= 1.9570976495742798 \t b= 0.0975269079208374\n",
      "y_pred= tensor([9.8830])\n",
      "376 : tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "w= 1.9572007656097412 \t b= 0.097292460501194\n",
      "y_pred= tensor([9.8833])\n",
      "377 : tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "w= 1.9573036432266235 \t b= 0.09705857932567596\n",
      "y_pred= tensor([9.8836])\n",
      "378 : tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "w= 1.9574062824249268 \t b= 0.0968252569437027\n",
      "y_pred= tensor([9.8839])\n",
      "379 : tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "w= 1.9575086832046509 \t b= 0.0965925008058548\n",
      "y_pred= tensor([9.8841])\n",
      "380 : tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "w= 1.957610845565796 \t b= 0.09636030346155167\n",
      "y_pred= tensor([9.8844])\n",
      "381 : tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "w= 1.9577127695083618 \t b= 0.0961286649107933\n",
      "y_pred= tensor([9.8847])\n",
      "382 : tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "w= 1.9578144550323486 \t b= 0.09589758515357971\n",
      "y_pred= tensor([9.8850])\n",
      "383 : tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "w= 1.9579159021377563 \t b= 0.0956670492887497\n",
      "y_pred= tensor([9.8852])\n",
      "384 : tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "w= 1.958017110824585 \t b= 0.09543707221746445\n",
      "y_pred= tensor([9.8855])\n",
      "385 : tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "w= 1.9581180810928345 \t b= 0.09520764648914337\n",
      "y_pred= tensor([9.8858])\n",
      "386 : tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "w= 1.9582188129425049 \t b= 0.09497877210378647\n",
      "y_pred= tensor([9.8861])\n",
      "387 : tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "w= 1.9583191871643066 \t b= 0.09475044161081314\n",
      "y_pred= tensor([9.8863])\n",
      "388 : tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "w= 1.9584193229675293 \t b= 0.09452266246080399\n",
      "y_pred= tensor([9.8866])\n",
      "389 : tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "w= 1.9585193395614624 \t b= 0.0942954421043396\n",
      "y_pred= tensor([9.8869])\n",
      "390 : tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "w= 1.9586189985275269 \t b= 0.09406875818967819\n",
      "y_pred= tensor([9.8872])\n",
      "391 : tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "w= 1.9587185382843018 \t b= 0.09384262561798096\n",
      "y_pred= tensor([9.8874])\n",
      "392 : tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "w= 1.958817720413208 \t b= 0.0936170294880867\n",
      "y_pred= tensor([9.8877])\n",
      "393 : tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "w= 1.9589166641235352 \t b= 0.09339197725057602\n",
      "y_pred= tensor([9.8880])\n",
      "394 : tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "w= 1.9590154886245728 \t b= 0.09316747635602951\n",
      "y_pred= tensor([9.8882])\n",
      "395 : tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "w= 1.9591139554977417 \t b= 0.09294350445270538\n",
      "y_pred= tensor([9.8885])\n",
      "396 : tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "w= 1.959212303161621 \t b= 0.09272007644176483\n",
      "y_pred= tensor([9.8888])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "397 : tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "w= 1.9593104124069214 \t b= 0.09249718487262726\n",
      "y_pred= tensor([9.8890])\n",
      "398 : tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "w= 1.959408164024353 \t b= 0.09227482229471207\n",
      "y_pred= tensor([9.8893])\n",
      "399 : tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "w= 1.9595057964324951 \t b= 0.09205300360918045\n",
      "y_pred= tensor([9.8896])\n",
      "400 : tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "w= 1.959603190422058 \t b= 0.09183171391487122\n",
      "y_pred= tensor([9.8898])\n",
      "401 : tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "w= 1.959700345993042 \t b= 0.09161095321178436\n",
      "y_pred= tensor([9.8901])\n",
      "402 : tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "w= 1.9597972631454468 \t b= 0.09139072149991989\n",
      "y_pred= tensor([9.8904])\n",
      "403 : tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "w= 1.9598939418792725 \t b= 0.0911710187792778\n",
      "y_pred= tensor([9.8906])\n",
      "404 : tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "w= 1.959990382194519 \t b= 0.0909518450498581\n",
      "y_pred= tensor([9.8909])\n",
      "405 : tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "w= 1.9600865840911865 \t b= 0.09073319286108017\n",
      "y_pred= tensor([9.8912])\n",
      "406 : tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "w= 1.960182547569275 \t b= 0.09051506966352463\n",
      "y_pred= tensor([9.8914])\n",
      "407 : tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "w= 1.9602782726287842 \t b= 0.09029746800661087\n",
      "y_pred= tensor([9.8917])\n",
      "408 : tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "w= 1.9603737592697144 \t b= 0.0900803878903389\n",
      "y_pred= tensor([9.8919])\n",
      "409 : tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "w= 1.9604690074920654 \t b= 0.08986382931470871\n",
      "y_pred= tensor([9.8922])\n",
      "410 : tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "w= 1.9605640172958374 \t b= 0.0896477922797203\n",
      "y_pred= tensor([9.8925])\n",
      "411 : tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "w= 1.9606587886810303 \t b= 0.08943227678537369\n",
      "y_pred= tensor([9.8927])\n",
      "412 : tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "w= 1.960753321647644 \t b= 0.08921728283166885\n",
      "y_pred= tensor([9.8930])\n",
      "413 : tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "w= 1.9608476161956787 \t b= 0.08900280296802521\n",
      "y_pred= tensor([9.8932])\n",
      "414 : tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "w= 1.9609417915344238 \t b= 0.08878884464502335\n",
      "y_pred= tensor([9.8935])\n",
      "415 : tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "w= 1.9610356092453003 \t b= 0.08857539296150208\n",
      "y_pred= tensor([9.8938])\n",
      "416 : tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "w= 1.9611293077468872 \t b= 0.08836246281862259\n",
      "y_pred= tensor([9.8940])\n",
      "417 : tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "w= 1.961222767829895 \t b= 0.0881500393152237\n",
      "y_pred= tensor([9.8943])\n",
      "418 : tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "w= 1.9613159894943237 \t b= 0.08793812245130539\n",
      "y_pred= tensor([9.8945])\n",
      "419 : tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "w= 1.9614089727401733 \t b= 0.08772671967744827\n",
      "y_pred= tensor([9.8948])\n",
      "420 : tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "w= 1.9615017175674438 \t b= 0.08751583099365234\n",
      "y_pred= tensor([9.8950])\n",
      "421 : tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "w= 1.9615942239761353 \t b= 0.087305448949337\n",
      "y_pred= tensor([9.8953])\n",
      "422 : tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "w= 1.9616864919662476 \t b= 0.08709557354450226\n",
      "y_pred= tensor([9.8955])\n",
      "423 : tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "w= 1.9617786407470703 \t b= 0.0868862047791481\n",
      "y_pred= tensor([9.8958])\n",
      "424 : tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "w= 1.961870551109314 \t b= 0.08667733520269394\n",
      "y_pred= tensor([9.8960])\n",
      "425 : tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "w= 1.9619622230529785 \t b= 0.08646896481513977\n",
      "y_pred= tensor([9.8963])\n",
      "426 : tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "w= 1.962053656578064 \t b= 0.0862610936164856\n",
      "y_pred= tensor([9.8965])\n",
      "427 : tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "w= 1.9621448516845703 \t b= 0.08605372160673141\n",
      "y_pred= tensor([9.8968])\n",
      "428 : tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "w= 1.9622358083724976 \t b= 0.08584684878587723\n",
      "y_pred= tensor([9.8970])\n",
      "429 : tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "w= 1.9623266458511353 \t b= 0.08564048260450363\n",
      "y_pred= tensor([9.8973])\n",
      "430 : tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "w= 1.9624172449111938 \t b= 0.08543460816144943\n",
      "y_pred= tensor([9.8975])\n",
      "431 : tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "w= 1.9625076055526733 \t b= 0.08522922545671463\n",
      "y_pred= tensor([9.8978])\n",
      "432 : tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "w= 1.9625977277755737 \t b= 0.08502433449029922\n",
      "y_pred= tensor([9.8980])\n",
      "433 : tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "w= 1.962687611579895 \t b= 0.08481994271278381\n",
      "y_pred= tensor([9.8983])\n",
      "434 : tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "w= 1.9627772569656372 \t b= 0.0846160426735878\n",
      "y_pred= tensor([9.8985])\n",
      "435 : tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "w= 1.9628667831420898 \t b= 0.08441262692213058\n",
      "y_pred= tensor([9.8987])\n",
      "436 : tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "w= 1.9629560708999634 \t b= 0.08420970290899277\n",
      "y_pred= tensor([9.8990])\n",
      "437 : tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "w= 1.9630451202392578 \t b= 0.08400726318359375\n",
      "y_pred= tensor([9.8992])\n",
      "438 : tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "w= 1.9631339311599731 \t b= 0.08380531519651413\n",
      "y_pred= tensor([9.8995])\n",
      "439 : tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "w= 1.9632225036621094 \t b= 0.08360385149717331\n",
      "y_pred= tensor([9.8997])\n",
      "440 : tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "w= 1.963310956954956 \t b= 0.08340287208557129\n",
      "y_pred= tensor([9.9000])\n",
      "441 : tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "w= 1.9633991718292236 \t b= 0.08320237696170807\n",
      "y_pred= tensor([9.9002])\n",
      "442 : tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "w= 1.963487148284912 \t b= 0.08300235867500305\n",
      "y_pred= tensor([9.9004])\n",
      "443 : tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "w= 1.9635748863220215 \t b= 0.08280282467603683\n",
      "y_pred= tensor([9.9007])\n",
      "444 : tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "w= 1.9636625051498413 \t b= 0.08260377496480942\n",
      "y_pred= tensor([9.9009])\n",
      "445 : tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "w= 1.963749885559082 \t b= 0.0824052020907402\n",
      "y_pred= tensor([9.9012])\n",
      "446 : tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "w= 1.9638370275497437 \t b= 0.0822071060538292\n",
      "y_pred= tensor([9.9014])\n",
      "447 : tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "w= 1.9639239311218262 \t b= 0.08200948685407639\n",
      "y_pred= tensor([9.9016])\n",
      "448 : tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "w= 1.9640107154846191 \t b= 0.08181234449148178\n",
      "y_pred= tensor([9.9019])\n",
      "449 : tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "w= 1.964097261428833 \t b= 0.08161567151546478\n",
      "y_pred= tensor([9.9021])\n",
      "450 : tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "w= 1.9641835689544678 \t b= 0.08141946792602539\n",
      "y_pred= tensor([9.9023])\n",
      "451 : tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "w= 1.9642696380615234 \t b= 0.0812237337231636\n",
      "y_pred= tensor([9.9026])\n",
      "452 : tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "w= 1.96435546875 \t b= 0.08102846890687943\n",
      "y_pred= tensor([9.9028])\n",
      "453 : tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "w= 1.964441180229187 \t b= 0.08083368092775345\n",
      "y_pred= tensor([9.9030])\n",
      "454 : tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "w= 1.964526653289795 \t b= 0.08063936233520508\n",
      "y_pred= tensor([9.9033])\n",
      "455 : tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "w= 1.9646118879318237 \t b= 0.08044550567865372\n",
      "y_pred= tensor([9.9035])\n",
      "456 : tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "w= 1.964697003364563 \t b= 0.08025212585926056\n",
      "y_pred= tensor([9.9037])\n",
      "457 : tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "w= 1.9647818803787231 \t b= 0.08005920052528381\n",
      "y_pred= tensor([9.9040])\n",
      "458 : tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "w= 1.9648665189743042 \t b= 0.07986674457788467\n",
      "y_pred= tensor([9.9042])\n",
      "459 : tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "w= 1.9649509191513062 \t b= 0.07967475056648254\n",
      "y_pred= tensor([9.9044])\n",
      "460 : tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "w= 1.9650352001190186 \t b= 0.07948321849107742\n",
      "y_pred= tensor([9.9047])\n",
      "461 : tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "w= 1.9651192426681519 \t b= 0.07929214835166931\n",
      "y_pred= tensor([9.9049])\n",
      "462 : tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "w= 1.965203046798706 \t b= 0.07910153269767761\n",
      "y_pred= tensor([9.9051])\n",
      "463 : tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "w= 1.9652867317199707 \t b= 0.07891137897968292\n",
      "y_pred= tensor([9.9053])\n",
      "464 : tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "w= 1.9653701782226562 \t b= 0.07872168719768524\n",
      "y_pred= tensor([9.9056])\n",
      "465 : tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "w= 1.9654533863067627 \t b= 0.07853244990110397\n",
      "y_pred= tensor([9.9058])\n",
      "466 : tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "w= 1.9655364751815796 \t b= 0.07834366708993912\n",
      "y_pred= tensor([9.9060])\n",
      "467 : tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "w= 1.9656193256378174 \t b= 0.07815533131361008\n",
      "y_pred= tensor([9.9063])\n",
      "468 : tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "w= 1.965701937675476 \t b= 0.07796745002269745\n",
      "y_pred= tensor([9.9065])\n",
      "469 : tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "w= 1.9657844305038452 \t b= 0.07778002321720123\n",
      "y_pred= tensor([9.9067])\n",
      "470 : tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "w= 1.9658666849136353 \t b= 0.07759304344654083\n",
      "y_pred= tensor([9.9069])\n",
      "471 : tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "w= 1.9659487009048462 \t b= 0.07740651816129684\n",
      "y_pred= tensor([9.9072])\n",
      "472 : tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "w= 1.9660305976867676 \t b= 0.07722043991088867\n",
      "y_pred= tensor([9.9074])\n",
      "473 : tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "w= 1.9661122560501099 \t b= 0.07703480869531631\n",
      "y_pred= tensor([9.9076])\n",
      "474 : tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "w= 1.966193675994873 \t b= 0.07684962451457977\n",
      "y_pred= tensor([9.9078])\n",
      "475 : tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "w= 1.9662749767303467 \t b= 0.07666488736867905\n",
      "y_pred= tensor([9.9080])\n",
      "476 : tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "w= 1.9663560390472412 \t b= 0.07648058980703354\n",
      "y_pred= tensor([9.9083])\n",
      "477 : tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "w= 1.9664369821548462 \t b= 0.07629673928022385\n",
      "y_pred= tensor([9.9085])\n",
      "478 : tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "w= 1.966517686843872 \t b= 0.07611332833766937\n",
      "y_pred= tensor([9.9087])\n",
      "479 : tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "w= 1.9665981531143188 \t b= 0.07593035697937012\n",
      "y_pred= tensor([9.9089])\n",
      "480 : tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "w= 1.9666783809661865 \t b= 0.07574781775474548\n",
      "y_pred= tensor([9.9091])\n",
      "481 : tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "w= 1.9667584896087646 \t b= 0.07556572556495667\n",
      "y_pred= tensor([9.9094])\n",
      "482 : tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "w= 1.9668383598327637 \t b= 0.07538407295942307\n",
      "y_pred= tensor([9.9096])\n",
      "483 : tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "w= 1.9669181108474731 \t b= 0.07520285248756409\n",
      "y_pred= tensor([9.9098])\n",
      "484 : tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "w= 1.9669976234436035 \t b= 0.07502207159996033\n",
      "y_pred= tensor([9.9100])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "485 : tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "w= 1.9670770168304443 \t b= 0.07484172284603119\n",
      "y_pred= tensor([9.9102])\n",
      "486 : tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "w= 1.967156171798706 \t b= 0.07466181367635727\n",
      "y_pred= tensor([9.9104])\n",
      "487 : tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "w= 1.9672350883483887 \t b= 0.07448233664035797\n",
      "y_pred= tensor([9.9107])\n",
      "488 : tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "w= 1.9673138856887817 \t b= 0.0743032842874527\n",
      "y_pred= tensor([9.9109])\n",
      "489 : tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "w= 1.9673924446105957 \t b= 0.07412466406822205\n",
      "y_pred= tensor([9.9111])\n",
      "490 : tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "w= 1.9674708843231201 \t b= 0.07394646853208542\n",
      "y_pred= tensor([9.9113])\n",
      "491 : tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "w= 1.9675490856170654 \t b= 0.07376870512962341\n",
      "y_pred= tensor([9.9115])\n",
      "492 : tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "w= 1.9676270484924316 \t b= 0.07359137386083603\n",
      "y_pred= tensor([9.9117])\n",
      "493 : tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "w= 1.9677048921585083 \t b= 0.07341446727514267\n",
      "y_pred= tensor([9.9119])\n",
      "494 : tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "w= 1.9677824974060059 \t b= 0.07323798537254333\n",
      "y_pred= tensor([9.9122])\n",
      "495 : tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "w= 1.9678599834442139 \t b= 0.07306192815303802\n",
      "y_pred= tensor([9.9124])\n",
      "496 : tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "w= 1.9679372310638428 \t b= 0.07288628816604614\n",
      "y_pred= tensor([9.9126])\n",
      "497 : tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "w= 1.9680143594741821 \t b= 0.07271107286214828\n",
      "y_pred= tensor([9.9128])\n",
      "498 : tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "w= 1.9680912494659424 \t b= 0.07253627479076385\n",
      "y_pred= tensor([9.9130])\n",
      "499 : tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "w= 1.9681679010391235 \t b= 0.07236189395189285\n",
      "y_pred= tensor([9.9132])\n",
      "500 : tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "w= 1.9682444334030151 \t b= 0.07218793779611588\n",
      "y_pred= tensor([9.9134])\n",
      "501 : tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "w= 1.9683207273483276 \t b= 0.07201439887285233\n",
      "y_pred= tensor([9.9136])\n",
      "502 : tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "w= 1.9683969020843506 \t b= 0.0718412846326828\n",
      "y_pred= tensor([9.9138])\n",
      "503 : tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "w= 1.9684728384017944 \t b= 0.0716685876250267\n",
      "y_pred= tensor([9.9140])\n",
      "504 : tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "w= 1.9685486555099487 \t b= 0.07149630784988403\n",
      "y_pred= tensor([9.9142])\n",
      "505 : tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "w= 1.968624234199524 \t b= 0.0713244378566742\n",
      "y_pred= tensor([9.9144])\n",
      "506 : tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "w= 1.9686996936798096 \t b= 0.07115297764539719\n",
      "y_pred= tensor([9.9147])\n",
      "507 : tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "w= 1.9687749147415161 \t b= 0.07098192721605301\n",
      "y_pred= tensor([9.9149])\n",
      "508 : tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "w= 1.968850016593933 \t b= 0.07081129401922226\n",
      "y_pred= tensor([9.9151])\n",
      "509 : tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "w= 1.968924880027771 \t b= 0.07064106315374374\n",
      "y_pred= tensor([9.9153])\n",
      "510 : tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "w= 1.9689996242523193 \t b= 0.07047124207019806\n",
      "y_pred= tensor([9.9155])\n",
      "511 : tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "w= 1.9690741300582886 \t b= 0.0703018307685852\n",
      "y_pred= tensor([9.9157])\n",
      "512 : tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "w= 1.9691485166549683 \t b= 0.07013282924890518\n",
      "y_pred= tensor([9.9159])\n",
      "513 : tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "w= 1.9692226648330688 \t b= 0.06996423006057739\n",
      "y_pred= tensor([9.9161])\n",
      "514 : tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "w= 1.9692966938018799 \t b= 0.06979603320360184\n",
      "y_pred= tensor([9.9163])\n",
      "515 : tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "w= 1.9693704843521118 \t b= 0.06962823867797852\n",
      "y_pred= tensor([9.9165])\n",
      "516 : tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "w= 1.9694441556930542 \t b= 0.06946085393428802\n",
      "y_pred= tensor([9.9167])\n",
      "517 : tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "w= 1.9695175886154175 \t b= 0.06929387152194977\n",
      "y_pred= tensor([9.9169])\n",
      "518 : tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "w= 1.9695909023284912 \t b= 0.06912729144096375\n",
      "y_pred= tensor([9.9171])\n",
      "519 : tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "w= 1.9696639776229858 \t b= 0.06896111369132996\n",
      "y_pred= tensor([9.9173])\n",
      "520 : tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "w= 1.969736933708191 \t b= 0.0687953382730484\n",
      "y_pred= tensor([9.9175])\n",
      "521 : tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "w= 1.969809651374817 \t b= 0.06862995773553848\n",
      "y_pred= tensor([9.9177])\n",
      "522 : tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "w= 1.9698822498321533 \t b= 0.0684649720788002\n",
      "y_pred= tensor([9.9179])\n",
      "523 : tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "w= 1.9699546098709106 \t b= 0.06830038130283356\n",
      "y_pred= tensor([9.9181])\n",
      "524 : tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "w= 1.9700268507003784 \t b= 0.06813618540763855\n",
      "y_pred= tensor([9.9183])\n",
      "525 : tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "w= 1.970098853111267 \t b= 0.06797239184379578\n",
      "y_pred= tensor([9.9185])\n",
      "526 : tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "w= 1.9701707363128662 \t b= 0.06780899316072464\n",
      "y_pred= tensor([9.9187])\n",
      "527 : tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "w= 1.9702425003051758 \t b= 0.06764598190784454\n",
      "y_pred= tensor([9.9189])\n",
      "528 : tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "w= 1.9703140258789062 \t b= 0.06748335808515549\n",
      "y_pred= tensor([9.9191])\n",
      "529 : tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "w= 1.9703854322433472 \t b= 0.06732112914323807\n",
      "y_pred= tensor([9.9192])\n",
      "530 : tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "w= 1.970456600189209 \t b= 0.06715928763151169\n",
      "y_pred= tensor([9.9194])\n",
      "531 : tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "w= 1.9705276489257812 \t b= 0.06699784100055695\n",
      "y_pred= tensor([9.9196])\n",
      "532 : tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "w= 1.9705984592437744 \t b= 0.06683677434921265\n",
      "y_pred= tensor([9.9198])\n",
      "533 : tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "w= 1.970669150352478 \t b= 0.06667609512805939\n",
      "y_pred= tensor([9.9200])\n",
      "534 : tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "w= 1.9707396030426025 \t b= 0.06651581078767776\n",
      "y_pred= tensor([9.9202])\n",
      "535 : tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "w= 1.9708099365234375 \t b= 0.06635590642690659\n",
      "y_pred= tensor([9.9204])\n",
      "536 : tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "w= 1.970880150794983 \t b= 0.06619639694690704\n",
      "y_pred= tensor([9.9206])\n",
      "537 : tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "w= 1.9709501266479492 \t b= 0.06603725999593735\n",
      "y_pred= tensor([9.9208])\n",
      "538 : tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "w= 1.971019983291626 \t b= 0.06587851047515869\n",
      "y_pred= tensor([9.9210])\n",
      "539 : tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "w= 1.9710896015167236 \t b= 0.06572014093399048\n",
      "y_pred= tensor([9.9212])\n",
      "540 : tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "w= 1.9711591005325317 \t b= 0.06556215137243271\n",
      "y_pred= tensor([9.9214])\n",
      "541 : tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "w= 1.9712284803390503 \t b= 0.06540454179048538\n",
      "y_pred= tensor([9.9215])\n",
      "542 : tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "w= 1.9712976217269897 \t b= 0.0652473121881485\n",
      "y_pred= tensor([9.9217])\n",
      "543 : tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "w= 1.9713666439056396 \t b= 0.06509046256542206\n",
      "y_pred= tensor([9.9219])\n",
      "544 : tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "w= 1.9714354276657104 \t b= 0.06493398547172546\n",
      "y_pred= tensor([9.9221])\n",
      "545 : tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "w= 1.9715040922164917 \t b= 0.06477788835763931\n",
      "y_pred= tensor([9.9223])\n",
      "546 : tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "w= 1.9715726375579834 \t b= 0.06462216377258301\n",
      "y_pred= tensor([9.9225])\n",
      "547 : tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "w= 1.971640944480896 \t b= 0.06446681916713715\n",
      "y_pred= tensor([9.9227])\n",
      "548 : tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "w= 1.971709132194519 \t b= 0.06431184709072113\n",
      "y_pred= tensor([9.9229])\n",
      "549 : tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "w= 1.971777081489563 \t b= 0.06415724754333496\n",
      "y_pred= tensor([9.9230])\n",
      "550 : tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "w= 1.9718449115753174 \t b= 0.06400302052497864\n",
      "y_pred= tensor([9.9232])\n",
      "551 : tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "w= 1.9719126224517822 \t b= 0.06384916603565216\n",
      "y_pred= tensor([9.9234])\n",
      "552 : tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "w= 1.971980094909668 \t b= 0.06369567662477493\n",
      "y_pred= tensor([9.9236])\n",
      "553 : tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "w= 1.9720474481582642 \t b= 0.06354255974292755\n",
      "y_pred= tensor([9.9238])\n",
      "554 : tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "w= 1.9721146821975708 \t b= 0.06338980793952942\n",
      "y_pred= tensor([9.9240])\n",
      "555 : tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "w= 1.9721816778182983 \t b= 0.06323742121458054\n",
      "y_pred= tensor([9.9241])\n",
      "556 : tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "w= 1.9722485542297363 \t b= 0.0630854070186615\n",
      "y_pred= tensor([9.9243])\n",
      "557 : tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "w= 1.9723153114318848 \t b= 0.06293375790119171\n",
      "y_pred= tensor([9.9245])\n",
      "558 : tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "w= 1.972381830215454 \t b= 0.06278246641159058\n",
      "y_pred= tensor([9.9247])\n",
      "559 : tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "w= 1.9724482297897339 \t b= 0.06263154745101929\n",
      "y_pred= tensor([9.9249])\n",
      "560 : tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "w= 1.9725145101547241 \t b= 0.06248098984360695\n",
      "y_pred= tensor([9.9251])\n",
      "561 : tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "w= 1.9725805521011353 \t b= 0.06233078986406326\n",
      "y_pred= tensor([9.9252])\n",
      "562 : tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "w= 1.9726464748382568 \t b= 0.06218095123767853\n",
      "y_pred= tensor([9.9254])\n",
      "563 : tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "w= 1.9727122783660889 \t b= 0.062031473964452744\n",
      "y_pred= tensor([9.9256])\n",
      "564 : tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "w= 1.9727778434753418 \t b= 0.06188235804438591\n",
      "y_pred= tensor([9.9258])\n",
      "565 : tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "w= 1.9728432893753052 \t b= 0.06173359602689743\n",
      "y_pred= tensor([9.9260])\n",
      "566 : tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "w= 1.972908616065979 \t b= 0.0615851916372776\n",
      "y_pred= tensor([9.9261])\n",
      "567 : tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "w= 1.9729737043380737 \t b= 0.06143714115023613\n",
      "y_pred= tensor([9.9263])\n",
      "568 : tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "w= 1.973038673400879 \t b= 0.06128944829106331\n",
      "y_pred= tensor([9.9265])\n",
      "569 : tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "w= 1.9731035232543945 \t b= 0.06114211305975914\n",
      "y_pred= tensor([9.9267])\n",
      "570 : tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "w= 1.973168134689331 \t b= 0.060995131731033325\n",
      "y_pred= tensor([9.9268])\n",
      "571 : tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "w= 1.973232626914978 \t b= 0.060848504304885864\n",
      "y_pred= tensor([9.9270])\n",
      "572 : tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "w= 1.9732969999313354 \t b= 0.06070222705602646\n",
      "y_pred= tensor([9.9272])\n",
      "573 : tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "w= 1.9733611345291138 \t b= 0.06055630370974541\n",
      "y_pred= tensor([9.9274])\n",
      "574 : tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "w= 1.9734251499176025 \t b= 0.06041073054075241\n",
      "y_pred= tensor([9.9275])\n",
      "575 : tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "w= 1.9734890460968018 \t b= 0.06026551127433777\n",
      "y_pred= tensor([9.9277])\n",
      "576 : tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "w= 1.9735528230667114 \t b= 0.06012063845992088\n",
      "y_pred= tensor([9.9279])\n",
      "577 : tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "w= 1.973616361618042 \t b= 0.059976112097501755\n",
      "y_pred= tensor([9.9281])\n",
      "578 : tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "w= 1.973679780960083 \t b= 0.05983193591237068\n",
      "y_pred= tensor([9.9282])\n",
      "579 : tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "w= 1.9737430810928345 \t b= 0.05968810245394707\n",
      "y_pred= tensor([9.9284])\n",
      "580 : tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "w= 1.9738061428070068 \t b= 0.05954461544752121\n",
      "y_pred= tensor([9.9286])\n",
      "581 : tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "w= 1.9738690853118896 \t b= 0.05940147861838341\n",
      "y_pred= tensor([9.9287])\n",
      "582 : tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "w= 1.973931908607483 \t b= 0.05925868824124336\n",
      "y_pred= tensor([9.9289])\n",
      "583 : tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "w= 1.9739946126937866 \t b= 0.059116240590810776\n",
      "y_pred= tensor([9.9291])\n",
      "584 : tensor(0.0005, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w= 1.9740570783615112 \t b= 0.05897413194179535\n",
      "y_pred= tensor([9.9293])\n",
      "585 : tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "w= 1.9741194248199463 \t b= 0.05883236601948738\n",
      "y_pred= tensor([9.9294])\n",
      "586 : tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "w= 1.9741816520690918 \t b= 0.05869094282388687\n",
      "y_pred= tensor([9.9296])\n",
      "587 : tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "w= 1.9742437601089478 \t b= 0.05854985490441322\n",
      "y_pred= tensor([9.9298])\n",
      "588 : tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "w= 1.9743056297302246 \t b= 0.058409109711647034\n",
      "y_pred= tensor([9.9299])\n",
      "589 : tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "w= 1.974367380142212 \t b= 0.058268699795007706\n",
      "y_pred= tensor([9.9301])\n",
      "590 : tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "w= 1.9744290113449097 \t b= 0.05812862887978554\n",
      "y_pred= tensor([9.9303])\n",
      "591 : tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "w= 1.9744905233383179 \t b= 0.05798889324069023\n",
      "y_pred= tensor([9.9304])\n",
      "592 : tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "w= 1.974551796913147 \t b= 0.057849492877721786\n",
      "y_pred= tensor([9.9306])\n",
      "593 : tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "w= 1.9746129512786865 \t b= 0.0577104315161705\n",
      "y_pred= tensor([9.9308])\n",
      "594 : tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "w= 1.9746739864349365 \t b= 0.05757170543074608\n",
      "y_pred= tensor([9.9309])\n",
      "595 : tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "w= 1.974734902381897 \t b= 0.05743331089615822\n",
      "y_pred= tensor([9.9311])\n",
      "596 : tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "w= 1.9747956991195679 \t b= 0.05729524791240692\n",
      "y_pred= tensor([9.9313])\n",
      "597 : tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "w= 1.9748562574386597 \t b= 0.05715751647949219\n",
      "y_pred= tensor([9.9314])\n",
      "598 : tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "w= 1.974916696548462 \t b= 0.05702011287212372\n",
      "y_pred= tensor([9.9316])\n",
      "599 : tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "w= 1.9749770164489746 \t b= 0.05688304081559181\n",
      "y_pred= tensor([9.9318])\n",
      "600 : tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "w= 1.9750372171401978 \t b= 0.05674630030989647\n",
      "y_pred= tensor([9.9319])\n",
      "601 : tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "w= 1.9750971794128418 \t b= 0.05660988390445709\n",
      "y_pred= tensor([9.9321])\n",
      "602 : tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "w= 1.9751570224761963 \t b= 0.05647379532456398\n",
      "y_pred= tensor([9.9323])\n",
      "603 : tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "w= 1.9752167463302612 \t b= 0.05633803829550743\n",
      "y_pred= tensor([9.9324])\n",
      "604 : tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "w= 1.9752763509750366 \t b= 0.05620260909199715\n",
      "y_pred= tensor([9.9326])\n",
      "605 : tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "w= 1.9753358364105225 \t b= 0.05606750398874283\n",
      "y_pred= tensor([9.9327])\n",
      "606 : tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "w= 1.9753950834274292 \t b= 0.055932722985744476\n",
      "y_pred= tensor([9.9329])\n",
      "607 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.9754542112350464 \t b= 0.05579826608300209\n",
      "y_pred= tensor([9.9331])\n",
      "608 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.975513219833374 \t b= 0.05566413328051567\n",
      "y_pred= tensor([9.9332])\n",
      "609 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.975572109222412 \t b= 0.05553032457828522\n",
      "y_pred= tensor([9.9334])\n",
      "610 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.9756308794021606 \t b= 0.05539683252573013\n",
      "y_pred= tensor([9.9336])\n",
      "611 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.97568941116333 \t b= 0.05526365712285042\n",
      "y_pred= tensor([9.9337])\n",
      "612 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.97574782371521 \t b= 0.05513080954551697\n",
      "y_pred= tensor([9.9339])\n",
      "613 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.9758061170578003 \t b= 0.054998282343149185\n",
      "y_pred= tensor([9.9340])\n",
      "614 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.975864291191101 \t b= 0.05486607179045677\n",
      "y_pred= tensor([9.9342])\n",
      "615 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.9759223461151123 \t b= 0.054734181612730026\n",
      "y_pred= tensor([9.9343])\n",
      "616 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.975980281829834 \t b= 0.05460260435938835\n",
      "y_pred= tensor([9.9345])\n",
      "617 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.9760379791259766 \t b= 0.05447134003043175\n",
      "y_pred= tensor([9.9347])\n",
      "618 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.9760955572128296 \t b= 0.05434039235115051\n",
      "y_pred= tensor([9.9348])\n",
      "619 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.976153016090393 \t b= 0.05420976132154465\n",
      "y_pred= tensor([9.9350])\n",
      "620 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.976210355758667 \t b= 0.05407944321632385\n",
      "y_pred= tensor([9.9351])\n",
      "621 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.9762675762176514 \t b= 0.05394944176077843\n",
      "y_pred= tensor([9.9353])\n",
      "622 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.9763246774673462 \t b= 0.05381975322961807\n",
      "y_pred= tensor([9.9354])\n",
      "623 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.976381540298462 \t b= 0.05369037017226219\n",
      "y_pred= tensor([9.9356])\n",
      "624 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.976438283920288 \t b= 0.05356130003929138\n",
      "y_pred= tensor([9.9358])\n",
      "625 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.9764949083328247 \t b= 0.05343254655599594\n",
      "y_pred= tensor([9.9359])\n",
      "626 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.9765514135360718 \t b= 0.053304098546504974\n",
      "y_pred= tensor([9.9361])\n",
      "627 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.9766077995300293 \t b= 0.05317595973610878\n",
      "y_pred= tensor([9.9362])\n",
      "628 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.9766640663146973 \t b= 0.05304813012480736\n",
      "y_pred= tensor([9.9364])\n",
      "629 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.9767202138900757 \t b= 0.05292060598731041\n",
      "y_pred= tensor([9.9365])\n",
      "630 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.976776123046875 \t b= 0.05279338359832764\n",
      "y_pred= tensor([9.9367])\n",
      "631 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.9768319129943848 \t b= 0.052666470408439636\n",
      "y_pred= tensor([9.9368])\n",
      "632 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.976887583732605 \t b= 0.05253986269235611\n",
      "y_pred= tensor([9.9370])\n",
      "633 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.9769431352615356 \t b= 0.05241356045007706\n",
      "y_pred= tensor([9.9371])\n",
      "634 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.9769985675811768 \t b= 0.05228756368160248\n",
      "y_pred= tensor([9.9373])\n",
      "635 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.9770538806915283 \t b= 0.052161868661642075\n",
      "y_pred= tensor([9.9374])\n",
      "636 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.9771090745925903 \t b= 0.05203647539019585\n",
      "y_pred= tensor([9.9376])\n",
      "637 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.9771641492843628 \t b= 0.051911383867263794\n",
      "y_pred= tensor([9.9377])\n",
      "638 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.9772189855575562 \t b= 0.05178659036755562\n",
      "y_pred= tensor([9.9379])\n",
      "639 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.97727370262146 \t b= 0.051662102341651917\n",
      "y_pred= tensor([9.9380])\n",
      "640 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.9773283004760742 \t b= 0.05153791606426239\n",
      "y_pred= tensor([9.9382])\n",
      "641 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.977382779121399 \t b= 0.05141402408480644\n",
      "y_pred= tensor([9.9383])\n",
      "642 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.977437138557434 \t b= 0.05129043012857437\n",
      "y_pred= tensor([9.9385])\n",
      "643 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.9774913787841797 \t b= 0.05116713419556618\n",
      "y_pred= tensor([9.9386])\n",
      "644 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.9775454998016357 \t b= 0.05104413628578186\n",
      "y_pred= tensor([9.9388])\n",
      "645 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.9775995016098022 \t b= 0.05092143639922142\n",
      "y_pred= tensor([9.9389])\n",
      "646 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.9776533842086792 \t b= 0.05079903081059456\n",
      "y_pred= tensor([9.9391])\n",
      "647 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.9777071475982666 \t b= 0.05067691579461098\n",
      "y_pred= tensor([9.9392])\n",
      "648 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.9777607917785645 \t b= 0.050555091351270676\n",
      "y_pred= tensor([9.9394])\n",
      "649 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.9778141975402832 \t b= 0.050433557480573654\n",
      "y_pred= tensor([9.9395])\n",
      "650 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.9778674840927124 \t b= 0.05031231790781021\n",
      "y_pred= tensor([9.9396])\n",
      "651 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.977920651435852 \t b= 0.05019137263298035\n",
      "y_pred= tensor([9.9398])\n",
      "652 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.9779736995697021 \t b= 0.05007072165608406\n",
      "y_pred= tensor([9.9399])\n",
      "653 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.9780266284942627 \t b= 0.049950361251831055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred= tensor([9.9401])\n",
      "654 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.9780794382095337 \t b= 0.04983028769493103\n",
      "y_pred= tensor([9.9402])\n",
      "655 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.9781321287155151 \t b= 0.049710504710674286\n",
      "y_pred= tensor([9.9404])\n",
      "656 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.978184700012207 \t b= 0.04959101229906082\n",
      "y_pred= tensor([9.9405])\n",
      "657 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.9782371520996094 \t b= 0.04947180300951004\n",
      "y_pred= tensor([9.9407])\n",
      "658 : tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "w= 1.9782894849777222 \t b= 0.04935288056731224\n",
      "y_pred= tensor([9.9408])\n",
      "659 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9783416986465454 \t b= 0.04923424497246742\n",
      "y_pred= tensor([9.9409])\n",
      "660 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.978393793106079 \t b= 0.04911589249968529\n",
      "y_pred= tensor([9.9411])\n",
      "661 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9784457683563232 \t b= 0.048997823148965836\n",
      "y_pred= tensor([9.9412])\n",
      "662 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9784976243972778 \t b= 0.04888003692030907\n",
      "y_pred= tensor([9.9414])\n",
      "663 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9785493612289429 \t b= 0.04876253008842468\n",
      "y_pred= tensor([9.9415])\n",
      "664 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9786009788513184 \t b= 0.04864530637860298\n",
      "y_pred= tensor([9.9417])\n",
      "665 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9786523580551147 \t b= 0.04852835834026337\n",
      "y_pred= tensor([9.9418])\n",
      "666 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9787036180496216 \t b= 0.04841169714927673\n",
      "y_pred= tensor([9.9419])\n",
      "667 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9787547588348389 \t b= 0.048295315355062485\n",
      "y_pred= tensor([9.9421])\n",
      "668 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9788057804107666 \t b= 0.04817921668291092\n",
      "y_pred= tensor([9.9422])\n",
      "669 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9788566827774048 \t b= 0.04806340113282204\n",
      "y_pred= tensor([9.9423])\n",
      "670 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9789074659347534 \t b= 0.04794786497950554\n",
      "y_pred= tensor([9.9425])\n",
      "671 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.978958249092102 \t b= 0.047832611948251724\n",
      "y_pred= tensor([9.9426])\n",
      "672 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9790087938308716 \t b= 0.0477176308631897\n",
      "y_pred= tensor([9.9428])\n",
      "673 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9790592193603516 \t b= 0.047602925449609756\n",
      "y_pred= tensor([9.9429])\n",
      "674 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.979109525680542 \t b= 0.0474884957075119\n",
      "y_pred= tensor([9.9430])\n",
      "675 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9791597127914429 \t b= 0.04737434536218643\n",
      "y_pred= tensor([9.9432])\n",
      "676 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9792097806930542 \t b= 0.04726047068834305\n",
      "y_pred= tensor([9.9433])\n",
      "677 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.979259729385376 \t b= 0.04714687168598175\n",
      "y_pred= tensor([9.9434])\n",
      "678 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9793095588684082 \t b= 0.04703354462981224\n",
      "y_pred= tensor([9.9436])\n",
      "679 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9793593883514404 \t b= 0.04692049324512482\n",
      "y_pred= tensor([9.9437])\n",
      "680 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9794089794158936 \t b= 0.04680771008133888\n",
      "y_pred= tensor([9.9439])\n",
      "681 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9794584512710571 \t b= 0.046695198863744736\n",
      "y_pred= tensor([9.9440])\n",
      "682 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9795078039169312 \t b= 0.04658295959234238\n",
      "y_pred= tensor([9.9441])\n",
      "683 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9795570373535156 \t b= 0.04647098854184151\n",
      "y_pred= tensor([9.9443])\n",
      "684 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9796061515808105 \t b= 0.046359285712242126\n",
      "y_pred= tensor([9.9444])\n",
      "685 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9796552658081055 \t b= 0.04624785855412483\n",
      "y_pred= tensor([9.9445])\n",
      "686 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9797041416168213 \t b= 0.04613668844103813\n",
      "y_pred= tensor([9.9447])\n",
      "687 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9797528982162476 \t b= 0.04602578654885292\n",
      "y_pred= tensor([9.9448])\n",
      "688 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9798016548156738 \t b= 0.0459151528775692\n",
      "y_pred= tensor([9.9449])\n",
      "689 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.979850172996521 \t b= 0.04580478370189667\n",
      "y_pred= tensor([9.9451])\n",
      "690 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9798985719680786 \t b= 0.04569467902183533\n",
      "y_pred= tensor([9.9452])\n",
      "691 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9799469709396362 \t b= 0.045584842562675476\n",
      "y_pred= tensor([9.9453])\n",
      "692 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9799952507019043 \t b= 0.04547526687383652\n",
      "y_pred= tensor([9.9455])\n",
      "693 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9800432920455933 \t b= 0.04536594823002815\n",
      "y_pred= tensor([9.9456])\n",
      "694 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9800913333892822 \t b= 0.04525689780712128\n",
      "y_pred= tensor([9.9457])\n",
      "695 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.980139136314392 \t b= 0.045148104429244995\n",
      "y_pred= tensor([9.9458])\n",
      "696 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.980186939239502 \t b= 0.0450395792722702\n",
      "y_pred= tensor([9.9460])\n",
      "697 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9802346229553223 \t b= 0.044931307435035706\n",
      "y_pred= tensor([9.9461])\n",
      "698 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.980282187461853 \t b= 0.0448232926428318\n",
      "y_pred= tensor([9.9462])\n",
      "699 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9803296327590942 \t b= 0.04471553862094879\n",
      "y_pred= tensor([9.9464])\n",
      "700 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.980376958847046 \t b= 0.044608041644096375\n",
      "y_pred= tensor([9.9465])\n",
      "701 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.980424165725708 \t b= 0.04450080171227455\n",
      "y_pred= tensor([9.9466])\n",
      "702 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9804712533950806 \t b= 0.044393815100193024\n",
      "y_pred= tensor([9.9467])\n",
      "703 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9805182218551636 \t b= 0.04428708553314209\n",
      "y_pred= tensor([9.9469])\n",
      "704 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.980565071105957 \t b= 0.04418061673641205\n",
      "y_pred= tensor([9.9470])\n",
      "705 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.980611801147461 \t b= 0.0440744049847126\n",
      "y_pred= tensor([9.9471])\n",
      "706 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9806584119796753 \t b= 0.04396844282746315\n",
      "y_pred= tensor([9.9473])\n",
      "707 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9807049036026 \t b= 0.04386273771524429\n",
      "y_pred= tensor([9.9474])\n",
      "708 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9807512760162354 \t b= 0.04375728592276573\n",
      "y_pred= tensor([9.9475])\n",
      "709 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.980797529220581 \t b= 0.043652087450027466\n",
      "y_pred= tensor([9.9476])\n",
      "710 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9808436632156372 \t b= 0.043547146022319794\n",
      "y_pred= tensor([9.9478])\n",
      "711 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9808896780014038 \t b= 0.04344245791435242\n",
      "y_pred= tensor([9.9479])\n",
      "712 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9809355735778809 \t b= 0.043338023126125336\n",
      "y_pred= tensor([9.9480])\n",
      "713 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9809813499450684 \t b= 0.04323384165763855\n",
      "y_pred= tensor([9.9481])\n",
      "714 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9810271263122559 \t b= 0.04312990978360176\n",
      "y_pred= tensor([9.9483])\n",
      "715 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9810727834701538 \t b= 0.04302622750401497\n",
      "y_pred= tensor([9.9484])\n",
      "716 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9811183214187622 \t b= 0.042922791093587875\n",
      "y_pred= tensor([9.9485])\n",
      "717 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.981163740158081 \t b= 0.04281960055232048\n",
      "y_pred= tensor([9.9486])\n",
      "718 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9812090396881104 \t b= 0.04271666333079338\n",
      "y_pred= tensor([9.9488])\n",
      "719 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.98125422000885 \t b= 0.04261396825313568\n",
      "y_pred= tensor([9.9489])\n",
      "720 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9812992811203003 \t b= 0.04251152276992798\n",
      "y_pred= tensor([9.9490])\n",
      "721 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.981344223022461 \t b= 0.042409319430589676\n",
      "y_pred= tensor([9.9491])\n",
      "722 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.981389045715332 \t b= 0.04230736196041107\n",
      "y_pred= tensor([9.9493])\n",
      "723 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9814337491989136 \t b= 0.042205654084682465\n",
      "y_pred= tensor([9.9494])\n",
      "724 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9814783334732056 \t b= 0.042104192078113556\n",
      "y_pred= tensor([9.9495])\n",
      "725 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.981522798538208 \t b= 0.042002975940704346\n",
      "y_pred= tensor([9.9496])\n",
      "726 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9815672636032104 \t b= 0.041902001947164536\n",
      "y_pred= tensor([9.9497])\n",
      "727 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9816116094589233 \t b= 0.041801273822784424\n",
      "y_pred= tensor([9.9499])\n",
      "728 : tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "w= 1.9816558361053467 \t b= 0.041700784116983414\n",
      "y_pred= tensor([9.9500])\n",
      "729 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9816999435424805 \t b= 0.041600536555051804\n",
      "y_pred= tensor([9.9501])\n",
      "730 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9817439317703247 \t b= 0.041500527411699295\n",
      "y_pred= tensor([9.9502])\n",
      "731 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9817878007888794 \t b= 0.041400760412216187\n",
      "y_pred= tensor([9.9503])\n",
      "732 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9818315505981445 \t b= 0.04130123183131218\n",
      "y_pred= tensor([9.9505])\n",
      "733 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9818751811981201 \t b= 0.04120194539427757\n",
      "y_pred= tensor([9.9506])\n",
      "734 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9819188117980957 \t b= 0.041102901101112366\n",
      "y_pred= tensor([9.9507])\n",
      "735 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9819623231887817 \t b= 0.04100409150123596\n",
      "y_pred= tensor([9.9508])\n",
      "736 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9820057153701782 \t b= 0.04090551659464836\n",
      "y_pred= tensor([9.9509])\n",
      "737 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9820489883422852 \t b= 0.040807176381349564\n",
      "y_pred= tensor([9.9511])\n",
      "738 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9820921421051025 \t b= 0.04070907086133957\n",
      "y_pred= tensor([9.9512])\n",
      "739 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9821351766586304 \t b= 0.040611203759908676\n",
      "y_pred= tensor([9.9513])\n",
      "740 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9821780920028687 \t b= 0.040513575077056885\n",
      "y_pred= tensor([9.9514])\n",
      "741 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9822208881378174 \t b= 0.040416181087493896\n",
      "y_pred= tensor([9.9515])\n",
      "742 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9822635650634766 \t b= 0.04031901806592941\n",
      "y_pred= tensor([9.9516])\n",
      "743 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9823062419891357 \t b= 0.04022209718823433\n",
      "y_pred= tensor([9.9518])\n",
      "744 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9823487997055054 \t b= 0.04012540727853775\n",
      "y_pred= tensor([9.9519])\n",
      "745 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9823912382125854 \t b= 0.04002894461154938\n",
      "y_pred= tensor([9.9520])\n",
      "746 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.982433557510376 \t b= 0.03993271663784981\n",
      "y_pred= tensor([9.9521])\n",
      "747 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.982475757598877 \t b= 0.03983671963214874\n",
      "y_pred= tensor([9.9522])\n",
      "748 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9825178384780884 \t b= 0.03974095359444618\n",
      "y_pred= tensor([9.9523])\n",
      "749 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9825599193572998 \t b= 0.039645422250032425\n",
      "y_pred= tensor([9.9524])\n",
      "750 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9826018810272217 \t b= 0.03955012187361717\n",
      "y_pred= tensor([9.9526])\n",
      "751 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.982643723487854 \t b= 0.03945504501461983\n",
      "y_pred= tensor([9.9527])\n",
      "752 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9826854467391968 \t b= 0.03936019539833069\n",
      "y_pred= tensor([9.9528])\n",
      "753 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.98272705078125 \t b= 0.039265576750040054\n",
      "y_pred= tensor([9.9529])\n",
      "754 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9827685356140137 \t b= 0.03917118161916733\n",
      "y_pred= tensor([9.9530])\n",
      "755 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9828099012374878 \t b= 0.039077017456293106\n",
      "y_pred= tensor([9.9531])\n",
      "756 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.982851266860962 \t b= 0.03898308426141739\n",
      "y_pred= tensor([9.9532])\n",
      "757 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9828925132751465 \t b= 0.03888937458395958\n",
      "y_pred= tensor([9.9534])\n",
      "758 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9829336404800415 \t b= 0.03879588469862938\n",
      "y_pred= tensor([9.9535])\n",
      "759 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.982974648475647 \t b= 0.038702622056007385\n",
      "y_pred= tensor([9.9536])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "760 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.983015537261963 \t b= 0.0386095829308033\n",
      "y_pred= tensor([9.9537])\n",
      "761 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9830564260482788 \t b= 0.03851677104830742\n",
      "y_pred= tensor([9.9538])\n",
      "762 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9830971956253052 \t b= 0.03842417895793915\n",
      "y_pred= tensor([9.9539])\n",
      "763 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.983137845993042 \t b= 0.038331806659698486\n",
      "y_pred= tensor([9.9540])\n",
      "764 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9831783771514893 \t b= 0.03823965787887573\n",
      "y_pred= tensor([9.9541])\n",
      "765 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.983218789100647 \t b= 0.03814772889018059\n",
      "y_pred= tensor([9.9542])\n",
      "766 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9832590818405151 \t b= 0.03805601969361305\n",
      "y_pred= tensor([9.9544])\n",
      "767 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9832993745803833 \t b= 0.03796453773975372\n",
      "y_pred= tensor([9.9545])\n",
      "768 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.983339548110962 \t b= 0.037873271852731705\n",
      "y_pred= tensor([9.9546])\n",
      "769 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.983379602432251 \t b= 0.037782222032547\n",
      "y_pred= tensor([9.9547])\n",
      "770 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9834195375442505 \t b= 0.0376913920044899\n",
      "y_pred= tensor([9.9548])\n",
      "771 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9834593534469604 \t b= 0.03760078176856041\n",
      "y_pred= tensor([9.9549])\n",
      "772 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9834991693496704 \t b= 0.03751039132475853\n",
      "y_pred= tensor([9.9550])\n",
      "773 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9835388660430908 \t b= 0.03742021694779396\n",
      "y_pred= tensor([9.9551])\n",
      "774 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9835784435272217 \t b= 0.037330254912376404\n",
      "y_pred= tensor([9.9552])\n",
      "775 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.983617901802063 \t b= 0.037240516394376755\n",
      "y_pred= tensor([9.9553])\n",
      "776 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9836572408676147 \t b= 0.03715098649263382\n",
      "y_pred= tensor([9.9554])\n",
      "777 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9836965799331665 \t b= 0.037061676383018494\n",
      "y_pred= tensor([9.9555])\n",
      "778 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9837357997894287 \t b= 0.03697258234024048\n",
      "y_pred= tensor([9.9557])\n",
      "779 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9837749004364014 \t b= 0.036883700639009476\n",
      "y_pred= tensor([9.9558])\n",
      "780 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9838138818740845 \t b= 0.036795031279325485\n",
      "y_pred= tensor([9.9559])\n",
      "781 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.983852744102478 \t b= 0.03670657426118851\n",
      "y_pred= tensor([9.9560])\n",
      "782 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9838916063308716 \t b= 0.03661833330988884\n",
      "y_pred= tensor([9.9561])\n",
      "783 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9839303493499756 \t b= 0.036530300974845886\n",
      "y_pred= tensor([9.9562])\n",
      "784 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.98396897315979 \t b= 0.036442480981349945\n",
      "y_pred= tensor([9.9563])\n",
      "785 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.984007477760315 \t b= 0.036354873329401016\n",
      "y_pred= tensor([9.9564])\n",
      "786 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9840458631515503 \t b= 0.0362674742937088\n",
      "y_pred= tensor([9.9565])\n",
      "787 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9840842485427856 \t b= 0.0361802913248539\n",
      "y_pred= tensor([9.9566])\n",
      "788 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9841225147247314 \t b= 0.03609331324696541\n",
      "y_pred= tensor([9.9567])\n",
      "789 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9841606616973877 \t b= 0.03600654751062393\n",
      "y_pred= tensor([9.9568])\n",
      "790 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9841986894607544 \t b= 0.03591999039053917\n",
      "y_pred= tensor([9.9569])\n",
      "791 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.984236717224121 \t b= 0.03583364188671112\n",
      "y_pred= tensor([9.9570])\n",
      "792 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9842746257781982 \t b= 0.03574749827384949\n",
      "y_pred= tensor([9.9571])\n",
      "793 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9843124151229858 \t b= 0.03566156327724457\n",
      "y_pred= tensor([9.9572])\n",
      "794 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9843500852584839 \t b= 0.03557583689689636\n",
      "y_pred= tensor([9.9573])\n",
      "795 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.984387755393982 \t b= 0.03549031913280487\n",
      "y_pred= tensor([9.9574])\n",
      "796 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9844253063201904 \t b= 0.035405002534389496\n",
      "y_pred= tensor([9.9575])\n",
      "797 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9844627380371094 \t b= 0.035319890826940536\n",
      "y_pred= tensor([9.9576])\n",
      "798 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9845000505447388 \t b= 0.03523498401045799\n",
      "y_pred= tensor([9.9577])\n",
      "799 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9845373630523682 \t b= 0.035150282084941864\n",
      "y_pred= tensor([9.9578])\n",
      "800 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.984574556350708 \t b= 0.03506578505039215\n",
      "y_pred= tensor([9.9579])\n",
      "801 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9846116304397583 \t b= 0.034981485456228256\n",
      "y_pred= tensor([9.9580])\n",
      "802 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.984648585319519 \t b= 0.03489739075303078\n",
      "y_pred= tensor([9.9581])\n",
      "803 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9846855401992798 \t b= 0.03481350094079971\n",
      "y_pred= tensor([9.9582])\n",
      "804 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.984722375869751 \t b= 0.03472980856895447\n",
      "y_pred= tensor([9.9583])\n",
      "805 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9847590923309326 \t b= 0.03464631363749504\n",
      "y_pred= tensor([9.9584])\n",
      "806 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9847956895828247 \t b= 0.03456302359700203\n",
      "y_pred= tensor([9.9585])\n",
      "807 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9848322868347168 \t b= 0.034479934722185135\n",
      "y_pred= tensor([9.9586])\n",
      "808 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9848687648773193 \t b= 0.03439704328775406\n",
      "y_pred= tensor([9.9587])\n",
      "809 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9849051237106323 \t b= 0.0343143530189991\n",
      "y_pred= tensor([9.9588])\n",
      "810 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9849413633346558 \t b= 0.03423186391592026\n",
      "y_pred= tensor([9.9589])\n",
      "811 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9849776029586792 \t b= 0.03414957597851753\n",
      "y_pred= tensor([9.9590])\n",
      "812 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.985013723373413 \t b= 0.03406748175621033\n",
      "y_pred= tensor([9.9591])\n",
      "813 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9850497245788574 \t b= 0.03398558497428894\n",
      "y_pred= tensor([9.9592])\n",
      "814 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9850856065750122 \t b= 0.033903881907463074\n",
      "y_pred= tensor([9.9593])\n",
      "815 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.985121488571167 \t b= 0.033822376281023026\n",
      "y_pred= tensor([9.9594])\n",
      "816 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9851572513580322 \t b= 0.033741068094968796\n",
      "y_pred= tensor([9.9595])\n",
      "817 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.985192894935608 \t b= 0.033659957349300385\n",
      "y_pred= tensor([9.9596])\n",
      "818 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9852285385131836 \t b= 0.03357904031872749\n",
      "y_pred= tensor([9.9597])\n",
      "819 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9852640628814697 \t b= 0.03349832072854042\n",
      "y_pred= tensor([9.9598])\n",
      "820 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9852994680404663 \t b= 0.03341779112815857\n",
      "y_pred= tensor([9.9599])\n",
      "821 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9853347539901733 \t b= 0.03333745524287224\n",
      "y_pred= tensor([9.9600])\n",
      "822 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9853700399398804 \t b= 0.033257316797971725\n",
      "y_pred= tensor([9.9601])\n",
      "823 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9854052066802979 \t b= 0.033177368342876434\n",
      "y_pred= tensor([9.9602])\n",
      "824 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9854402542114258 \t b= 0.033097609877586365\n",
      "y_pred= tensor([9.9603])\n",
      "825 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9854753017425537 \t b= 0.033018045127391815\n",
      "y_pred= tensor([9.9604])\n",
      "826 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.985510230064392 \t b= 0.032938674092292786\n",
      "y_pred= tensor([9.9605])\n",
      "827 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.985545039176941 \t b= 0.03285949304699898\n",
      "y_pred= tensor([9.9606])\n",
      "828 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9855798482894897 \t b= 0.03278050199151039\n",
      "y_pred= tensor([9.9607])\n",
      "829 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.985614538192749 \t b= 0.03270169720053673\n",
      "y_pred= tensor([9.9608])\n",
      "830 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9856491088867188 \t b= 0.032623086124658585\n",
      "y_pred= tensor([9.9609])\n",
      "831 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.985683560371399 \t b= 0.032544657588005066\n",
      "y_pred= tensor([9.9610])\n",
      "832 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.985718011856079 \t b= 0.03246641904115677\n",
      "y_pred= tensor([9.9611])\n",
      "833 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9857523441314697 \t b= 0.03238837048411369\n",
      "y_pred= tensor([9.9612])\n",
      "834 : tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "w= 1.9857865571975708 \t b= 0.03231050819158554\n",
      "y_pred= tensor([9.9612])\n",
      "835 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9858207702636719 \t b= 0.03223283216357231\n",
      "y_pred= tensor([9.9613])\n",
      "836 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9858548641204834 \t b= 0.032155346125364304\n",
      "y_pred= tensor([9.9614])\n",
      "837 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9858888387680054 \t b= 0.03207804262638092\n",
      "y_pred= tensor([9.9615])\n",
      "838 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9859228134155273 \t b= 0.03200092539191246\n",
      "y_pred= tensor([9.9616])\n",
      "839 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9859566688537598 \t b= 0.031923990696668625\n",
      "y_pred= tensor([9.9617])\n",
      "840 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9859904050827026 \t b= 0.03184724599123001\n",
      "y_pred= tensor([9.9618])\n",
      "841 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.986024022102356 \t b= 0.03177068382501602\n",
      "y_pred= tensor([9.9619])\n",
      "842 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9860576391220093 \t b= 0.031694311648607254\n",
      "y_pred= tensor([9.9620])\n",
      "843 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.986091136932373 \t b= 0.03161812201142311\n",
      "y_pred= tensor([9.9621])\n",
      "844 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9861245155334473 \t b= 0.03154211491346359\n",
      "y_pred= tensor([9.9622])\n",
      "845 : tensor(0.0001, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w= 1.9861578941345215 \t b= 0.0314662903547287\n",
      "y_pred= tensor([9.9623])\n",
      "846 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9861911535263062 \t b= 0.03139064833521843\n",
      "y_pred= tensor([9.9623])\n",
      "847 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9862244129180908 \t b= 0.031315192580223083\n",
      "y_pred= tensor([9.9624])\n",
      "848 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.986257553100586 \t b= 0.031239910051226616\n",
      "y_pred= tensor([9.9625])\n",
      "849 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9862905740737915 \t b= 0.031164808198809624\n",
      "y_pred= tensor([9.9626])\n",
      "850 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9863234758377075 \t b= 0.031089888885617256\n",
      "y_pred= tensor([9.9627])\n",
      "851 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9863563776016235 \t b= 0.031015150249004364\n",
      "y_pred= tensor([9.9628])\n",
      "852 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.98638916015625 \t b= 0.030940594151616096\n",
      "y_pred= tensor([9.9629])\n",
      "853 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9864219427108765 \t b= 0.030866216868162155\n",
      "y_pred= tensor([9.9630])\n",
      "854 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9864546060562134 \t b= 0.03079201467335224\n",
      "y_pred= tensor([9.9631])\n",
      "855 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9864871501922607 \t b= 0.030717987567186356\n",
      "y_pred= tensor([9.9632])\n",
      "856 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.986519694328308 \t b= 0.030644144862890244\n",
      "y_pred= tensor([9.9632])\n",
      "857 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.986552119255066 \t b= 0.03057047724723816\n",
      "y_pred= tensor([9.9633])\n",
      "858 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9865844249725342 \t b= 0.030496984720230103\n",
      "y_pred= tensor([9.9634])\n",
      "859 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.986616611480713 \t b= 0.030423667281866074\n",
      "y_pred= tensor([9.9635])\n",
      "860 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9866487979888916 \t b= 0.03035053052008152\n",
      "y_pred= tensor([9.9636])\n",
      "861 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9866808652877808 \t b= 0.030277565121650696\n",
      "y_pred= tensor([9.9637])\n",
      "862 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.98671293258667 \t b= 0.030204778537154198\n",
      "y_pred= tensor([9.9638])\n",
      "863 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9867448806762695 \t b= 0.030132165178656578\n",
      "y_pred= tensor([9.9639])\n",
      "864 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9867767095565796 \t b= 0.030059725046157837\n",
      "y_pred= tensor([9.9639])\n",
      "865 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9868085384368896 \t b= 0.029987463727593422\n",
      "y_pred= tensor([9.9640])\n",
      "866 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9868402481079102 \t b= 0.029915375635027885\n",
      "y_pred= tensor([9.9641])\n",
      "867 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9868718385696411 \t b= 0.029843458905816078\n",
      "y_pred= tensor([9.9642])\n",
      "868 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.986903429031372 \t b= 0.02977171540260315\n",
      "y_pred= tensor([9.9643])\n",
      "869 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9869349002838135 \t b= 0.0297001414000988\n",
      "y_pred= tensor([9.9644])\n",
      "870 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9869662523269653 \t b= 0.02962874062359333\n",
      "y_pred= tensor([9.9645])\n",
      "871 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9869976043701172 \t b= 0.029557516798377037\n",
      "y_pred= tensor([9.9645])\n",
      "872 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9870288372039795 \t b= 0.029486460611224174\n",
      "y_pred= tensor([9.9646])\n",
      "873 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9870600700378418 \t b= 0.02941557765007019\n",
      "y_pred= tensor([9.9647])\n",
      "874 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9870911836624146 \t b= 0.029344862326979637\n",
      "y_pred= tensor([9.9648])\n",
      "875 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9871221780776978 \t b= 0.029274316504597664\n",
      "y_pred= tensor([9.9649])\n",
      "876 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.987153172492981 \t b= 0.02920394204556942\n",
      "y_pred= tensor([9.9650])\n",
      "877 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9871840476989746 \t b= 0.029133733361959457\n",
      "y_pred= tensor([9.9651])\n",
      "878 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9872148036956787 \t b= 0.029063696041703224\n",
      "y_pred= tensor([9.9651])\n",
      "879 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9872455596923828 \t b= 0.02899383194744587\n",
      "y_pred= tensor([9.9652])\n",
      "880 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9872761964797974 \t b= 0.028924135491251945\n",
      "y_pred= tensor([9.9653])\n",
      "881 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.987306833267212 \t b= 0.028854604810476303\n",
      "y_pred= tensor([9.9654])\n",
      "882 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.987337350845337 \t b= 0.02878524176776409\n",
      "y_pred= tensor([9.9655])\n",
      "883 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9873677492141724 \t b= 0.028716040775179863\n",
      "y_pred= tensor([9.9656])\n",
      "884 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9873981475830078 \t b= 0.028647011145949364\n",
      "y_pred= tensor([9.9656])\n",
      "885 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9874284267425537 \t b= 0.028578145429491997\n",
      "y_pred= tensor([9.9657])\n",
      "886 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9874587059020996 \t b= 0.02850944548845291\n",
      "y_pred= tensor([9.9658])\n",
      "887 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.987488865852356 \t b= 0.02844090573489666\n",
      "y_pred= tensor([9.9659])\n",
      "888 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9875189065933228 \t b= 0.02837253361940384\n",
      "y_pred= tensor([9.9660])\n",
      "889 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9875489473342896 \t b= 0.028304323554039\n",
      "y_pred= tensor([9.9660])\n",
      "890 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9875788688659668 \t b= 0.028236281126737595\n",
      "y_pred= tensor([9.9661])\n",
      "891 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9876086711883545 \t b= 0.02816839888691902\n",
      "y_pred= tensor([9.9662])\n",
      "892 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9876384735107422 \t b= 0.028100688010454178\n",
      "y_pred= tensor([9.9663])\n",
      "893 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9876681566238403 \t b= 0.028033137321472168\n",
      "y_pred= tensor([9.9664])\n",
      "894 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9876978397369385 \t b= 0.027965746819972992\n",
      "y_pred= tensor([9.9665])\n",
      "895 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.987727403640747 \t b= 0.0278985183686018\n",
      "y_pred= tensor([9.9665])\n",
      "896 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9877568483352661 \t b= 0.02783145196735859\n",
      "y_pred= tensor([9.9666])\n",
      "897 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9877862930297852 \t b= 0.027764547616243362\n",
      "y_pred= tensor([9.9667])\n",
      "898 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9878156185150146 \t b= 0.027697807177901268\n",
      "y_pred= tensor([9.9668])\n",
      "899 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9878449440002441 \t b= 0.027631225064396858\n",
      "y_pred= tensor([9.9669])\n",
      "900 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.987874150276184 \t b= 0.027564801275730133\n",
      "y_pred= tensor([9.9669])\n",
      "901 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.987903356552124 \t b= 0.027498537674546242\n",
      "y_pred= tensor([9.9670])\n",
      "902 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9879324436187744 \t b= 0.027432432398200035\n",
      "y_pred= tensor([9.9671])\n",
      "903 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9879614114761353 \t b= 0.027366483584046364\n",
      "y_pred= tensor([9.9672])\n",
      "904 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.987990379333496 \t b= 0.027300696820020676\n",
      "y_pred= tensor([9.9673])\n",
      "905 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9880192279815674 \t b= 0.027235066518187523\n",
      "y_pred= tensor([9.9673])\n",
      "906 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9880480766296387 \t b= 0.027169594541192055\n",
      "y_pred= tensor([9.9674])\n",
      "907 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9880768060684204 \t b= 0.027104277163743973\n",
      "y_pred= tensor([9.9675])\n",
      "908 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9881054162979126 \t b= 0.027039118111133575\n",
      "y_pred= tensor([9.9676])\n",
      "909 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9881340265274048 \t b= 0.026974117383360863\n",
      "y_pred= tensor([9.9676])\n",
      "910 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9881625175476074 \t b= 0.026909273117780685\n",
      "y_pred= tensor([9.9677])\n",
      "911 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.98819100856781 \t b= 0.026844585314393044\n",
      "y_pred= tensor([9.9678])\n",
      "912 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9882193803787231 \t b= 0.026780053973197937\n",
      "y_pred= tensor([9.9679])\n",
      "913 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9882477521896362 \t b= 0.026715677231550217\n",
      "y_pred= tensor([9.9680])\n",
      "914 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9882760047912598 \t b= 0.026651455089449883\n",
      "y_pred= tensor([9.9680])\n",
      "915 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9883041381835938 \t b= 0.026587387546896935\n",
      "y_pred= tensor([9.9681])\n",
      "916 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9883322715759277 \t b= 0.026523472741246223\n",
      "y_pred= tensor([9.9682])\n",
      "917 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9883602857589722 \t b= 0.02645971067249775\n",
      "y_pred= tensor([9.9683])\n",
      "918 : tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "w= 1.9883882999420166 \t b= 0.02639610320329666\n",
      "y_pred= tensor([9.9683])\n",
      "919 : tensor(9.9951e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9884161949157715 \t b= 0.02633264660835266\n",
      "y_pred= tensor([9.9684])\n",
      "920 : tensor(9.9473e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9884440898895264 \t b= 0.026269342750310898\n",
      "y_pred= tensor([9.9685])\n",
      "921 : tensor(9.8998e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9884718656539917 \t b= 0.02620619162917137\n",
      "y_pred= tensor([9.9686])\n",
      "922 : tensor(9.8522e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.988499641418457 \t b= 0.02614319510757923\n",
      "y_pred= tensor([9.9686])\n",
      "923 : tensor(9.8049e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9885272979736328 \t b= 0.02608034573495388\n",
      "y_pred= tensor([9.9687])\n",
      "924 : tensor(9.7577e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.988554835319519 \t b= 0.026017649099230766\n",
      "y_pred= tensor([9.9688])\n",
      "925 : tensor(9.7107e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9885823726654053 \t b= 0.02595510520040989\n",
      "y_pred= tensor([9.9689])\n",
      "926 : tensor(9.6644e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.988609790802002 \t b= 0.0258927084505558\n",
      "y_pred= tensor([9.9689])\n",
      "927 : tensor(9.6178e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9886372089385986 \t b= 0.0258304625749588\n",
      "y_pred= tensor([9.9690])\n",
      "928 : tensor(9.5715e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9886645078659058 \t b= 0.02576836571097374\n",
      "y_pred= tensor([9.9691])\n",
      "929 : tensor(9.5258e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.988691806793213 \t b= 0.025706417858600616\n",
      "y_pred= tensor([9.9692])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "930 : tensor(9.4800e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9887189865112305 \t b= 0.025644619017839432\n",
      "y_pred= tensor([9.9692])\n",
      "931 : tensor(9.4342e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9887460470199585 \t b= 0.025582965463399887\n",
      "y_pred= tensor([9.9693])\n",
      "932 : tensor(9.3891e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9887731075286865 \t b= 0.02552146650850773\n",
      "y_pred= tensor([9.9694])\n",
      "933 : tensor(9.3442e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.988800048828125 \t b= 0.02546011656522751\n",
      "y_pred= tensor([9.9695])\n",
      "934 : tensor(9.2989e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9888269901275635 \t b= 0.02539891190826893\n",
      "y_pred= tensor([9.9695])\n",
      "935 : tensor(9.2544e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9888538122177124 \t b= 0.025337854400277138\n",
      "y_pred= tensor([9.9696])\n",
      "936 : tensor(9.2101e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9888806343078613 \t b= 0.025276945903897285\n",
      "y_pred= tensor([9.9697])\n",
      "937 : tensor(9.1657e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9889073371887207 \t b= 0.025216178968548775\n",
      "y_pred= tensor([9.9698])\n",
      "938 : tensor(9.1217e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.98893404006958 \t b= 0.02515556290745735\n",
      "y_pred= tensor([9.9698])\n",
      "939 : tensor(9.0779e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.98896062374115 \t b= 0.02509509027004242\n",
      "y_pred= tensor([9.9699])\n",
      "940 : tensor(9.0345e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9889872074127197 \t b= 0.025034764781594276\n",
      "y_pred= tensor([9.9700])\n",
      "941 : tensor(8.9907e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.989013671875 \t b= 0.024974578991532326\n",
      "y_pred= tensor([9.9700])\n",
      "942 : tensor(8.9479e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9890401363372803 \t b= 0.024914544075727463\n",
      "y_pred= tensor([9.9701])\n",
      "943 : tensor(8.9048e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.989066481590271 \t b= 0.024854646995663643\n",
      "y_pred= tensor([9.9702])\n",
      "944 : tensor(8.8619e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9890927076339722 \t b= 0.024794897064566612\n",
      "y_pred= tensor([9.9703])\n",
      "945 : tensor(8.8195e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9891189336776733 \t b= 0.024735290557146072\n",
      "y_pred= tensor([9.9703])\n",
      "946 : tensor(8.7771e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.989145040512085 \t b= 0.024675827473402023\n",
      "y_pred= tensor([9.9704])\n",
      "947 : tensor(8.7351e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9891711473464966 \t b= 0.024616509675979614\n",
      "y_pred= tensor([9.9705])\n",
      "948 : tensor(8.6930e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9891971349716187 \t b= 0.024557331576943398\n",
      "y_pred= tensor([9.9705])\n",
      "949 : tensor(8.6514e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9892231225967407 \t b= 0.02449830248951912\n",
      "y_pred= tensor([9.9706])\n",
      "950 : tensor(8.6097e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9892489910125732 \t b= 0.024439409375190735\n",
      "y_pred= tensor([9.9707])\n",
      "951 : tensor(8.5685e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9892748594284058 \t b= 0.02438066527247429\n",
      "y_pred= tensor([9.9708])\n",
      "952 : tensor(8.5272e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9893006086349487 \t b= 0.024322060868144035\n",
      "y_pred= tensor([9.9708])\n",
      "953 : tensor(8.4866e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9893263578414917 \t b= 0.024263594299554825\n",
      "y_pred= tensor([9.9709])\n",
      "954 : tensor(8.4457e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9893519878387451 \t b= 0.024205271154642105\n",
      "y_pred= tensor([9.9710])\n",
      "955 : tensor(8.4051e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9893776178359985 \t b= 0.024147089570760727\n",
      "y_pred= tensor([9.9710])\n",
      "956 : tensor(8.3647e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9894031286239624 \t b= 0.024089040234684944\n",
      "y_pred= tensor([9.9711])\n",
      "957 : tensor(8.3246e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9894286394119263 \t b= 0.0240311361849308\n",
      "y_pred= tensor([9.9712])\n",
      "958 : tensor(8.2843e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9894540309906006 \t b= 0.023973368108272552\n",
      "y_pred= tensor([9.9712])\n",
      "959 : tensor(8.2449e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.989479422569275 \t b= 0.023915739730000496\n",
      "y_pred= tensor([9.9713])\n",
      "960 : tensor(8.2051e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9895046949386597 \t b= 0.02385825105011463\n",
      "y_pred= tensor([9.9714])\n",
      "961 : tensor(8.1659e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9895299673080444 \t b= 0.02380090020596981\n",
      "y_pred= tensor([9.9715])\n",
      "962 : tensor(8.1267e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9895551204681396 \t b= 0.023743683472275734\n",
      "y_pred= tensor([9.9715])\n",
      "963 : tensor(8.0876e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9895802736282349 \t b= 0.023686608299613\n",
      "y_pred= tensor([9.9716])\n",
      "964 : tensor(8.0487e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9896053075790405 \t b= 0.02362966537475586\n",
      "y_pred= tensor([9.9717])\n",
      "965 : tensor(8.0101e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9896303415298462 \t b= 0.023572862148284912\n",
      "y_pred= tensor([9.9717])\n",
      "966 : tensor(7.9714e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9896552562713623 \t b= 0.02351619116961956\n",
      "y_pred= tensor([9.9718])\n",
      "967 : tensor(7.9334e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9896801710128784 \t b= 0.02345965802669525\n",
      "y_pred= tensor([9.9719])\n",
      "968 : tensor(7.8952e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.989704966545105 \t b= 0.023403260856866837\n",
      "y_pred= tensor([9.9719])\n",
      "969 : tensor(7.8574e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9897297620773315 \t b= 0.023346997797489166\n",
      "y_pred= tensor([9.9720])\n",
      "970 : tensor(7.8197e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9897544384002686 \t b= 0.02329086698591709\n",
      "y_pred= tensor([9.9721])\n",
      "971 : tensor(7.7822e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9897791147232056 \t b= 0.02323487587273121\n",
      "y_pred= tensor([9.9721])\n",
      "972 : tensor(7.7446e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.989803671836853 \t b= 0.023179011419415474\n",
      "y_pred= tensor([9.9722])\n",
      "973 : tensor(7.7073e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9898282289505005 \t b= 0.02312328666448593\n",
      "y_pred= tensor([9.9723])\n",
      "974 : tensor(7.6704e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9898526668548584 \t b= 0.023067688569426537\n",
      "y_pred= tensor([9.9723])\n",
      "975 : tensor(7.6337e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9898771047592163 \t b= 0.023012232035398483\n",
      "y_pred= tensor([9.9724])\n",
      "976 : tensor(7.5970e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9899014234542847 \t b= 0.022956902161240578\n",
      "y_pred= tensor([9.9725])\n",
      "977 : tensor(7.5605e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9899256229400635 \t b= 0.022901706397533417\n",
      "y_pred= tensor([9.9725])\n",
      "978 : tensor(7.5241e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9899498224258423 \t b= 0.0228466484695673\n",
      "y_pred= tensor([9.9726])\n",
      "979 : tensor(7.4878e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.989974021911621 \t b= 0.022791722789406776\n",
      "y_pred= tensor([9.9727])\n",
      "980 : tensor(7.4519e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9899981021881104 \t b= 0.02273692563176155\n",
      "y_pred= tensor([9.9727])\n",
      "981 : tensor(7.4161e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9900221824645996 \t b= 0.02268225885927677\n",
      "y_pred= tensor([9.9728])\n",
      "982 : tensor(7.3805e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9900461435317993 \t b= 0.022627728059887886\n",
      "y_pred= tensor([9.9729])\n",
      "983 : tensor(7.3450e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.990070104598999 \t b= 0.022573327645659447\n",
      "y_pred= tensor([9.9729])\n",
      "984 : tensor(7.3098e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9900939464569092 \t b= 0.022519057616591454\n",
      "y_pred= tensor([9.9730])\n",
      "985 : tensor(7.2749e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9901177883148193 \t b= 0.022464919835329056\n",
      "y_pred= tensor([9.9731])\n",
      "986 : tensor(7.2400e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.99014151096344 \t b= 0.022410912439227104\n",
      "y_pred= tensor([9.9731])\n",
      "987 : tensor(7.2051e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9901652336120605 \t b= 0.0223570317029953\n",
      "y_pred= tensor([9.9732])\n",
      "988 : tensor(7.1704e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9901888370513916 \t b= 0.022303283214569092\n",
      "y_pred= tensor([9.9732])\n",
      "989 : tensor(7.1360e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9902124404907227 \t b= 0.02224966511130333\n",
      "y_pred= tensor([9.9733])\n",
      "990 : tensor(7.1018e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9902359247207642 \t b= 0.022196173667907715\n",
      "y_pred= tensor([9.9734])\n",
      "991 : tensor(7.0675e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9902594089508057 \t b= 0.022142812609672546\n",
      "y_pred= tensor([9.9734])\n",
      "992 : tensor(7.0336e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9902827739715576 \t b= 0.022089576348662376\n",
      "y_pred= tensor([9.9735])\n",
      "993 : tensor(7.0000e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9903061389923096 \t b= 0.02203647792339325\n",
      "y_pred= tensor([9.9736])\n",
      "994 : tensor(6.9662e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.990329384803772 \t b= 0.021983500570058823\n",
      "y_pred= tensor([9.9736])\n",
      "995 : tensor(6.9327e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9903526306152344 \t b= 0.021930653601884842\n",
      "y_pred= tensor([9.9737])\n",
      "996 : tensor(6.8996e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9903758764266968 \t b= 0.021877935156226158\n",
      "y_pred= tensor([9.9738])\n",
      "997 : tensor(6.8665e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9903990030288696 \t b= 0.021825343370437622\n",
      "y_pred= tensor([9.9738])\n",
      "998 : tensor(6.8335e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9904221296310425 \t b= 0.021772874519228935\n",
      "y_pred= tensor([9.9739])\n",
      "999 : tensor(6.8008e-05, grad_fn=<MseLossBackward>)\n",
      "w= 1.9904451370239258 \t b= 0.021720530465245247\n",
      "y_pred= tensor([9.9739])\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "import torch\n",
    "\n",
    "# x_data = [1.0, 2.0, 3.0]\n",
    "# y_data = [2.0, 4.0, 6.0]\n",
    "\n",
    "x_data = torch.Tensor([[1.0], [2.0], [3.0]])\n",
    "y_data = torch.Tensor([[2.0], [4.0], [6.0]])\n",
    "\n",
    "class LinearModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1, 1, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "\n",
    "model = LinearModel()\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    y_pred = model(x_data)\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    print(epoch, \":\", loss)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('w=', model.linear.weight.item(), \"\\t b=\", model.linear.bias.item())\n",
    "    x_test = torch.Tensor([5])\n",
    "    y_test = model(x_test)\n",
    "    print(\"y_pred=\", y_test.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[1.9904]], requires_grad=True)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.,  4.,  6.],\n",
       "        [ 4.,  8., 12.],\n",
       "        [ 6., 12., 18.]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(x_data, y_data.t())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Logistic 回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 0.8695695996284485\n",
      "1 : 0.8678262233734131\n",
      "2 : 0.8661195635795593\n",
      "3 : 0.8644490838050842\n",
      "4 : 0.8628137111663818\n",
      "5 : 0.861212968826294\n",
      "6 : 0.859645426273346\n",
      "7 : 0.8581109046936035\n",
      "8 : 0.8566081523895264\n",
      "9 : 0.8551368713378906\n",
      "10 : 0.8536959290504456\n",
      "11 : 0.8522849082946777\n",
      "12 : 0.850902795791626\n",
      "13 : 0.8495489954948425\n",
      "14 : 0.8482230305671692\n",
      "15 : 0.8469238877296448\n",
      "16 : 0.8456513285636902\n",
      "17 : 0.8444042205810547\n",
      "18 : 0.8431821465492249\n",
      "19 : 0.8419846892356873\n",
      "20 : 0.8408108353614807\n",
      "21 : 0.8396602272987366\n",
      "22 : 0.8385323882102966\n",
      "23 : 0.8374266624450684\n",
      "24 : 0.8363423347473145\n",
      "25 : 0.8352790474891663\n",
      "26 : 0.8342361450195312\n",
      "27 : 0.8332131505012512\n",
      "28 : 0.8322098255157471\n",
      "29 : 0.8312252163887024\n",
      "30 : 0.8302591443061829\n",
      "31 : 0.829310953617096\n",
      "32 : 0.8283803462982178\n",
      "33 : 0.8274667263031006\n",
      "34 : 0.8265697956085205\n",
      "35 : 0.8256890773773193\n",
      "36 : 0.8248241543769836\n",
      "37 : 0.823974609375\n",
      "38 : 0.8231399655342102\n",
      "39 : 0.8223199844360352\n",
      "40 : 0.8215141892433167\n",
      "41 : 0.8207221627235413\n",
      "42 : 0.8199436664581299\n",
      "43 : 0.8191782832145691\n",
      "44 : 0.818425714969635\n",
      "45 : 0.8176855444908142\n",
      "46 : 0.8169575333595276\n",
      "47 : 0.8162413239479065\n",
      "48 : 0.8155365586280823\n",
      "49 : 0.8148429989814758\n",
      "50 : 0.8141604065895081\n",
      "51 : 0.8134883046150208\n",
      "52 : 0.8128268122673035\n",
      "53 : 0.8121750950813293\n",
      "54 : 0.8115332722663879\n",
      "55 : 0.8109009861946106\n",
      "56 : 0.8102779984474182\n",
      "57 : 0.8096640110015869\n",
      "58 : 0.8090589642524719\n",
      "59 : 0.8084623217582703\n",
      "60 : 0.8078740239143372\n",
      "61 : 0.8072938919067383\n",
      "62 : 0.8067216873168945\n",
      "63 : 0.8061571717262268\n",
      "64 : 0.8056002259254456\n",
      "65 : 0.8050505518913269\n",
      "66 : 0.8045079708099365\n",
      "67 : 0.8039722442626953\n",
      "68 : 0.803443431854248\n",
      "69 : 0.8029210567474365\n",
      "70 : 0.802405059337616\n",
      "71 : 0.8018953204154968\n",
      "72 : 0.8013916015625\n",
      "73 : 0.8008937835693359\n",
      "74 : 0.8004016876220703\n",
      "75 : 0.7999151349067688\n",
      "76 : 0.7994341254234314\n",
      "77 : 0.7989583611488342\n",
      "78 : 0.7984878420829773\n",
      "79 : 0.7980220913887024\n",
      "80 : 0.7975614666938782\n",
      "81 : 0.7971055507659912\n",
      "82 : 0.796654224395752\n",
      "83 : 0.7962074875831604\n",
      "84 : 0.7957651615142822\n",
      "85 : 0.7953270077705383\n",
      "86 : 0.7948930859565735\n",
      "87 : 0.7944632172584534\n",
      "88 : 0.7940373420715332\n",
      "89 : 0.7936152815818787\n",
      "90 : 0.793196976184845\n",
      "91 : 0.7927822470664978\n",
      "92 : 0.7923710942268372\n",
      "93 : 0.7919635772705078\n",
      "94 : 0.7915592789649963\n",
      "95 : 0.7911582589149475\n",
      "96 : 0.7907605171203613\n",
      "97 : 0.7903658747673035\n",
      "98 : 0.7899742722511292\n",
      "99 : 0.7895856499671936\n",
      "100 : 0.7891997694969177\n",
      "101 : 0.7888168692588806\n",
      "102 : 0.7884367108345032\n",
      "103 : 0.7880591750144958\n",
      "104 : 0.7876841425895691\n",
      "105 : 0.7873117923736572\n",
      "106 : 0.7869418263435364\n",
      "107 : 0.7865741848945618\n",
      "108 : 0.7862091064453125\n",
      "109 : 0.7858460545539856\n",
      "110 : 0.7854854464530945\n",
      "111 : 0.7851269245147705\n",
      "112 : 0.7847705483436584\n",
      "113 : 0.784416139125824\n",
      "114 : 0.7840638160705566\n",
      "115 : 0.7837135195732117\n",
      "116 : 0.78336501121521\n",
      "117 : 0.7830185294151306\n",
      "118 : 0.7826736569404602\n",
      "119 : 0.7823306918144226\n",
      "120 : 0.7819895148277283\n",
      "121 : 0.7816498279571533\n",
      "122 : 0.7813118100166321\n",
      "123 : 0.7809755206108093\n",
      "124 : 0.7806407809257507\n",
      "125 : 0.7803075313568115\n",
      "126 : 0.7799757122993469\n",
      "127 : 0.7796453833580017\n",
      "128 : 0.7793164849281311\n",
      "129 : 0.7789890170097351\n",
      "130 : 0.7786628603935242\n",
      "131 : 0.7783379554748535\n",
      "132 : 0.7780144810676575\n",
      "133 : 0.7776921391487122\n",
      "134 : 0.7773711085319519\n",
      "135 : 0.7770512104034424\n",
      "136 : 0.7767325043678284\n",
      "137 : 0.7764149308204651\n",
      "138 : 0.7760984897613525\n",
      "139 : 0.775783121585846\n",
      "140 : 0.7754688262939453\n",
      "141 : 0.7751555442810059\n",
      "142 : 0.7748432755470276\n",
      "143 : 0.7745320796966553\n",
      "144 : 0.7742218375205994\n",
      "145 : 0.7739124298095703\n",
      "146 : 0.7736039757728577\n",
      "147 : 0.7732965350151062\n",
      "148 : 0.7729899287223816\n",
      "149 : 0.7726841568946838\n",
      "150 : 0.7723792195320129\n",
      "151 : 0.7720751762390137\n",
      "152 : 0.7717719078063965\n",
      "153 : 0.7714693546295166\n",
      "154 : 0.7711676955223083\n",
      "155 : 0.7708666920661926\n",
      "156 : 0.7705665230751038\n",
      "157 : 0.7702670693397522\n",
      "158 : 0.7699682712554932\n",
      "159 : 0.7696701884269714\n",
      "160 : 0.7693727612495422\n",
      "161 : 0.7690760493278503\n",
      "162 : 0.768779993057251\n",
      "163 : 0.7684845328330994\n",
      "164 : 0.7681897282600403\n",
      "165 : 0.767895519733429\n",
      "166 : 0.7676019668579102\n",
      "167 : 0.7673088908195496\n",
      "168 : 0.767016589641571\n",
      "169 : 0.7667245864868164\n",
      "170 : 0.7664334177970886\n",
      "171 : 0.7661425471305847\n",
      "172 : 0.7658522725105286\n",
      "173 : 0.7655627131462097\n",
      "174 : 0.7652735114097595\n",
      "175 : 0.7649847865104675\n",
      "176 : 0.7646965980529785\n",
      "177 : 0.7644088864326477\n",
      "178 : 0.7641217112541199\n",
      "179 : 0.7638348937034607\n",
      "180 : 0.7635485529899597\n",
      "181 : 0.7632628083229065\n",
      "182 : 0.7629773616790771\n",
      "183 : 0.7626924514770508\n",
      "184 : 0.7624079585075378\n",
      "185 : 0.7621237635612488\n",
      "186 : 0.7618401050567627\n",
      "187 : 0.7615568041801453\n",
      "188 : 0.7612738609313965\n",
      "189 : 0.7609913945198059\n",
      "190 : 0.760709285736084\n",
      "191 : 0.7604276537895203\n",
      "192 : 0.7601462006568909\n",
      "193 : 0.7598652839660645\n",
      "194 : 0.7595846056938171\n",
      "195 : 0.7593043446540833\n",
      "196 : 0.759024441242218\n",
      "197 : 0.7587449550628662\n",
      "198 : 0.7584657669067383\n",
      "199 : 0.7581868767738342\n",
      "200 : 0.7579083442687988\n",
      "201 : 0.7576300501823425\n",
      "202 : 0.7573521733283997\n",
      "203 : 0.7570745944976807\n",
      "204 : 0.7567973732948303\n",
      "205 : 0.7565204501152039\n",
      "206 : 0.7562437057495117\n",
      "207 : 0.7559674382209778\n",
      "208 : 0.7556913495063782\n",
      "209 : 0.755415678024292\n",
      "210 : 0.7551401257514954\n",
      "211 : 0.7548651099205017\n",
      "212 : 0.7545900940895081\n",
      "213 : 0.7543155550956726\n",
      "214 : 0.7540411949157715\n",
      "215 : 0.753767192363739\n",
      "216 : 0.7534933686256409\n",
      "217 : 0.7532199025154114\n",
      "218 : 0.752946674823761\n",
      "219 : 0.7526736259460449\n",
      "220 : 0.7524009346961975\n",
      "221 : 0.7521284222602844\n",
      "222 : 0.7518561482429504\n",
      "223 : 0.7515842914581299\n",
      "224 : 0.7513125538825989\n",
      "225 : 0.7510409951210022\n",
      "226 : 0.750769853591919\n",
      "227 : 0.7504987716674805\n",
      "228 : 0.7502281665802002\n",
      "229 : 0.7499576210975647\n",
      "230 : 0.7496873736381531\n",
      "231 : 0.7494173049926758\n",
      "232 : 0.7491474151611328\n",
      "233 : 0.7488778233528137\n",
      "234 : 0.748608410358429\n",
      "235 : 0.7483394145965576\n",
      "236 : 0.748070478439331\n",
      "237 : 0.7478017807006836\n",
      "238 : 0.7475333213806152\n",
      "239 : 0.7472650408744812\n",
      "240 : 0.7469968795776367\n",
      "241 : 0.7467290759086609\n",
      "242 : 0.7464615702629089\n",
      "243 : 0.7461941242218018\n",
      "244 : 0.7459268569946289\n",
      "245 : 0.7456600069999695\n",
      "246 : 0.7453930974006653\n",
      "247 : 0.7451265454292297\n",
      "248 : 0.7448601722717285\n",
      "249 : 0.7445939183235168\n",
      "250 : 0.7443280220031738\n",
      "251 : 0.7440622448921204\n",
      "252 : 0.7437966465950012\n",
      "253 : 0.7435312867164612\n",
      "254 : 0.7432661056518555\n",
      "255 : 0.7430011630058289\n",
      "256 : 0.7427363395690918\n",
      "257 : 0.7424717545509338\n",
      "258 : 0.7422072887420654\n",
      "259 : 0.7419430613517761\n",
      "260 : 0.7416791319847107\n",
      "261 : 0.7414152026176453\n",
      "262 : 0.7411515712738037\n",
      "263 : 0.7408881187438965\n",
      "264 : 0.7406248450279236\n",
      "265 : 0.740361750125885\n",
      "266 : 0.740098774433136\n",
      "267 : 0.7398360371589661\n",
      "268 : 0.7395735383033752\n",
      "269 : 0.7393112182617188\n",
      "270 : 0.739048957824707\n",
      "271 : 0.7387869954109192\n",
      "272 : 0.7385252118110657\n",
      "273 : 0.7382636070251465\n",
      "274 : 0.7380020022392273\n",
      "275 : 0.7377408146858215\n",
      "276 : 0.7374796271324158\n",
      "277 : 0.7372187972068787\n",
      "278 : 0.736957848072052\n",
      "279 : 0.7366973757743835\n",
      "280 : 0.7364370226860046\n",
      "281 : 0.7361766695976257\n",
      "282 : 0.7359166741371155\n",
      "283 : 0.7356569170951843\n",
      "284 : 0.7353970408439636\n",
      "285 : 0.7351375222206116\n",
      "286 : 0.7348780632019043\n",
      "287 : 0.7346189618110657\n",
      "288 : 0.7343598008155823\n",
      "289 : 0.7341009974479675\n",
      "290 : 0.7338423132896423\n",
      "291 : 0.7335837483406067\n",
      "292 : 0.7333253026008606\n",
      "293 : 0.7330672144889832\n",
      "294 : 0.7328091263771057\n",
      "295 : 0.7325512766838074\n",
      "296 : 0.7322936058044434\n",
      "297 : 0.7320360541343689\n",
      "298 : 0.731778621673584\n",
      "299 : 0.7315215468406677\n",
      "300 : 0.731264591217041\n",
      "301 : 0.7310075759887695\n",
      "302 : 0.7307508587837219\n",
      "303 : 0.7304943203926086\n",
      "304 : 0.7302379608154297\n",
      "305 : 0.7299817204475403\n",
      "306 : 0.7297256588935852\n",
      "307 : 0.7294697761535645\n",
      "308 : 0.7292140126228333\n",
      "309 : 0.7289584279060364\n",
      "310 : 0.7287030816078186\n",
      "311 : 0.7284478545188904\n",
      "312 : 0.7281927466392517\n",
      "313 : 0.7279377579689026\n",
      "314 : 0.7276830077171326\n",
      "315 : 0.7274284362792969\n",
      "316 : 0.7271738648414612\n",
      "317 : 0.7269196510314941\n",
      "318 : 0.7266655564308167\n",
      "319 : 0.7264115810394287\n",
      "320 : 0.7261577248573303\n",
      "321 : 0.7259040474891663\n",
      "322 : 0.7256505489349365\n",
      "323 : 0.7253972887992859\n",
      "324 : 0.7251441478729248\n",
      "325 : 0.7248910069465637\n",
      "326 : 0.7246381640434265\n",
      "327 : 0.7243854403495789\n",
      "328 : 0.7241328358650208\n",
      "329 : 0.7238804697990417\n",
      "330 : 0.7236282229423523\n",
      "331 : 0.7233760952949524\n",
      "332 : 0.7231242060661316\n",
      "333 : 0.7228723168373108\n",
      "334 : 0.7226207256317139\n",
      "335 : 0.722369372844696\n",
      "336 : 0.7221179604530334\n",
      "337 : 0.72186678647995\n",
      "338 : 0.7216157913208008\n",
      "339 : 0.7213649153709412\n",
      "340 : 0.7211141586303711\n",
      "341 : 0.7208635807037354\n",
      "342 : 0.7206133008003235\n",
      "343 : 0.7203630805015564\n",
      "344 : 0.7201129794120789\n",
      "345 : 0.7198629975318909\n",
      "346 : 0.7196131348609924\n",
      "347 : 0.7193636298179626\n",
      "348 : 0.7191140651702881\n",
      "349 : 0.7188647389411926\n",
      "350 : 0.718615710735321\n",
      "351 : 0.7183664441108704\n",
      "352 : 0.7181177139282227\n",
      "353 : 0.717868983745575\n",
      "354 : 0.7176204323768616\n",
      "355 : 0.7173720002174377\n",
      "356 : 0.7171237468719482\n",
      "357 : 0.7168756127357483\n",
      "358 : 0.7166276574134827\n",
      "359 : 0.7163798213005066\n",
      "360 : 0.7161321640014648\n",
      "361 : 0.7158846855163574\n",
      "362 : 0.7156372666358948\n",
      "363 : 0.7153900265693665\n",
      "364 : 0.7151430249214172\n",
      "365 : 0.7148962020874023\n",
      "366 : 0.7146493792533875\n",
      "367 : 0.7144026756286621\n",
      "368 : 0.7141562104225159\n",
      "369 : 0.713909924030304\n",
      "370 : 0.7136637568473816\n",
      "371 : 0.7134177684783936\n",
      "372 : 0.7131718993186951\n",
      "373 : 0.7129261493682861\n",
      "374 : 0.7126806378364563\n",
      "375 : 0.712435245513916\n",
      "376 : 0.7121899127960205\n",
      "377 : 0.7119448184967041\n",
      "378 : 0.7116997838020325\n",
      "379 : 0.7114550471305847\n",
      "380 : 0.7112102508544922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "381 : 0.7109658122062683\n",
      "382 : 0.7107214331626892\n",
      "383 : 0.7104771137237549\n",
      "384 : 0.7102330327033997\n",
      "385 : 0.709989070892334\n",
      "386 : 0.7097452282905579\n",
      "387 : 0.7095016837120056\n",
      "388 : 0.7092581391334534\n",
      "389 : 0.7090148329734802\n",
      "390 : 0.7087716460227966\n",
      "391 : 0.7085285186767578\n",
      "392 : 0.7082856297492981\n",
      "393 : 0.7080428600311279\n",
      "394 : 0.7078002095222473\n",
      "395 : 0.7075576782226562\n",
      "396 : 0.7073153853416443\n",
      "397 : 0.7070732116699219\n",
      "398 : 0.7068312168121338\n",
      "399 : 0.70658940076828\n",
      "400 : 0.7063475251197815\n",
      "401 : 0.7061059474945068\n",
      "402 : 0.7058644890785217\n",
      "403 : 0.705623209476471\n",
      "404 : 0.7053820490837097\n",
      "405 : 0.705141007900238\n",
      "406 : 0.7049000859260559\n",
      "407 : 0.7046594619750977\n",
      "408 : 0.7044188380241394\n",
      "409 : 0.7041783332824707\n",
      "410 : 0.7039381861686707\n",
      "411 : 0.703697919845581\n",
      "412 : 0.7034580111503601\n",
      "413 : 0.7032181620597839\n",
      "414 : 0.7029783129692078\n",
      "415 : 0.7027387619018555\n",
      "416 : 0.7024993896484375\n",
      "417 : 0.7022600173950195\n",
      "418 : 0.7020208835601807\n",
      "419 : 0.7017819285392761\n",
      "420 : 0.7015430927276611\n",
      "421 : 0.7013043761253357\n",
      "422 : 0.7010657787322998\n",
      "423 : 0.700827419757843\n",
      "424 : 0.7005890011787415\n",
      "425 : 0.7003509998321533\n",
      "426 : 0.7001129984855652\n",
      "427 : 0.6998751163482666\n",
      "428 : 0.6996374130249023\n",
      "429 : 0.6993997693061829\n",
      "430 : 0.6991624236106873\n",
      "431 : 0.6989250779151917\n",
      "432 : 0.6986880302429199\n",
      "433 : 0.6984509825706482\n",
      "434 : 0.6982141137123108\n",
      "435 : 0.6979774832725525\n",
      "436 : 0.6977408528327942\n",
      "437 : 0.697504460811615\n",
      "438 : 0.6972681879997253\n",
      "439 : 0.6970320343971252\n",
      "440 : 0.6967960000038147\n",
      "441 : 0.6965602040290833\n",
      "442 : 0.6963245272636414\n",
      "443 : 0.6960888504981995\n",
      "444 : 0.6958534717559814\n",
      "445 : 0.6956181526184082\n",
      "446 : 0.6953830122947693\n",
      "447 : 0.6951479911804199\n",
      "448 : 0.6949130892753601\n",
      "449 : 0.6946783661842346\n",
      "450 : 0.6944437623023987\n",
      "451 : 0.6942093372344971\n",
      "452 : 0.693975031375885\n",
      "453 : 0.6937408447265625\n",
      "454 : 0.6935068964958191\n",
      "455 : 0.6932730674743652\n",
      "456 : 0.6930392384529114\n",
      "457 : 0.6928057074546814\n",
      "458 : 0.6925721764564514\n",
      "459 : 0.6923388838768005\n",
      "460 : 0.6921055912971497\n",
      "461 : 0.6918725967407227\n",
      "462 : 0.6916397213935852\n",
      "463 : 0.6914069652557373\n",
      "464 : 0.691174328327179\n",
      "465 : 0.6909418106079102\n",
      "466 : 0.6907095909118652\n",
      "467 : 0.6904773712158203\n",
      "468 : 0.6902453303337097\n",
      "469 : 0.6900132298469543\n",
      "470 : 0.6897814869880676\n",
      "471 : 0.6895499229431152\n",
      "472 : 0.6893183588981628\n",
      "473 : 0.6890870928764343\n",
      "474 : 0.6888557076454163\n",
      "475 : 0.6886246800422668\n",
      "476 : 0.6883936524391174\n",
      "477 : 0.6881629824638367\n",
      "478 : 0.6879321932792664\n",
      "479 : 0.6877017021179199\n",
      "480 : 0.687471330165863\n",
      "481 : 0.6872410774230957\n",
      "482 : 0.6870109438896179\n",
      "483 : 0.6867809295654297\n",
      "484 : 0.686551034450531\n",
      "485 : 0.6863213181495667\n",
      "486 : 0.6860918402671814\n",
      "487 : 0.6858623623847961\n",
      "488 : 0.68563312292099\n",
      "489 : 0.6854040026664734\n",
      "490 : 0.6851748824119568\n",
      "491 : 0.6849460005760193\n",
      "492 : 0.6847172379493713\n",
      "493 : 0.6844885945320129\n",
      "494 : 0.6842601895332336\n",
      "495 : 0.6840317845344543\n",
      "496 : 0.6838036179542542\n",
      "497 : 0.6835755705833435\n",
      "498 : 0.6833476424217224\n",
      "499 : 0.6831198334693909\n",
      "500 : 0.6828922629356384\n",
      "501 : 0.682664692401886\n",
      "502 : 0.6824372410774231\n",
      "503 : 0.6822100281715393\n",
      "504 : 0.6819829940795898\n",
      "505 : 0.6817560195922852\n",
      "506 : 0.6815291047096252\n",
      "507 : 0.6813023686408997\n",
      "508 : 0.6810758113861084\n",
      "509 : 0.6808493733406067\n",
      "510 : 0.6806232333183289\n",
      "511 : 0.6803970336914062\n",
      "512 : 0.680171012878418\n",
      "513 : 0.6799450516700745\n",
      "514 : 0.6797192692756653\n",
      "515 : 0.6794936656951904\n",
      "516 : 0.6792683005332947\n",
      "517 : 0.6790428757667542\n",
      "518 : 0.6788177490234375\n",
      "519 : 0.6785926222801208\n",
      "520 : 0.6783676743507385\n",
      "521 : 0.6781428456306458\n",
      "522 : 0.6779181957244873\n",
      "523 : 0.6776935458183289\n",
      "524 : 0.6774692535400391\n",
      "525 : 0.6772450804710388\n",
      "526 : 0.677020788192749\n",
      "527 : 0.6767968535423279\n",
      "528 : 0.6765730381011963\n",
      "529 : 0.6763492226600647\n",
      "530 : 0.6761255860328674\n",
      "531 : 0.6759021282196045\n",
      "532 : 0.6756787896156311\n",
      "533 : 0.6754557490348816\n",
      "534 : 0.6752326488494873\n",
      "535 : 0.6750096678733826\n",
      "536 : 0.6747868061065674\n",
      "537 : 0.6745641827583313\n",
      "538 : 0.6743416786193848\n",
      "539 : 0.6741192936897278\n",
      "540 : 0.6738969683647156\n",
      "541 : 0.6736748814582825\n",
      "542 : 0.6734528541564941\n",
      "543 : 0.6732311248779297\n",
      "544 : 0.6730092167854309\n",
      "545 : 0.6727876663208008\n",
      "546 : 0.6725662350654602\n",
      "547 : 0.672344982624054\n",
      "548 : 0.6721237301826477\n",
      "549 : 0.6719026565551758\n",
      "550 : 0.6716817021369934\n",
      "551 : 0.6714609265327454\n",
      "552 : 0.6712402701377869\n",
      "553 : 0.6710197329521179\n",
      "554 : 0.6707992553710938\n",
      "555 : 0.6705790162086487\n",
      "556 : 0.6703589558601379\n",
      "557 : 0.6701388955116272\n",
      "558 : 0.6699190139770508\n",
      "559 : 0.6696992516517639\n",
      "560 : 0.6694796085357666\n",
      "561 : 0.6692602038383484\n",
      "562 : 0.669040858745575\n",
      "563 : 0.6688216328620911\n",
      "564 : 0.668602466583252\n",
      "565 : 0.6683835387229919\n",
      "566 : 0.6681647300720215\n",
      "567 : 0.6679460406303406\n",
      "568 : 0.6677274107933044\n",
      "569 : 0.6675090789794922\n",
      "570 : 0.6672907471656799\n",
      "571 : 0.6670724749565125\n",
      "572 : 0.6668545603752136\n",
      "573 : 0.6666365265846252\n",
      "574 : 0.6664188504219055\n",
      "575 : 0.6662011742591858\n",
      "576 : 0.6659836173057556\n",
      "577 : 0.6657662391662598\n",
      "578 : 0.6655489802360535\n",
      "579 : 0.6653318405151367\n",
      "580 : 0.6651148200035095\n",
      "581 : 0.6648979187011719\n",
      "582 : 0.6646812558174133\n",
      "583 : 0.66446453332901\n",
      "584 : 0.6642481088638306\n",
      "585 : 0.6640317440032959\n",
      "586 : 0.6638154983520508\n",
      "587 : 0.6635993719100952\n",
      "588 : 0.663383424282074\n",
      "589 : 0.6631675958633423\n",
      "590 : 0.6629518866539001\n",
      "591 : 0.6627363562583923\n",
      "592 : 0.6625208854675293\n",
      "593 : 0.6623055338859558\n",
      "594 : 0.6620903611183167\n",
      "595 : 0.661875307559967\n",
      "596 : 0.661660373210907\n",
      "597 : 0.6614454388618469\n",
      "598 : 0.6612308621406555\n",
      "599 : 0.6610162854194641\n",
      "600 : 0.660801887512207\n",
      "601 : 0.6605875492095947\n",
      "602 : 0.6603733897209167\n",
      "603 : 0.6601594090461731\n",
      "604 : 0.6599454283714294\n",
      "605 : 0.6597316861152649\n",
      "606 : 0.6595180034637451\n",
      "607 : 0.6593044400215149\n",
      "608 : 0.6590911149978638\n",
      "609 : 0.6588777899742126\n",
      "610 : 0.6586645841598511\n",
      "611 : 0.6584515571594238\n",
      "612 : 0.6582387089729309\n",
      "613 : 0.6580260396003723\n",
      "614 : 0.6578133702278137\n",
      "615 : 0.6576009392738342\n",
      "616 : 0.65738844871521\n",
      "617 : 0.6571761965751648\n",
      "618 : 0.6569640040397644\n",
      "619 : 0.6567521095275879\n",
      "620 : 0.6565401554107666\n",
      "621 : 0.6563284397125244\n",
      "622 : 0.656116783618927\n",
      "623 : 0.6559053063392639\n",
      "624 : 0.6556938290596008\n",
      "625 : 0.6554827094078064\n",
      "626 : 0.655271589756012\n",
      "627 : 0.6550605297088623\n",
      "628 : 0.6548497080802917\n",
      "629 : 0.654638946056366\n",
      "630 : 0.6544283032417297\n",
      "631 : 0.6542178392410278\n",
      "632 : 0.6540074944496155\n",
      "633 : 0.6537972092628479\n",
      "634 : 0.6535872220993042\n",
      "635 : 0.653377115726471\n",
      "636 : 0.6531673073768616\n",
      "637 : 0.6529574394226074\n",
      "638 : 0.6527478694915771\n",
      "639 : 0.6525383591651917\n",
      "640 : 0.6523290276527405\n",
      "641 : 0.6521198153495789\n",
      "642 : 0.6519107222557068\n",
      "643 : 0.6517016887664795\n",
      "644 : 0.6514928340911865\n",
      "645 : 0.6512840390205383\n",
      "646 : 0.6510754823684692\n",
      "647 : 0.6508669853210449\n",
      "648 : 0.6506586074829102\n",
      "649 : 0.6504502892494202\n",
      "650 : 0.650242269039154\n",
      "651 : 0.6500343084335327\n",
      "652 : 0.6498263478279114\n",
      "653 : 0.6496186256408691\n",
      "654 : 0.6494110226631165\n",
      "655 : 0.6492034792900085\n",
      "656 : 0.6489961743354797\n",
      "657 : 0.6487889289855957\n",
      "658 : 0.6485817432403564\n",
      "659 : 0.6483747363090515\n",
      "660 : 0.6481678485870361\n",
      "661 : 0.6479611396789551\n",
      "662 : 0.6477544903755188\n",
      "663 : 0.6475479602813721\n",
      "664 : 0.6473415493965149\n",
      "665 : 0.647135317325592\n",
      "666 : 0.6469292044639587\n",
      "667 : 0.6467230916023254\n",
      "668 : 0.646517276763916\n",
      "669 : 0.6463115215301514\n",
      "670 : 0.6461057066917419\n",
      "671 : 0.645900309085846\n",
      "672 : 0.6456948518753052\n",
      "673 : 0.6454896330833435\n",
      "674 : 0.6452844142913818\n",
      "675 : 0.6450793147087097\n",
      "676 : 0.6448744535446167\n",
      "677 : 0.6446695923805237\n",
      "678 : 0.6444649696350098\n",
      "679 : 0.6442604064941406\n",
      "680 : 0.6440560221672058\n",
      "681 : 0.6438516974449158\n",
      "682 : 0.6436475515365601\n",
      "683 : 0.6434433460235596\n",
      "684 : 0.6432394981384277\n",
      "685 : 0.6430356502532959\n",
      "686 : 0.6428319215774536\n",
      "687 : 0.6426283717155457\n",
      "688 : 0.6424248814582825\n",
      "689 : 0.6422215104103088\n",
      "690 : 0.6420182585716248\n",
      "691 : 0.6418151259422302\n",
      "692 : 0.6416122913360596\n",
      "693 : 0.6414094567298889\n",
      "694 : 0.6412065625190735\n",
      "695 : 0.6410040259361267\n",
      "696 : 0.6408014893531799\n",
      "697 : 0.6405990719795227\n",
      "698 : 0.6403968930244446\n",
      "699 : 0.6401947736740112\n",
      "700 : 0.6399927139282227\n",
      "701 : 0.6397908926010132\n",
      "702 : 0.6395890712738037\n",
      "703 : 0.6393874287605286\n",
      "704 : 0.6391859650611877\n",
      "705 : 0.6389845013618469\n",
      "706 : 0.6387830376625061\n",
      "707 : 0.6385819315910339\n",
      "708 : 0.6383809447288513\n",
      "709 : 0.6381799578666687\n",
      "710 : 0.6379792094230652\n",
      "711 : 0.6377784609794617\n",
      "712 : 0.6375778317451477\n",
      "713 : 0.6373774409294128\n",
      "714 : 0.6371771097183228\n",
      "715 : 0.6369768977165222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "716 : 0.6367767453193665\n",
      "717 : 0.6365768313407898\n",
      "718 : 0.6363769173622131\n",
      "719 : 0.636177122592926\n",
      "720 : 0.635977566242218\n",
      "721 : 0.63577800989151\n",
      "722 : 0.6355786919593811\n",
      "723 : 0.6353793144226074\n",
      "724 : 0.6351802349090576\n",
      "725 : 0.6349812150001526\n",
      "726 : 0.6347822546958923\n",
      "727 : 0.6345834732055664\n",
      "728 : 0.6343846917152405\n",
      "729 : 0.6341862082481384\n",
      "730 : 0.6339876651763916\n",
      "731 : 0.6337893605232239\n",
      "732 : 0.6335912346839905\n",
      "733 : 0.6333931088447571\n",
      "734 : 0.6331951022148132\n",
      "735 : 0.6329972743988037\n",
      "736 : 0.6327994465827942\n",
      "737 : 0.6326019167900085\n",
      "738 : 0.6324043869972229\n",
      "739 : 0.6322069764137268\n",
      "740 : 0.6320096850395203\n",
      "741 : 0.6318126320838928\n",
      "742 : 0.6316154599189758\n",
      "743 : 0.6314185261726379\n",
      "744 : 0.6312217712402344\n",
      "745 : 0.6310251355171204\n",
      "746 : 0.6308285593986511\n",
      "747 : 0.6306320428848267\n",
      "748 : 0.6304357647895813\n",
      "749 : 0.6302394270896912\n",
      "750 : 0.6300433874130249\n",
      "751 : 0.6298473477363586\n",
      "752 : 0.6296514868736267\n",
      "753 : 0.6294557452201843\n",
      "754 : 0.6292601227760315\n",
      "755 : 0.6290646195411682\n",
      "756 : 0.6288692355155945\n",
      "757 : 0.6286738514900208\n",
      "758 : 0.6284787654876709\n",
      "759 : 0.6282835602760315\n",
      "760 : 0.628088653087616\n",
      "761 : 0.62789386510849\n",
      "762 : 0.6276991367340088\n",
      "763 : 0.6275045275688171\n",
      "764 : 0.6273100972175598\n",
      "765 : 0.6271156668663025\n",
      "766 : 0.6269214153289795\n",
      "767 : 0.626727283000946\n",
      "768 : 0.6265332102775574\n",
      "769 : 0.6263392567634583\n",
      "770 : 0.6261454820632935\n",
      "771 : 0.6259518265724182\n",
      "772 : 0.6257582306861877\n",
      "773 : 0.6255647540092468\n",
      "774 : 0.6253714561462402\n",
      "775 : 0.6251782774925232\n",
      "776 : 0.6249851584434509\n",
      "777 : 0.6247920989990234\n",
      "778 : 0.6245992183685303\n",
      "779 : 0.6244063973426819\n",
      "780 : 0.6242138743400574\n",
      "781 : 0.6240212917327881\n",
      "782 : 0.6238288283348083\n",
      "783 : 0.6236364841461182\n",
      "784 : 0.6234443187713623\n",
      "785 : 0.6232522130012512\n",
      "786 : 0.6230601668357849\n",
      "787 : 0.6228683590888977\n",
      "788 : 0.6226766109466553\n",
      "789 : 0.6224849820137024\n",
      "790 : 0.6222934722900391\n",
      "791 : 0.6221020817756653\n",
      "792 : 0.6219107508659363\n",
      "793 : 0.6217195987701416\n",
      "794 : 0.6215284466743469\n",
      "795 : 0.6213375329971313\n",
      "796 : 0.6211466789245605\n",
      "797 : 0.6209560036659241\n",
      "798 : 0.6207653284072876\n",
      "799 : 0.6205747723579407\n",
      "800 : 0.6203843951225281\n",
      "801 : 0.620194137096405\n",
      "802 : 0.6200039982795715\n",
      "803 : 0.6198139190673828\n",
      "804 : 0.6196239590644836\n",
      "805 : 0.619434118270874\n",
      "806 : 0.6192445158958435\n",
      "807 : 0.6190548539161682\n",
      "808 : 0.6188653707504272\n",
      "809 : 0.618675947189331\n",
      "810 : 0.6184867024421692\n",
      "811 : 0.6182975172996521\n",
      "812 : 0.6181084513664246\n",
      "813 : 0.6179195046424866\n",
      "814 : 0.6177306771278381\n",
      "815 : 0.6175419688224792\n",
      "816 : 0.6173533797264099\n",
      "817 : 0.6171648502349854\n",
      "818 : 0.6169764399528503\n",
      "819 : 0.6167881488800049\n",
      "820 : 0.6166000366210938\n",
      "821 : 0.6164119839668274\n",
      "822 : 0.6162240505218506\n",
      "823 : 0.6160361766815186\n",
      "824 : 0.6158485412597656\n",
      "825 : 0.6156608462333679\n",
      "826 : 0.6154734492301941\n",
      "827 : 0.6152860522270203\n",
      "828 : 0.6150987148284912\n",
      "829 : 0.6149115562438965\n",
      "830 : 0.6147245168685913\n",
      "831 : 0.6145375370979309\n",
      "832 : 0.6143507361412048\n",
      "833 : 0.6141639947891235\n",
      "834 : 0.6139773726463318\n",
      "835 : 0.6137909293174744\n",
      "836 : 0.6136044859886169\n",
      "837 : 0.6134181618690491\n",
      "838 : 0.6132319569587708\n",
      "839 : 0.6130459308624268\n",
      "840 : 0.6128599643707275\n",
      "841 : 0.6126741170883179\n",
      "842 : 0.6124884486198425\n",
      "843 : 0.6123027801513672\n",
      "844 : 0.6121172308921814\n",
      "845 : 0.6119318008422852\n",
      "846 : 0.6117464900016785\n",
      "847 : 0.6115612983703613\n",
      "848 : 0.6113762855529785\n",
      "849 : 0.6111912727355957\n",
      "850 : 0.6110064387321472\n",
      "851 : 0.6108216643333435\n",
      "852 : 0.6106370091438293\n",
      "853 : 0.61045241355896\n",
      "854 : 0.6102679371833801\n",
      "855 : 0.6100836992263794\n",
      "856 : 0.6098994612693787\n",
      "857 : 0.6097152829170227\n",
      "858 : 0.6095312237739563\n",
      "859 : 0.609347403049469\n",
      "860 : 0.6091635823249817\n",
      "861 : 0.6089799404144287\n",
      "862 : 0.6087964177131653\n",
      "863 : 0.6086129546165466\n",
      "864 : 0.6084296107292175\n",
      "865 : 0.6082463264465332\n",
      "866 : 0.6080632209777832\n",
      "867 : 0.6078801155090332\n",
      "868 : 0.6076972484588623\n",
      "869 : 0.6075143814086914\n",
      "870 : 0.6073316931724548\n",
      "871 : 0.6071491241455078\n",
      "872 : 0.6069666147232056\n",
      "873 : 0.6067842841148376\n",
      "874 : 0.6066019535064697\n",
      "875 : 0.6064197421073914\n",
      "876 : 0.6062376499176025\n",
      "877 : 0.6060556769371033\n",
      "878 : 0.6058738827705383\n",
      "879 : 0.6056920886039734\n",
      "880 : 0.6055104732513428\n",
      "881 : 0.6053289771080017\n",
      "882 : 0.6051474213600159\n",
      "883 : 0.6049661636352539\n",
      "884 : 0.6047849655151367\n",
      "885 : 0.6046038269996643\n",
      "886 : 0.6044228672981262\n",
      "887 : 0.6042420268058777\n",
      "888 : 0.6040611267089844\n",
      "889 : 0.6038804650306702\n",
      "890 : 0.6036999225616455\n",
      "891 : 0.6035194396972656\n",
      "892 : 0.6033391356468201\n",
      "893 : 0.6031587719917297\n",
      "894 : 0.6029785871505737\n",
      "895 : 0.6027985215187073\n",
      "896 : 0.6026186347007751\n",
      "897 : 0.6024388670921326\n",
      "898 : 0.60225909948349\n",
      "899 : 0.6020793914794922\n",
      "900 : 0.6018999218940735\n",
      "901 : 0.6017205119132996\n",
      "902 : 0.6015412211418152\n",
      "903 : 0.6013619899749756\n",
      "904 : 0.601182758808136\n",
      "905 : 0.6010038256645203\n",
      "906 : 0.6008249521255493\n",
      "907 : 0.6006461381912231\n",
      "908 : 0.6004674434661865\n",
      "909 : 0.6002888083457947\n",
      "910 : 0.6001104712486267\n",
      "911 : 0.5999320149421692\n",
      "912 : 0.5997536778450012\n",
      "913 : 0.5995755791664124\n",
      "914 : 0.5993974804878235\n",
      "915 : 0.599219560623169\n",
      "916 : 0.5990416407585144\n",
      "917 : 0.5988638997077942\n",
      "918 : 0.5986862778663635\n",
      "919 : 0.5985087752342224\n",
      "920 : 0.5983312129974365\n",
      "921 : 0.5981540083885193\n",
      "922 : 0.5979767441749573\n",
      "923 : 0.5977995991706848\n",
      "924 : 0.5976225733757019\n",
      "925 : 0.5974456667900085\n",
      "926 : 0.5972687602043152\n",
      "927 : 0.5970920920372009\n",
      "928 : 0.5969154834747314\n",
      "929 : 0.5967390537261963\n",
      "930 : 0.5965625643730164\n",
      "931 : 0.5963862538337708\n",
      "932 : 0.5962100625038147\n",
      "933 : 0.5960338711738586\n",
      "934 : 0.5958580374717712\n",
      "935 : 0.5956820845603943\n",
      "936 : 0.5955062508583069\n",
      "937 : 0.5953305959701538\n",
      "938 : 0.5951550006866455\n",
      "939 : 0.5949795246124268\n",
      "940 : 0.5948041081428528\n",
      "941 : 0.5946288108825684\n",
      "942 : 0.5944536328315735\n",
      "943 : 0.5942785739898682\n",
      "944 : 0.5941035747528076\n",
      "945 : 0.5939286947250366\n",
      "946 : 0.5937539935112\n",
      "947 : 0.5935792922973633\n",
      "948 : 0.5934047698974609\n",
      "949 : 0.5932301878929138\n",
      "950 : 0.5930559039115906\n",
      "951 : 0.5928816199302673\n",
      "952 : 0.5927073955535889\n",
      "953 : 0.5925334095954895\n",
      "954 : 0.5923593640327454\n",
      "955 : 0.5921855568885803\n",
      "956 : 0.5920118093490601\n",
      "957 : 0.5918381214141846\n",
      "958 : 0.5916646122932434\n",
      "959 : 0.591491162776947\n",
      "960 : 0.5913177728652954\n",
      "961 : 0.5911446213722229\n",
      "962 : 0.5909714102745056\n",
      "963 : 0.5907983183860779\n",
      "964 : 0.5906254649162292\n",
      "965 : 0.5904524922370911\n",
      "966 : 0.5902798175811768\n",
      "967 : 0.5901071429252625\n",
      "968 : 0.5899346470832825\n",
      "969 : 0.5897621512413025\n",
      "970 : 0.5895897746086121\n",
      "971 : 0.589417576789856\n",
      "972 : 0.5892453193664551\n",
      "973 : 0.5890733599662781\n",
      "974 : 0.5889014005661011\n",
      "975 : 0.5887295603752136\n",
      "976 : 0.588557779788971\n",
      "977 : 0.5883861184120178\n",
      "978 : 0.588214635848999\n",
      "979 : 0.5880430936813354\n",
      "980 : 0.5878718495368958\n",
      "981 : 0.5877005457878113\n",
      "982 : 0.5875294208526611\n",
      "983 : 0.5873584151268005\n",
      "984 : 0.5871874690055847\n",
      "985 : 0.5870165228843689\n",
      "986 : 0.5868458151817322\n",
      "987 : 0.5866751670837402\n",
      "988 : 0.5865045189857483\n",
      "989 : 0.5863340497016907\n",
      "990 : 0.5861636996269226\n",
      "991 : 0.5859934687614441\n",
      "992 : 0.5858232975006104\n",
      "993 : 0.5856532454490662\n",
      "994 : 0.5854832530021667\n",
      "995 : 0.5853133797645569\n",
      "996 : 0.5851436257362366\n",
      "997 : 0.5849739909172058\n",
      "998 : 0.584804356098175\n",
      "999 : 0.5846349596977234\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.functional import F  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# x_data = [1.0, 2.0, 3.0]\n",
    "# y_data = [2.0, 4.0, 6.0]\n",
    "\n",
    "# 学习时间和是否通过考试分类\n",
    "x_data = torch.Tensor([[1.0], [2.0], [3.0]])\n",
    "y_data = torch.Tensor([[0], [0], [1]])\n",
    "\n",
    "class LogisticRegressionModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 和线性回归的区别\n",
    "        y_pred = torch.sigmoid(self.linear(x))\n",
    "        return y_pred\n",
    "\n",
    "model = LogisticRegressionModel()\n",
    "\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    y_pred = model(x_data)\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    print(epoch, \":\", loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print('w=', model.linear.weight.item(), \"\\t b=\", model.linear.bias.item())\n",
    "    # x_test = torch.Tensor([5])\n",
    "    # y_test = model(x_test)\n",
    "    # print(\"y_pred=\", y_test.data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(0, 10, 200)\n",
    "x_t = torch.Tensor(x).view((200,1))\n",
    "y_t = model(x_t)\n",
    "y = y_t.data.numpy()\n",
    "plt.plot(x, y)\n",
    "plt.plot([0, 10], [0.5, 0.5], c='r')\n",
    "plt.xlabel('Hours')\n",
    "plt.ylabel('Probability of Pass')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 处理多维特征输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = np.loadtxt('../doc/diabetes.csv.gz', delimiter=',', dtype=np.float32)\n",
    "x_data = torch.from_numpy(xy[:, :-1])\n",
    "y_data = torch.from_numpy(xy[:, [-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((759, 9), torch.Size([759, 8]), torch.Size([759, 1]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy.shape, x_data.shape, y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0    Loss: 0.7150929570198059\n",
      "Epoch:  1    Loss: 0.7073532342910767\n",
      "Epoch:  2    Loss: 0.700061559677124\n",
      "Epoch:  3    Loss: 0.6932289600372314\n",
      "Epoch:  4    Loss: 0.6868628859519958\n",
      "Epoch:  5    Loss: 0.6809678077697754\n",
      "Epoch:  6    Loss: 0.6755450963973999\n",
      "Epoch:  7    Loss: 0.6705924272537231\n",
      "Epoch:  8    Loss: 0.6661044359207153\n",
      "Epoch:  9    Loss: 0.6620724201202393\n",
      "Epoch:  10    Loss: 0.6584846377372742\n",
      "Epoch:  11    Loss: 0.6553259491920471\n",
      "Epoch:  12    Loss: 0.6525784134864807\n",
      "Epoch:  13    Loss: 0.6502215266227722\n",
      "Epoch:  14    Loss: 0.648231565952301\n",
      "Epoch:  15    Loss: 0.6465824842453003\n",
      "Epoch:  16    Loss: 0.6452462077140808\n",
      "Epoch:  17    Loss: 0.6441925764083862\n",
      "Epoch:  18    Loss: 0.6433899998664856\n",
      "Epoch:  19    Loss: 0.642805814743042\n",
      "Epoch:  20    Loss: 0.642406702041626\n",
      "Epoch:  21    Loss: 0.642159640789032\n",
      "Epoch:  22    Loss: 0.6420319676399231\n",
      "Epoch:  23    Loss: 0.6419929265975952\n",
      "Epoch:  24    Loss: 0.6420135498046875\n",
      "Epoch:  25    Loss: 0.6420679092407227\n",
      "Epoch:  26    Loss: 0.642133355140686\n",
      "Epoch:  27    Loss: 0.6421909332275391\n",
      "Epoch:  28    Loss: 0.6422256827354431\n",
      "Epoch:  29    Loss: 0.6422266364097595\n",
      "Epoch:  30    Loss: 0.6421863436698914\n",
      "Epoch:  31    Loss: 0.6421011686325073\n",
      "Epoch:  32    Loss: 0.641970157623291\n",
      "Epoch:  33    Loss: 0.6417953372001648\n",
      "Epoch:  34    Loss: 0.6415805220603943\n",
      "Epoch:  35    Loss: 0.6413310766220093\n",
      "Epoch:  36    Loss: 0.641053318977356\n",
      "Epoch:  37    Loss: 0.640753984451294\n",
      "Epoch:  38    Loss: 0.6404399275779724\n",
      "Epoch:  39    Loss: 0.6401175260543823\n",
      "Epoch:  40    Loss: 0.6397924423217773\n",
      "Epoch:  41    Loss: 0.6394697427749634\n",
      "Epoch:  42    Loss: 0.6391531825065613\n",
      "Epoch:  43    Loss: 0.6388455629348755\n",
      "Epoch:  44    Loss: 0.6385486125946045\n",
      "Epoch:  45    Loss: 0.6382628679275513\n",
      "Epoch:  46    Loss: 0.6379880905151367\n",
      "Epoch:  47    Loss: 0.6377229690551758\n",
      "Epoch:  48    Loss: 0.6374659538269043\n",
      "Epoch:  49    Loss: 0.6372143626213074\n",
      "Epoch:  50    Loss: 0.6369660496711731\n",
      "Epoch:  51    Loss: 0.6367177963256836\n",
      "Epoch:  52    Loss: 0.6364671587944031\n",
      "Epoch:  53    Loss: 0.6362113356590271\n",
      "Epoch:  54    Loss: 0.6359480023384094\n",
      "Epoch:  55    Loss: 0.6356751322746277\n",
      "Epoch:  56    Loss: 0.6353907585144043\n",
      "Epoch:  57    Loss: 0.6350937485694885\n",
      "Epoch:  58    Loss: 0.6347830891609192\n",
      "Epoch:  59    Loss: 0.6344580054283142\n",
      "Epoch:  60    Loss: 0.6341182589530945\n",
      "Epoch:  61    Loss: 0.6337636709213257\n",
      "Epoch:  62    Loss: 0.6333943605422974\n",
      "Epoch:  63    Loss: 0.6330106258392334\n",
      "Epoch:  64    Loss: 0.632612407207489\n",
      "Epoch:  65    Loss: 0.6322001218795776\n",
      "Epoch:  66    Loss: 0.631773829460144\n",
      "Epoch:  67    Loss: 0.6313335299491882\n",
      "Epoch:  68    Loss: 0.6308792233467102\n",
      "Epoch:  69    Loss: 0.6304103136062622\n",
      "Epoch:  70    Loss: 0.6299265623092651\n",
      "Epoch:  71    Loss: 0.6294272541999817\n",
      "Epoch:  72    Loss: 0.6289117336273193\n",
      "Epoch:  73    Loss: 0.6283787488937378\n",
      "Epoch:  74    Loss: 0.6278275847434998\n",
      "Epoch:  75    Loss: 0.6272570490837097\n",
      "Epoch:  76    Loss: 0.6266659498214722\n",
      "Epoch:  77    Loss: 0.6260532140731812\n",
      "Epoch:  78    Loss: 0.6254174113273621\n",
      "Epoch:  79    Loss: 0.6247576475143433\n",
      "Epoch:  80    Loss: 0.6240725517272949\n",
      "Epoch:  81    Loss: 0.6233611106872559\n",
      "Epoch:  82    Loss: 0.6226219534873962\n",
      "Epoch:  83    Loss: 0.6218540072441101\n",
      "Epoch:  84    Loss: 0.621056079864502\n",
      "Epoch:  85    Loss: 0.6202268600463867\n",
      "Epoch:  86    Loss: 0.6193649172782898\n",
      "Epoch:  87    Loss: 0.6184689402580261\n",
      "Epoch:  88    Loss: 0.6175374388694763\n",
      "Epoch:  89    Loss: 0.6165688633918762\n",
      "Epoch:  90    Loss: 0.6155612468719482\n",
      "Epoch:  91    Loss: 0.6145130395889282\n",
      "Epoch:  92    Loss: 0.613422155380249\n",
      "Epoch:  93    Loss: 0.6122866272926331\n",
      "Epoch:  94    Loss: 0.6111046671867371\n",
      "Epoch:  95    Loss: 0.6098739504814148\n",
      "Epoch:  96    Loss: 0.6085926294326782\n",
      "Epoch:  97    Loss: 0.6072586178779602\n",
      "Epoch:  98    Loss: 0.6058701276779175\n",
      "Epoch:  99    Loss: 0.6044253706932068\n",
      "Epoch:  100    Loss: 0.6029227375984192\n",
      "Epoch:  101    Loss: 0.6013607382774353\n",
      "Epoch:  102    Loss: 0.5997384190559387\n",
      "Epoch:  103    Loss: 0.5980546474456787\n",
      "Epoch:  104    Loss: 0.5963089466094971\n",
      "Epoch:  105    Loss: 0.5945010781288147\n",
      "Epoch:  106    Loss: 0.5926310420036316\n",
      "Epoch:  107    Loss: 0.5906994938850403\n",
      "Epoch:  108    Loss: 0.5887071490287781\n",
      "Epoch:  109    Loss: 0.5866555571556091\n",
      "Epoch:  110    Loss: 0.5845465064048767\n",
      "Epoch:  111    Loss: 0.5823820233345032\n",
      "Epoch:  112    Loss: 0.5801650285720825\n",
      "Epoch:  113    Loss: 0.5778987407684326\n",
      "Epoch:  114    Loss: 0.5755864977836609\n",
      "Epoch:  115    Loss: 0.5732323527336121\n",
      "Epoch:  116    Loss: 0.5708402395248413\n",
      "Epoch:  117    Loss: 0.5684148073196411\n",
      "Epoch:  118    Loss: 0.5659602880477905\n",
      "Epoch:  119    Loss: 0.5634816884994507\n",
      "Epoch:  120    Loss: 0.5609837174415588\n",
      "Epoch:  121    Loss: 0.558471143245697\n",
      "Epoch:  122    Loss: 0.5559490919113159\n",
      "Epoch:  123    Loss: 0.5534225106239319\n",
      "Epoch:  124    Loss: 0.5508962869644165\n",
      "Epoch:  125    Loss: 0.5483754873275757\n",
      "Epoch:  126    Loss: 0.5458645224571228\n",
      "Epoch:  127    Loss: 0.5433681607246399\n",
      "Epoch:  128    Loss: 0.5408903956413269\n",
      "Epoch:  129    Loss: 0.5384353399276733\n",
      "Epoch:  130    Loss: 0.5360063314437866\n",
      "Epoch:  131    Loss: 0.5336065888404846\n",
      "Epoch:  132    Loss: 0.5312389731407166\n",
      "Epoch:  133    Loss: 0.5289060473442078\n",
      "Epoch:  134    Loss: 0.5266098976135254\n",
      "Epoch:  135    Loss: 0.5243525505065918\n",
      "Epoch:  136    Loss: 0.5221354961395264\n",
      "Epoch:  137    Loss: 0.5199601054191589\n",
      "Epoch:  138    Loss: 0.5178276896476746\n",
      "Epoch:  139    Loss: 0.5157394409179688\n",
      "Epoch:  140    Loss: 0.5136964321136475\n",
      "Epoch:  141    Loss: 0.511699914932251\n",
      "Epoch:  142    Loss: 0.5097512006759644\n",
      "Epoch:  143    Loss: 0.5078514218330383\n",
      "Epoch:  144    Loss: 0.5060020089149475\n",
      "Epoch:  145    Loss: 0.5042045712471008\n",
      "Epoch:  146    Loss: 0.5024601817131042\n",
      "Epoch:  147    Loss: 0.5007704496383667\n",
      "Epoch:  148    Loss: 0.4991365373134613\n",
      "Epoch:  149    Loss: 0.49755963683128357\n",
      "Epoch:  150    Loss: 0.4960409104824066\n",
      "Epoch:  151    Loss: 0.4945808947086334\n",
      "Epoch:  152    Loss: 0.49318021535873413\n",
      "Epoch:  153    Loss: 0.49183905124664307\n",
      "Epoch:  154    Loss: 0.4905571937561035\n",
      "Epoch:  155    Loss: 0.4893342852592468\n",
      "Epoch:  156    Loss: 0.48816943168640137\n",
      "Epoch:  157    Loss: 0.48706161975860596\n",
      "Epoch:  158    Loss: 0.4860093891620636\n",
      "Epoch:  159    Loss: 0.48501116037368774\n",
      "Epoch:  160    Loss: 0.4840649664402008\n",
      "Epoch:  161    Loss: 0.4831688404083252\n",
      "Epoch:  162    Loss: 0.482320636510849\n",
      "Epoch:  163    Loss: 0.48151808977127075\n",
      "Epoch:  164    Loss: 0.4807589650154114\n",
      "Epoch:  165    Loss: 0.48004111647605896\n",
      "Epoch:  166    Loss: 0.4793621599674225\n",
      "Epoch:  167    Loss: 0.4787200391292572\n",
      "Epoch:  168    Loss: 0.47811266779899597\n",
      "Epoch:  169    Loss: 0.4775381088256836\n",
      "Epoch:  170    Loss: 0.47699424624443054\n",
      "Epoch:  171    Loss: 0.4764794707298279\n",
      "Epoch:  172    Loss: 0.47599178552627563\n",
      "Epoch:  173    Loss: 0.4755297601222992\n",
      "Epoch:  174    Loss: 0.4750915467739105\n",
      "Epoch:  175    Loss: 0.4746757447719574\n",
      "Epoch:  176    Loss: 0.47428086400032043\n",
      "Epoch:  177    Loss: 0.4739055931568146\n",
      "Epoch:  178    Loss: 0.4735485017299652\n",
      "Epoch:  179    Loss: 0.4732084572315216\n",
      "Epoch:  180    Loss: 0.47288447618484497\n",
      "Epoch:  181    Loss: 0.47257518768310547\n",
      "Epoch:  182    Loss: 0.4722799062728882\n",
      "Epoch:  183    Loss: 0.47199755907058716\n",
      "Epoch:  184    Loss: 0.4717274606227875\n",
      "Epoch:  185    Loss: 0.47146883606910706\n",
      "Epoch:  186    Loss: 0.4712209105491638\n",
      "Epoch:  187    Loss: 0.47098299860954285\n",
      "Epoch:  188    Loss: 0.47075456380844116\n",
      "Epoch:  189    Loss: 0.47053489089012146\n",
      "Epoch:  190    Loss: 0.4703233540058136\n",
      "Epoch:  191    Loss: 0.4701194167137146\n",
      "Epoch:  192    Loss: 0.46992257237434387\n",
      "Epoch:  193    Loss: 0.46973228454589844\n",
      "Epoch:  194    Loss: 0.46954795718193054\n",
      "Epoch:  195    Loss: 0.4693693220615387\n",
      "Epoch:  196    Loss: 0.46919581294059753\n",
      "Epoch:  197    Loss: 0.46902716159820557\n",
      "Epoch:  198    Loss: 0.46886274218559265\n",
      "Epoch:  199    Loss: 0.46870243549346924\n",
      "Epoch:  200    Loss: 0.4685458540916443\n",
      "Epoch:  201    Loss: 0.4683927297592163\n",
      "Epoch:  202    Loss: 0.4682427942752838\n",
      "Epoch:  203    Loss: 0.4680957794189453\n",
      "Epoch:  204    Loss: 0.46795135736465454\n",
      "Epoch:  205    Loss: 0.4678093492984772\n",
      "Epoch:  206    Loss: 0.4676695764064789\n",
      "Epoch:  207    Loss: 0.467531681060791\n",
      "Epoch:  208    Loss: 0.46739545464515686\n",
      "Epoch:  209    Loss: 0.4672606289386749\n",
      "Epoch:  210    Loss: 0.46712711453437805\n",
      "Epoch:  211    Loss: 0.4669945240020752\n",
      "Epoch:  212    Loss: 0.46686264872550964\n",
      "Epoch:  213    Loss: 0.46673133969306946\n",
      "Epoch:  214    Loss: 0.4666004180908203\n",
      "Epoch:  215    Loss: 0.4664696156978607\n",
      "Epoch:  216    Loss: 0.46633875370025635\n",
      "Epoch:  217    Loss: 0.46620771288871765\n",
      "Epoch:  218    Loss: 0.46607640385627747\n",
      "Epoch:  219    Loss: 0.46594443917274475\n",
      "Epoch:  220    Loss: 0.4658118188381195\n",
      "Epoch:  221    Loss: 0.4656784236431122\n",
      "Epoch:  222    Loss: 0.4655441343784332\n",
      "Epoch:  223    Loss: 0.4654087424278259\n",
      "Epoch:  224    Loss: 0.4652721583843231\n",
      "Epoch:  225    Loss: 0.4651343822479248\n",
      "Epoch:  226    Loss: 0.4649951756000519\n",
      "Epoch:  227    Loss: 0.4648544490337372\n",
      "Epoch:  228    Loss: 0.46471214294433594\n",
      "Epoch:  229    Loss: 0.464568167924881\n",
      "Epoch:  230    Loss: 0.464422345161438\n",
      "Epoch:  231    Loss: 0.46427470445632935\n",
      "Epoch:  232    Loss: 0.4641250669956207\n",
      "Epoch:  233    Loss: 0.46397337317466736\n",
      "Epoch:  234    Loss: 0.4638194739818573\n",
      "Epoch:  235    Loss: 0.46366333961486816\n",
      "Epoch:  236    Loss: 0.46350476145744324\n",
      "Epoch:  237    Loss: 0.4633437991142273\n",
      "Epoch:  238    Loss: 0.4631803333759308\n",
      "Epoch:  239    Loss: 0.4630141854286194\n",
      "Epoch:  240    Loss: 0.4628453254699707\n",
      "Epoch:  241    Loss: 0.4626736640930176\n",
      "Epoch:  242    Loss: 0.4624989926815033\n",
      "Epoch:  243    Loss: 0.4623214602470398\n",
      "Epoch:  244    Loss: 0.4621407985687256\n",
      "Epoch:  245    Loss: 0.4619569182395935\n",
      "Epoch:  246    Loss: 0.46176984906196594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  247    Loss: 0.46157950162887573\n",
      "Epoch:  248    Loss: 0.46138566732406616\n",
      "Epoch:  249    Loss: 0.4611884355545044\n",
      "Epoch:  250    Loss: 0.4609876275062561\n",
      "Epoch:  251    Loss: 0.4607833921909332\n",
      "Epoch:  252    Loss: 0.4605754613876343\n",
      "Epoch:  253    Loss: 0.4603639245033264\n",
      "Epoch:  254    Loss: 0.46014875173568726\n",
      "Epoch:  255    Loss: 0.459929883480072\n",
      "Epoch:  256    Loss: 0.4597073495388031\n",
      "Epoch:  257    Loss: 0.4594811797142029\n",
      "Epoch:  258    Loss: 0.45925140380859375\n",
      "Epoch:  259    Loss: 0.4590179920196533\n",
      "Epoch:  260    Loss: 0.45878103375434875\n",
      "Epoch:  261    Loss: 0.45854052901268005\n",
      "Epoch:  262    Loss: 0.45829659700393677\n",
      "Epoch:  263    Loss: 0.45804914832115173\n",
      "Epoch:  264    Loss: 0.45779848098754883\n",
      "Epoch:  265    Loss: 0.4575445353984833\n",
      "Epoch:  266    Loss: 0.45728737115859985\n",
      "Epoch:  267    Loss: 0.4570271074771881\n",
      "Epoch:  268    Loss: 0.45676377415657043\n",
      "Epoch:  269    Loss: 0.45649752020835876\n",
      "Epoch:  270    Loss: 0.45622846484184265\n",
      "Epoch:  271    Loss: 0.45595666766166687\n",
      "Epoch:  272    Loss: 0.45568203926086426\n",
      "Epoch:  273    Loss: 0.45540493726730347\n",
      "Epoch:  274    Loss: 0.45512524247169495\n",
      "Epoch:  275    Loss: 0.4548431634902954\n",
      "Epoch:  276    Loss: 0.454558789730072\n",
      "Epoch:  277    Loss: 0.45427221059799194\n",
      "Epoch:  278    Loss: 0.4539834260940552\n",
      "Epoch:  279    Loss: 0.4536927342414856\n",
      "Epoch:  280    Loss: 0.4534001052379608\n",
      "Epoch:  281    Loss: 0.4531056880950928\n",
      "Epoch:  282    Loss: 0.4528096616268158\n",
      "Epoch:  283    Loss: 0.45251214504241943\n",
      "Epoch:  284    Loss: 0.45221325755119324\n",
      "Epoch:  285    Loss: 0.45191308856010437\n",
      "Epoch:  286    Loss: 0.45161187648773193\n",
      "Epoch:  287    Loss: 0.4513097107410431\n",
      "Epoch:  288    Loss: 0.45100682973861694\n",
      "Epoch:  289    Loss: 0.45070332288742065\n",
      "Epoch:  290    Loss: 0.45039936900138855\n",
      "Epoch:  291    Loss: 0.45009511709213257\n",
      "Epoch:  292    Loss: 0.44979071617126465\n",
      "Epoch:  293    Loss: 0.4494863748550415\n",
      "Epoch:  294    Loss: 0.4491822421550751\n",
      "Epoch:  295    Loss: 0.44887852668762207\n",
      "Epoch:  296    Loss: 0.44857531785964966\n",
      "Epoch:  297    Loss: 0.4482727348804474\n",
      "Epoch:  298    Loss: 0.4479711353778839\n",
      "Epoch:  299    Loss: 0.4476705491542816\n",
      "Epoch:  300    Loss: 0.44737112522125244\n",
      "Epoch:  301    Loss: 0.4470730721950531\n",
      "Epoch:  302    Loss: 0.44677653908729553\n",
      "Epoch:  303    Loss: 0.44648173451423645\n",
      "Epoch:  304    Loss: 0.446188747882843\n",
      "Epoch:  305    Loss: 0.4458977282047272\n",
      "Epoch:  306    Loss: 0.44560888409614563\n",
      "Epoch:  307    Loss: 0.4453223943710327\n",
      "Epoch:  308    Loss: 0.4450383186340332\n",
      "Epoch:  309    Loss: 0.4447568655014038\n",
      "Epoch:  310    Loss: 0.4444781243801117\n",
      "Epoch:  311    Loss: 0.4442022740840912\n",
      "Epoch:  312    Loss: 0.4439294934272766\n",
      "Epoch:  313    Loss: 0.44365981221199036\n",
      "Epoch:  314    Loss: 0.44339337944984436\n",
      "Epoch:  315    Loss: 0.44313034415245056\n",
      "Epoch:  316    Loss: 0.44287076592445374\n",
      "Epoch:  317    Loss: 0.4426148533821106\n",
      "Epoch:  318    Loss: 0.44236257672309875\n",
      "Epoch:  319    Loss: 0.4421141445636749\n",
      "Epoch:  320    Loss: 0.4418694078922272\n",
      "Epoch:  321    Loss: 0.44162875413894653\n",
      "Epoch:  322    Loss: 0.4413920044898987\n",
      "Epoch:  323    Loss: 0.44115936756134033\n",
      "Epoch:  324    Loss: 0.4409308433532715\n",
      "Epoch:  325    Loss: 0.44070640206336975\n",
      "Epoch:  326    Loss: 0.4404861330986023\n",
      "Epoch:  327    Loss: 0.44027015566825867\n",
      "Epoch:  328    Loss: 0.44005829095840454\n",
      "Epoch:  329    Loss: 0.43985065817832947\n",
      "Epoch:  330    Loss: 0.43964725732803345\n",
      "Epoch:  331    Loss: 0.4394480288028717\n",
      "Epoch:  332    Loss: 0.43925294280052185\n",
      "Epoch:  333    Loss: 0.43906205892562866\n",
      "Epoch:  334    Loss: 0.4388752579689026\n",
      "Epoch:  335    Loss: 0.43869251012802124\n",
      "Epoch:  336    Loss: 0.43851378560066223\n",
      "Epoch:  337    Loss: 0.438338965177536\n",
      "Epoch:  338    Loss: 0.4381680488586426\n",
      "Epoch:  339    Loss: 0.43800094723701477\n",
      "Epoch:  340    Loss: 0.4378375709056854\n",
      "Epoch:  341    Loss: 0.437677800655365\n",
      "Epoch:  342    Loss: 0.4375215470790863\n",
      "Epoch:  343    Loss: 0.43736881017684937\n",
      "Epoch:  344    Loss: 0.43721938133239746\n",
      "Epoch:  345    Loss: 0.4370732009410858\n",
      "Epoch:  346    Loss: 0.43693020939826965\n",
      "Epoch:  347    Loss: 0.43679022789001465\n",
      "Epoch:  348    Loss: 0.43665313720703125\n",
      "Epoch:  349    Loss: 0.43651893734931946\n",
      "Epoch:  350    Loss: 0.4363873600959778\n",
      "Epoch:  351    Loss: 0.43625837564468384\n",
      "Epoch:  352    Loss: 0.43613189458847046\n",
      "Epoch:  353    Loss: 0.4360078275203705\n",
      "Epoch:  354    Loss: 0.4358859062194824\n",
      "Epoch:  355    Loss: 0.43576622009277344\n",
      "Epoch:  356    Loss: 0.4356485903263092\n",
      "Epoch:  357    Loss: 0.435532808303833\n",
      "Epoch:  358    Loss: 0.4354189336299896\n",
      "Epoch:  359    Loss: 0.43530675768852234\n",
      "Epoch:  360    Loss: 0.435196191072464\n",
      "Epoch:  361    Loss: 0.4350871741771698\n",
      "Epoch:  362    Loss: 0.4349796175956726\n",
      "Epoch:  363    Loss: 0.43487343192100525\n",
      "Epoch:  364    Loss: 0.43476852774620056\n",
      "Epoch:  365    Loss: 0.43466484546661377\n",
      "Epoch:  366    Loss: 0.4345623254776001\n",
      "Epoch:  367    Loss: 0.4344607889652252\n",
      "Epoch:  368    Loss: 0.4343602657318115\n",
      "Epoch:  369    Loss: 0.4342607259750366\n",
      "Epoch:  370    Loss: 0.43416205048561096\n",
      "Epoch:  371    Loss: 0.43406417965888977\n",
      "Epoch:  372    Loss: 0.43396714329719543\n",
      "Epoch:  373    Loss: 0.4338708519935608\n",
      "Epoch:  374    Loss: 0.43377527594566345\n",
      "Epoch:  375    Loss: 0.4336804449558258\n",
      "Epoch:  376    Loss: 0.43358615040779114\n",
      "Epoch:  377    Loss: 0.433492511510849\n",
      "Epoch:  378    Loss: 0.433399498462677\n",
      "Epoch:  379    Loss: 0.433307021856308\n",
      "Epoch:  380    Loss: 0.4332152009010315\n",
      "Epoch:  381    Loss: 0.4331238567829132\n",
      "Epoch:  382    Loss: 0.4330330193042755\n",
      "Epoch:  383    Loss: 0.43294277787208557\n",
      "Epoch:  384    Loss: 0.43285301327705383\n",
      "Epoch:  385    Loss: 0.4327637553215027\n",
      "Epoch:  386    Loss: 0.43267491459846497\n",
      "Epoch:  387    Loss: 0.432586669921875\n",
      "Epoch:  388    Loss: 0.4324988126754761\n",
      "Epoch:  389    Loss: 0.43241143226623535\n",
      "Epoch:  390    Loss: 0.43232446908950806\n",
      "Epoch:  391    Loss: 0.43223798274993896\n",
      "Epoch:  392    Loss: 0.4321519434452057\n",
      "Epoch:  393    Loss: 0.43206629157066345\n",
      "Epoch:  394    Loss: 0.43198105692863464\n",
      "Epoch:  395    Loss: 0.4318961799144745\n",
      "Epoch:  396    Loss: 0.431811660528183\n",
      "Epoch:  397    Loss: 0.4317275583744049\n",
      "Epoch:  398    Loss: 0.4316438138484955\n",
      "Epoch:  399    Loss: 0.43156036734580994\n",
      "Epoch:  400    Loss: 0.43147721886634827\n",
      "Epoch:  401    Loss: 0.43139439821243286\n",
      "Epoch:  402    Loss: 0.43131184577941895\n",
      "Epoch:  403    Loss: 0.4312295913696289\n",
      "Epoch:  404    Loss: 0.4311475157737732\n",
      "Epoch:  405    Loss: 0.43106573820114136\n",
      "Epoch:  406    Loss: 0.43098416924476624\n",
      "Epoch:  407    Loss: 0.4309026896953583\n",
      "Epoch:  408    Loss: 0.4308214783668518\n",
      "Epoch:  409    Loss: 0.4307404160499573\n",
      "Epoch:  410    Loss: 0.4306594431400299\n",
      "Epoch:  411    Loss: 0.4305785894393921\n",
      "Epoch:  412    Loss: 0.4304978549480438\n",
      "Epoch:  413    Loss: 0.4304172694683075\n",
      "Epoch:  414    Loss: 0.4303366243839264\n",
      "Epoch:  415    Loss: 0.43025603890419006\n",
      "Epoch:  416    Loss: 0.4301755428314209\n",
      "Epoch:  417    Loss: 0.43009501695632935\n",
      "Epoch:  418    Loss: 0.4300144612789154\n",
      "Epoch:  419    Loss: 0.4299338459968567\n",
      "Epoch:  420    Loss: 0.4298532009124756\n",
      "Epoch:  421    Loss: 0.4297724664211273\n",
      "Epoch:  422    Loss: 0.4296916425228119\n",
      "Epoch:  423    Loss: 0.4296106994152069\n",
      "Epoch:  424    Loss: 0.42952960729599\n",
      "Epoch:  425    Loss: 0.42944833636283875\n",
      "Epoch:  426    Loss: 0.42936697602272034\n",
      "Epoch:  427    Loss: 0.42928534746170044\n",
      "Epoch:  428    Loss: 0.42920348048210144\n",
      "Epoch:  429    Loss: 0.4291214644908905\n",
      "Epoch:  430    Loss: 0.4290391206741333\n",
      "Epoch:  431    Loss: 0.4289564788341522\n",
      "Epoch:  432    Loss: 0.42887356877326965\n",
      "Epoch:  433    Loss: 0.4287903606891632\n",
      "Epoch:  434    Loss: 0.42870670557022095\n",
      "Epoch:  435    Loss: 0.4286228120326996\n",
      "Epoch:  436    Loss: 0.42853844165802\n",
      "Epoch:  437    Loss: 0.4284536838531494\n",
      "Epoch:  438    Loss: 0.428368479013443\n",
      "Epoch:  439    Loss: 0.42828288674354553\n",
      "Epoch:  440    Loss: 0.4281967580318451\n",
      "Epoch:  441    Loss: 0.42811015248298645\n",
      "Epoch:  442    Loss: 0.42802298069000244\n",
      "Epoch:  443    Loss: 0.42793533205986023\n",
      "Epoch:  444    Loss: 0.4278470277786255\n",
      "Epoch:  445    Loss: 0.42775821685791016\n",
      "Epoch:  446    Loss: 0.4276687502861023\n",
      "Epoch:  447    Loss: 0.4275786578655243\n",
      "Epoch:  448    Loss: 0.42748793959617615\n",
      "Epoch:  449    Loss: 0.4273965060710907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  450    Loss: 0.4273044466972351\n",
      "Epoch:  451    Loss: 0.42721158266067505\n",
      "Epoch:  452    Loss: 0.4271180331707001\n",
      "Epoch:  453    Loss: 0.4270237386226654\n",
      "Epoch:  454    Loss: 0.42692872881889343\n",
      "Epoch:  455    Loss: 0.4268328845500946\n",
      "Epoch:  456    Loss: 0.4267362654209137\n",
      "Epoch:  457    Loss: 0.4266388714313507\n",
      "Epoch:  458    Loss: 0.42654070258140564\n",
      "Epoch:  459    Loss: 0.42644166946411133\n",
      "Epoch:  460    Loss: 0.42634183168411255\n",
      "Epoch:  461    Loss: 0.4262412190437317\n",
      "Epoch:  462    Loss: 0.42613983154296875\n",
      "Epoch:  463    Loss: 0.42603757977485657\n",
      "Epoch:  464    Loss: 0.4259345233440399\n",
      "Epoch:  465    Loss: 0.4258306622505188\n",
      "Epoch:  466    Loss: 0.42572611570358276\n",
      "Epoch:  467    Loss: 0.4256207346916199\n",
      "Epoch:  468    Loss: 0.4255146384239197\n",
      "Epoch:  469    Loss: 0.42540785670280457\n",
      "Epoch:  470    Loss: 0.4253004193305969\n",
      "Epoch:  471    Loss: 0.4251922369003296\n",
      "Epoch:  472    Loss: 0.4250833988189697\n",
      "Epoch:  473    Loss: 0.42497408390045166\n",
      "Epoch:  474    Loss: 0.42486414313316345\n",
      "Epoch:  475    Loss: 0.42475366592407227\n",
      "Epoch:  476    Loss: 0.42464277148246765\n",
      "Epoch:  477    Loss: 0.4245314300060272\n",
      "Epoch:  478    Loss: 0.42441970109939575\n",
      "Epoch:  479    Loss: 0.424307644367218\n",
      "Epoch:  480    Loss: 0.4241952896118164\n",
      "Epoch:  481    Loss: 0.42408275604248047\n",
      "Epoch:  482    Loss: 0.42396998405456543\n",
      "Epoch:  483    Loss: 0.4238571226596832\n",
      "Epoch:  484    Loss: 0.42374417185783386\n",
      "Epoch:  485    Loss: 0.4236311912536621\n",
      "Epoch:  486    Loss: 0.42351824045181274\n",
      "Epoch:  487    Loss: 0.42340540885925293\n",
      "Epoch:  488    Loss: 0.42329272627830505\n",
      "Epoch:  489    Loss: 0.4231801927089691\n",
      "Epoch:  490    Loss: 0.42306792736053467\n",
      "Epoch:  491    Loss: 0.4229559302330017\n",
      "Epoch:  492    Loss: 0.4228442907333374\n",
      "Epoch:  493    Loss: 0.42273303866386414\n",
      "Epoch:  494    Loss: 0.4226222634315491\n",
      "Epoch:  495    Loss: 0.42251187562942505\n",
      "Epoch:  496    Loss: 0.4224020540714264\n",
      "Epoch:  497    Loss: 0.4222927689552307\n",
      "Epoch:  498    Loss: 0.4221840500831604\n",
      "Epoch:  499    Loss: 0.4220760464668274\n",
      "Epoch:  500    Loss: 0.42196863889694214\n",
      "Epoch:  501    Loss: 0.4218619763851166\n",
      "Epoch:  502    Loss: 0.4217560291290283\n",
      "Epoch:  503    Loss: 0.42165079712867737\n",
      "Epoch:  504    Loss: 0.4215463697910309\n",
      "Epoch:  505    Loss: 0.42144280672073364\n",
      "Epoch:  506    Loss: 0.42134004831314087\n",
      "Epoch:  507    Loss: 0.4212380349636078\n",
      "Epoch:  508    Loss: 0.4211369454860687\n",
      "Epoch:  509    Loss: 0.4210367500782013\n",
      "Epoch:  510    Loss: 0.4209374785423279\n",
      "Epoch:  511    Loss: 0.4208391010761261\n",
      "Epoch:  512    Loss: 0.42074161767959595\n",
      "Epoch:  513    Loss: 0.4206451177597046\n",
      "Epoch:  514    Loss: 0.4205494821071625\n",
      "Epoch:  515    Loss: 0.4204549193382263\n",
      "Epoch:  516    Loss: 0.420361191034317\n",
      "Epoch:  517    Loss: 0.4202684760093689\n",
      "Epoch:  518    Loss: 0.4201767146587372\n",
      "Epoch:  519    Loss: 0.42008596658706665\n",
      "Epoch:  520    Loss: 0.41999614238739014\n",
      "Epoch:  521    Loss: 0.41990727186203003\n",
      "Epoch:  522    Loss: 0.4198194146156311\n",
      "Epoch:  523    Loss: 0.41973254084587097\n",
      "Epoch:  524    Loss: 0.4196465313434601\n",
      "Epoch:  525    Loss: 0.41956156492233276\n",
      "Epoch:  526    Loss: 0.41947758197784424\n",
      "Epoch:  527    Loss: 0.41939446330070496\n",
      "Epoch:  528    Loss: 0.41931232810020447\n",
      "Epoch:  529    Loss: 0.419231116771698\n",
      "Epoch:  530    Loss: 0.41915085911750793\n",
      "Epoch:  531    Loss: 0.4190714955329895\n",
      "Epoch:  532    Loss: 0.4189929962158203\n",
      "Epoch:  533    Loss: 0.4189154803752899\n",
      "Epoch:  534    Loss: 0.41883882880210876\n",
      "Epoch:  535    Loss: 0.41876301169395447\n",
      "Epoch:  536    Loss: 0.4186881482601166\n",
      "Epoch:  537    Loss: 0.41861408948898315\n",
      "Epoch:  538    Loss: 0.418540894985199\n",
      "Epoch:  539    Loss: 0.41846850514411926\n",
      "Epoch:  540    Loss: 0.4183969497680664\n",
      "Epoch:  541    Loss: 0.418326199054718\n",
      "Epoch:  542    Loss: 0.41825631260871887\n",
      "Epoch:  543    Loss: 0.4181871712207794\n",
      "Epoch:  544    Loss: 0.41811880469322205\n",
      "Epoch:  545    Loss: 0.4180511236190796\n",
      "Epoch:  546    Loss: 0.4179843068122864\n",
      "Epoch:  547    Loss: 0.4179181158542633\n",
      "Epoch:  548    Loss: 0.4178527891635895\n",
      "Epoch:  549    Loss: 0.4177880585193634\n",
      "Epoch:  550    Loss: 0.41772404313087463\n",
      "Epoch:  551    Loss: 0.4176607131958008\n",
      "Epoch:  552    Loss: 0.41759803891181946\n",
      "Epoch:  553    Loss: 0.41753607988357544\n",
      "Epoch:  554    Loss: 0.41747474670410156\n",
      "Epoch:  555    Loss: 0.4174140393733978\n",
      "Epoch:  556    Loss: 0.41735389828681946\n",
      "Epoch:  557    Loss: 0.417294442653656\n",
      "Epoch:  558    Loss: 0.4172355532646179\n",
      "Epoch:  559    Loss: 0.4171772599220276\n",
      "Epoch:  560    Loss: 0.417119562625885\n",
      "Epoch:  561    Loss: 0.4170624017715454\n",
      "Epoch:  562    Loss: 0.4170057773590088\n",
      "Epoch:  563    Loss: 0.41694971919059753\n",
      "Epoch:  564    Loss: 0.41689422726631165\n",
      "Epoch:  565    Loss: 0.4168391525745392\n",
      "Epoch:  566    Loss: 0.4167845845222473\n",
      "Epoch:  567    Loss: 0.4167306423187256\n",
      "Epoch:  568    Loss: 0.4166770875453949\n",
      "Epoch:  569    Loss: 0.4166240394115448\n",
      "Epoch:  570    Loss: 0.41657140851020813\n",
      "Epoch:  571    Loss: 0.41651931405067444\n",
      "Epoch:  572    Loss: 0.4164676070213318\n",
      "Epoch:  573    Loss: 0.4164164066314697\n",
      "Epoch:  574    Loss: 0.4163655638694763\n",
      "Epoch:  575    Loss: 0.4163151681423187\n",
      "Epoch:  576    Loss: 0.4162651002407074\n",
      "Epoch:  577    Loss: 0.41621553897857666\n",
      "Epoch:  578    Loss: 0.41616636514663696\n",
      "Epoch:  579    Loss: 0.41611751914024353\n",
      "Epoch:  580    Loss: 0.41606906056404114\n",
      "Epoch:  581    Loss: 0.4160209596157074\n",
      "Epoch:  582    Loss: 0.4159732758998871\n",
      "Epoch:  583    Loss: 0.41592592000961304\n",
      "Epoch:  584    Loss: 0.4158788323402405\n",
      "Epoch:  585    Loss: 0.41583219170570374\n",
      "Epoch:  586    Loss: 0.41578584909439087\n",
      "Epoch:  587    Loss: 0.4157398045063019\n",
      "Epoch:  588    Loss: 0.41569405794143677\n",
      "Epoch:  589    Loss: 0.41564860939979553\n",
      "Epoch:  590    Loss: 0.41560348868370056\n",
      "Epoch:  591    Loss: 0.4155586361885071\n",
      "Epoch:  592    Loss: 0.41551411151885986\n",
      "Epoch:  593    Loss: 0.41546979546546936\n",
      "Epoch:  594    Loss: 0.4154258072376251\n",
      "Epoch:  595    Loss: 0.4153820872306824\n",
      "Epoch:  596    Loss: 0.4153386056423187\n",
      "Epoch:  597    Loss: 0.41529542207717896\n",
      "Epoch:  598    Loss: 0.4152524471282959\n",
      "Epoch:  599    Loss: 0.41520965099334717\n",
      "Epoch:  600    Loss: 0.4151671528816223\n",
      "Epoch:  601    Loss: 0.41512489318847656\n",
      "Epoch:  602    Loss: 0.41508281230926514\n",
      "Epoch:  603    Loss: 0.4150409996509552\n",
      "Epoch:  604    Loss: 0.4149993360042572\n",
      "Epoch:  605    Loss: 0.4149579703807831\n",
      "Epoch:  606    Loss: 0.4149167239665985\n",
      "Epoch:  607    Loss: 0.41487574577331543\n",
      "Epoch:  608    Loss: 0.4148348867893219\n",
      "Epoch:  609    Loss: 0.4147942066192627\n",
      "Epoch:  610    Loss: 0.4147537350654602\n",
      "Epoch:  611    Loss: 0.41471341252326965\n",
      "Epoch:  612    Loss: 0.4146732985973358\n",
      "Epoch:  613    Loss: 0.41463327407836914\n",
      "Epoch:  614    Loss: 0.41459348797798157\n",
      "Epoch:  615    Loss: 0.41455379128456116\n",
      "Epoch:  616    Loss: 0.4145142734050751\n",
      "Epoch:  617    Loss: 0.4144749045372009\n",
      "Epoch:  618    Loss: 0.41443565487861633\n",
      "Epoch:  619    Loss: 0.4143965542316437\n",
      "Epoch:  620    Loss: 0.4143575429916382\n",
      "Epoch:  621    Loss: 0.414318710565567\n",
      "Epoch:  622    Loss: 0.4142799973487854\n",
      "Epoch:  623    Loss: 0.41424134373664856\n",
      "Epoch:  624    Loss: 0.4142027795314789\n",
      "Epoch:  625    Loss: 0.41416439414024353\n",
      "Epoch:  626    Loss: 0.41412606835365295\n",
      "Epoch:  627    Loss: 0.41408780217170715\n",
      "Epoch:  628    Loss: 0.4140496253967285\n",
      "Epoch:  629    Loss: 0.4140115976333618\n",
      "Epoch:  630    Loss: 0.4139736294746399\n",
      "Epoch:  631    Loss: 0.41393572092056274\n",
      "Epoch:  632    Loss: 0.413897842168808\n",
      "Epoch:  633    Loss: 0.41386014223098755\n",
      "Epoch:  634    Loss: 0.4138224124908447\n",
      "Epoch:  635    Loss: 0.4137847423553467\n",
      "Epoch:  636    Loss: 0.4137471914291382\n",
      "Epoch:  637    Loss: 0.4137096405029297\n",
      "Epoch:  638    Loss: 0.4136720895767212\n",
      "Epoch:  639    Loss: 0.41363465785980225\n",
      "Epoch:  640    Loss: 0.4135972261428833\n",
      "Epoch:  641    Loss: 0.41355982422828674\n",
      "Epoch:  642    Loss: 0.4135224223136902\n",
      "Epoch:  643    Loss: 0.4134850800037384\n",
      "Epoch:  644    Loss: 0.413447767496109\n",
      "Epoch:  645    Loss: 0.4134104549884796\n",
      "Epoch:  646    Loss: 0.4133731722831726\n",
      "Epoch:  647    Loss: 0.4133358597755432\n",
      "Epoch:  648    Loss: 0.413298636674881\n",
      "Epoch:  649    Loss: 0.4132613241672516\n",
      "Epoch:  650    Loss: 0.4132240414619446\n",
      "Epoch:  651    Loss: 0.4131867289543152\n",
      "Epoch:  652    Loss: 0.4131493866443634\n",
      "Epoch:  653    Loss: 0.413112074136734\n",
      "Epoch:  654    Loss: 0.41307467222213745\n",
      "Epoch:  655    Loss: 0.41303735971450806\n",
      "Epoch:  656    Loss: 0.4129999279975891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  657    Loss: 0.41296249628067017\n",
      "Epoch:  658    Loss: 0.41292503476142883\n",
      "Epoch:  659    Loss: 0.4128875136375427\n",
      "Epoch:  660    Loss: 0.4128499925136566\n",
      "Epoch:  661    Loss: 0.41281241178512573\n",
      "Epoch:  662    Loss: 0.4127747416496277\n",
      "Epoch:  663    Loss: 0.41273704171180725\n",
      "Epoch:  664    Loss: 0.41269925236701965\n",
      "Epoch:  665    Loss: 0.41266143321990967\n",
      "Epoch:  666    Loss: 0.4126235842704773\n",
      "Epoch:  667    Loss: 0.412585586309433\n",
      "Epoch:  668    Loss: 0.4125475585460663\n",
      "Epoch:  669    Loss: 0.4125095307826996\n",
      "Epoch:  670    Loss: 0.41247132420539856\n",
      "Epoch:  671    Loss: 0.41243305802345276\n",
      "Epoch:  672    Loss: 0.4123947322368622\n",
      "Epoch:  673    Loss: 0.41235631704330444\n",
      "Epoch:  674    Loss: 0.41231781244277954\n",
      "Epoch:  675    Loss: 0.4122792184352875\n",
      "Epoch:  676    Loss: 0.41224053502082825\n",
      "Epoch:  677    Loss: 0.4122017025947571\n",
      "Epoch:  678    Loss: 0.41216281056404114\n",
      "Epoch:  679    Loss: 0.4121238589286804\n",
      "Epoch:  680    Loss: 0.4120847284793854\n",
      "Epoch:  681    Loss: 0.41204556822776794\n",
      "Epoch:  682    Loss: 0.4120061993598938\n",
      "Epoch:  683    Loss: 0.41196680068969727\n",
      "Epoch:  684    Loss: 0.4119272828102112\n",
      "Epoch:  685    Loss: 0.4118875563144684\n",
      "Epoch:  686    Loss: 0.4118477702140808\n",
      "Epoch:  687    Loss: 0.4118078947067261\n",
      "Epoch:  688    Loss: 0.4117678999900818\n",
      "Epoch:  689    Loss: 0.4117277264595032\n",
      "Epoch:  690    Loss: 0.4116874635219574\n",
      "Epoch:  691    Loss: 0.4116469919681549\n",
      "Epoch:  692    Loss: 0.41160646080970764\n",
      "Epoch:  693    Loss: 0.41156578063964844\n",
      "Epoch:  694    Loss: 0.41152501106262207\n",
      "Epoch:  695    Loss: 0.4114840626716614\n",
      "Epoch:  696    Loss: 0.41144293546676636\n",
      "Epoch:  697    Loss: 0.4114017188549042\n",
      "Epoch:  698    Loss: 0.41136038303375244\n",
      "Epoch:  699    Loss: 0.411318838596344\n",
      "Epoch:  700    Loss: 0.41127723455429077\n",
      "Epoch:  701    Loss: 0.41123542189598083\n",
      "Epoch:  702    Loss: 0.41119349002838135\n",
      "Epoch:  703    Loss: 0.4111514687538147\n",
      "Epoch:  704    Loss: 0.41110920906066895\n",
      "Epoch:  705    Loss: 0.41106685996055603\n",
      "Epoch:  706    Loss: 0.4110243618488312\n",
      "Epoch:  707    Loss: 0.4109817147254944\n",
      "Epoch:  708    Loss: 0.41093891859054565\n",
      "Epoch:  709    Loss: 0.4108959138393402\n",
      "Epoch:  710    Loss: 0.4108528792858124\n",
      "Epoch:  711    Loss: 0.41080957651138306\n",
      "Epoch:  712    Loss: 0.41076621413230896\n",
      "Epoch:  713    Loss: 0.4107227027416229\n",
      "Epoch:  714    Loss: 0.4106789827346802\n",
      "Epoch:  715    Loss: 0.41063520312309265\n",
      "Epoch:  716    Loss: 0.410591185092926\n",
      "Epoch:  717    Loss: 0.41054707765579224\n",
      "Epoch:  718    Loss: 0.4105027914047241\n",
      "Epoch:  719    Loss: 0.41045835614204407\n",
      "Epoch:  720    Loss: 0.41041383147239685\n",
      "Epoch:  721    Loss: 0.4103691577911377\n",
      "Epoch:  722    Loss: 0.4103243052959442\n",
      "Epoch:  723    Loss: 0.4102793335914612\n",
      "Epoch:  724    Loss: 0.4102342128753662\n",
      "Epoch:  725    Loss: 0.4101889729499817\n",
      "Epoch:  726    Loss: 0.41014358401298523\n",
      "Epoch:  727    Loss: 0.41009801626205444\n",
      "Epoch:  728    Loss: 0.4100523889064789\n",
      "Epoch:  729    Loss: 0.4100065529346466\n",
      "Epoch:  730    Loss: 0.4099605977535248\n",
      "Epoch:  731    Loss: 0.4099145233631134\n",
      "Epoch:  732    Loss: 0.40986835956573486\n",
      "Epoch:  733    Loss: 0.4098220467567444\n",
      "Epoch:  734    Loss: 0.4097755551338196\n",
      "Epoch:  735    Loss: 0.40972900390625\n",
      "Epoch:  736    Loss: 0.40968233346939087\n",
      "Epoch:  737    Loss: 0.4096354842185974\n",
      "Epoch:  738    Loss: 0.4095885455608368\n",
      "Epoch:  739    Loss: 0.4095415472984314\n",
      "Epoch:  740    Loss: 0.4094943404197693\n",
      "Epoch:  741    Loss: 0.4094470739364624\n",
      "Epoch:  742    Loss: 0.4093996584415436\n",
      "Epoch:  743    Loss: 0.4093521237373352\n",
      "Epoch:  744    Loss: 0.40930455923080444\n",
      "Epoch:  745    Loss: 0.40925684571266174\n",
      "Epoch:  746    Loss: 0.4092089533805847\n",
      "Epoch:  747    Loss: 0.40916112065315247\n",
      "Epoch:  748    Loss: 0.4091131091117859\n",
      "Epoch:  749    Loss: 0.40906500816345215\n",
      "Epoch:  750    Loss: 0.40901684761047363\n",
      "Epoch:  751    Loss: 0.40896856784820557\n",
      "Epoch:  752    Loss: 0.40892019867897034\n",
      "Epoch:  753    Loss: 0.40887171030044556\n",
      "Epoch:  754    Loss: 0.408823162317276\n",
      "Epoch:  755    Loss: 0.40877455472946167\n",
      "Epoch:  756    Loss: 0.4087258577346802\n",
      "Epoch:  757    Loss: 0.4086771011352539\n",
      "Epoch:  758    Loss: 0.40862828493118286\n",
      "Epoch:  759    Loss: 0.40857937932014465\n",
      "Epoch:  760    Loss: 0.40853041410446167\n",
      "Epoch:  761    Loss: 0.4084813892841339\n",
      "Epoch:  762    Loss: 0.4084323048591614\n",
      "Epoch:  763    Loss: 0.40838316082954407\n",
      "Epoch:  764    Loss: 0.408333957195282\n",
      "Epoch:  765    Loss: 0.4082847535610199\n",
      "Epoch:  766    Loss: 0.40823546051979065\n",
      "Epoch:  767    Loss: 0.4081861674785614\n",
      "Epoch:  768    Loss: 0.4081367552280426\n",
      "Epoch:  769    Loss: 0.4080873727798462\n",
      "Epoch:  770    Loss: 0.4080379605293274\n",
      "Epoch:  771    Loss: 0.40798842906951904\n",
      "Epoch:  772    Loss: 0.40793898701667786\n",
      "Epoch:  773    Loss: 0.4078894257545471\n",
      "Epoch:  774    Loss: 0.40783992409706116\n",
      "Epoch:  775    Loss: 0.4077903628349304\n",
      "Epoch:  776    Loss: 0.40774083137512207\n",
      "Epoch:  777    Loss: 0.40769121050834656\n",
      "Epoch:  778    Loss: 0.40764161944389343\n",
      "Epoch:  779    Loss: 0.4075920581817627\n",
      "Epoch:  780    Loss: 0.4075424373149872\n",
      "Epoch:  781    Loss: 0.40749281644821167\n",
      "Epoch:  782    Loss: 0.40744325518608093\n",
      "Epoch:  783    Loss: 0.4073936343193054\n",
      "Epoch:  784    Loss: 0.40734410285949707\n",
      "Epoch:  785    Loss: 0.40729451179504395\n",
      "Epoch:  786    Loss: 0.4072449803352356\n",
      "Epoch:  787    Loss: 0.40719547867774963\n",
      "Epoch:  788    Loss: 0.40714597702026367\n",
      "Epoch:  789    Loss: 0.4070965051651001\n",
      "Epoch:  790    Loss: 0.4070470631122589\n",
      "Epoch:  791    Loss: 0.4069976210594177\n",
      "Epoch:  792    Loss: 0.4069482982158661\n",
      "Epoch:  793    Loss: 0.40689894556999207\n",
      "Epoch:  794    Loss: 0.4068496525287628\n",
      "Epoch:  795    Loss: 0.40680038928985596\n",
      "Epoch:  796    Loss: 0.40675118565559387\n",
      "Epoch:  797    Loss: 0.40670204162597656\n",
      "Epoch:  798    Loss: 0.4066529870033264\n",
      "Epoch:  799    Loss: 0.40660396218299866\n",
      "Epoch:  800    Loss: 0.4065549373626709\n",
      "Epoch:  801    Loss: 0.4065060317516327\n",
      "Epoch:  802    Loss: 0.40645715594291687\n",
      "Epoch:  803    Loss: 0.4064083397388458\n",
      "Epoch:  804    Loss: 0.40635964274406433\n",
      "Epoch:  805    Loss: 0.40631091594696045\n",
      "Epoch:  806    Loss: 0.4062623977661133\n",
      "Epoch:  807    Loss: 0.4062138795852661\n",
      "Epoch:  808    Loss: 0.40616539120674133\n",
      "Epoch:  809    Loss: 0.4061170816421509\n",
      "Epoch:  810    Loss: 0.4060687720775604\n",
      "Epoch:  811    Loss: 0.4060206115245819\n",
      "Epoch:  812    Loss: 0.4059724807739258\n",
      "Epoch:  813    Loss: 0.4059244692325592\n",
      "Epoch:  814    Loss: 0.4058765172958374\n",
      "Epoch:  815    Loss: 0.4058286249637604\n",
      "Epoch:  816    Loss: 0.4057808220386505\n",
      "Epoch:  817    Loss: 0.4057331681251526\n",
      "Epoch:  818    Loss: 0.4056856036186218\n",
      "Epoch:  819    Loss: 0.40563806891441345\n",
      "Epoch:  820    Loss: 0.4055907130241394\n",
      "Epoch:  821    Loss: 0.40554341673851013\n",
      "Epoch:  822    Loss: 0.40549618005752563\n",
      "Epoch:  823    Loss: 0.4054490625858307\n",
      "Epoch:  824    Loss: 0.4054020643234253\n",
      "Epoch:  825    Loss: 0.4053551256656647\n",
      "Epoch:  826    Loss: 0.4053083062171936\n",
      "Epoch:  827    Loss: 0.4052616059780121\n",
      "Epoch:  828    Loss: 0.40521496534347534\n",
      "Epoch:  829    Loss: 0.4051685035228729\n",
      "Epoch:  830    Loss: 0.4051220417022705\n",
      "Epoch:  831    Loss: 0.40507569909095764\n",
      "Epoch:  832    Loss: 0.4050295054912567\n",
      "Epoch:  833    Loss: 0.40498337149620056\n",
      "Epoch:  834    Loss: 0.40493741631507874\n",
      "Epoch:  835    Loss: 0.4048914313316345\n",
      "Epoch:  836    Loss: 0.40484562516212463\n",
      "Epoch:  837    Loss: 0.4047999083995819\n",
      "Epoch:  838    Loss: 0.40475428104400635\n",
      "Epoch:  839    Loss: 0.40470877289772034\n",
      "Epoch:  840    Loss: 0.4046632945537567\n",
      "Epoch:  841    Loss: 0.4046180248260498\n",
      "Epoch:  842    Loss: 0.4045727849006653\n",
      "Epoch:  843    Loss: 0.40452757477760315\n",
      "Epoch:  844    Loss: 0.4044826030731201\n",
      "Epoch:  845    Loss: 0.4044376015663147\n",
      "Epoch:  846    Loss: 0.404392808675766\n",
      "Epoch:  847    Loss: 0.4043480455875397\n",
      "Epoch:  848    Loss: 0.4043034017086029\n",
      "Epoch:  849    Loss: 0.4042588472366333\n",
      "Epoch:  850    Loss: 0.40421438217163086\n",
      "Epoch:  851    Loss: 0.4041699767112732\n",
      "Epoch:  852    Loss: 0.4041256606578827\n",
      "Epoch:  853    Loss: 0.4040815234184265\n",
      "Epoch:  854    Loss: 0.40403738617897034\n",
      "Epoch:  855    Loss: 0.4039933979511261\n",
      "Epoch:  856    Loss: 0.40394943952560425\n",
      "Epoch:  857    Loss: 0.40390557050704956\n",
      "Epoch:  858    Loss: 0.40386179089546204\n",
      "Epoch:  859    Loss: 0.4038180708885193\n",
      "Epoch:  860    Loss: 0.4037744998931885\n",
      "Epoch:  861    Loss: 0.40373095870018005\n",
      "Epoch:  862    Loss: 0.40368756651878357\n",
      "Epoch:  863    Loss: 0.4036441445350647\n",
      "Epoch:  864    Loss: 0.40360087156295776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  865    Loss: 0.4035576581954956\n",
      "Epoch:  866    Loss: 0.4035145044326782\n",
      "Epoch:  867    Loss: 0.403471440076828\n",
      "Epoch:  868    Loss: 0.40342843532562256\n",
      "Epoch:  869    Loss: 0.4033854901790619\n",
      "Epoch:  870    Loss: 0.40334269404411316\n",
      "Epoch:  871    Loss: 0.4032999277114868\n",
      "Epoch:  872    Loss: 0.4032571315765381\n",
      "Epoch:  873    Loss: 0.4032145142555237\n",
      "Epoch:  874    Loss: 0.4031718969345093\n",
      "Epoch:  875    Loss: 0.40312936902046204\n",
      "Epoch:  876    Loss: 0.40308696031570435\n",
      "Epoch:  877    Loss: 0.40304452180862427\n",
      "Epoch:  878    Loss: 0.40300220251083374\n",
      "Epoch:  879    Loss: 0.402959942817688\n",
      "Epoch:  880    Loss: 0.40291768312454224\n",
      "Epoch:  881    Loss: 0.40287554264068604\n",
      "Epoch:  882    Loss: 0.40283340215682983\n",
      "Epoch:  883    Loss: 0.4027913510799408\n",
      "Epoch:  884    Loss: 0.40274932980537415\n",
      "Epoch:  885    Loss: 0.4027073383331299\n",
      "Epoch:  886    Loss: 0.40266549587249756\n",
      "Epoch:  887    Loss: 0.40262359380722046\n",
      "Epoch:  888    Loss: 0.4025818109512329\n",
      "Epoch:  889    Loss: 0.40254005789756775\n",
      "Epoch:  890    Loss: 0.402498334646225\n",
      "Epoch:  891    Loss: 0.4024566411972046\n",
      "Epoch:  892    Loss: 0.40241503715515137\n",
      "Epoch:  893    Loss: 0.40237343311309814\n",
      "Epoch:  894    Loss: 0.4023319184780121\n",
      "Epoch:  895    Loss: 0.4022904336452484\n",
      "Epoch:  896    Loss: 0.40224897861480713\n",
      "Epoch:  897    Loss: 0.40220755338668823\n",
      "Epoch:  898    Loss: 0.4021661877632141\n",
      "Epoch:  899    Loss: 0.40212482213974\n",
      "Epoch:  900    Loss: 0.40208351612091064\n",
      "Epoch:  901    Loss: 0.4020422697067261\n",
      "Epoch:  902    Loss: 0.4020010232925415\n",
      "Epoch:  903    Loss: 0.4019598364830017\n",
      "Epoch:  904    Loss: 0.4019186496734619\n",
      "Epoch:  905    Loss: 0.4018775522708893\n",
      "Epoch:  906    Loss: 0.40183642506599426\n",
      "Epoch:  907    Loss: 0.40179532766342163\n",
      "Epoch:  908    Loss: 0.40175431966781616\n",
      "Epoch:  909    Loss: 0.4017132818698883\n",
      "Epoch:  910    Loss: 0.4016723334789276\n",
      "Epoch:  911    Loss: 0.40163135528564453\n",
      "Epoch:  912    Loss: 0.40159040689468384\n",
      "Epoch:  913    Loss: 0.40154945850372314\n",
      "Epoch:  914    Loss: 0.401508629322052\n",
      "Epoch:  915    Loss: 0.4014677405357361\n",
      "Epoch:  916    Loss: 0.40142688155174255\n",
      "Epoch:  917    Loss: 0.40138617157936096\n",
      "Epoch:  918    Loss: 0.40134531259536743\n",
      "Epoch:  919    Loss: 0.40130457282066345\n",
      "Epoch:  920    Loss: 0.4012637734413147\n",
      "Epoch:  921    Loss: 0.4012230932712555\n",
      "Epoch:  922    Loss: 0.4011823832988739\n",
      "Epoch:  923    Loss: 0.4011416733264923\n",
      "Epoch:  924    Loss: 0.4011010229587555\n",
      "Epoch:  925    Loss: 0.4010603129863739\n",
      "Epoch:  926    Loss: 0.40101975202560425\n",
      "Epoch:  927    Loss: 0.4009791910648346\n",
      "Epoch:  928    Loss: 0.4009385406970978\n",
      "Epoch:  929    Loss: 0.40089794993400574\n",
      "Epoch:  930    Loss: 0.4008573591709137\n",
      "Epoch:  931    Loss: 0.40081682801246643\n",
      "Epoch:  932    Loss: 0.4007762670516968\n",
      "Epoch:  933    Loss: 0.4007357358932495\n",
      "Epoch:  934    Loss: 0.40069523453712463\n",
      "Epoch:  935    Loss: 0.40065470337867737\n",
      "Epoch:  936    Loss: 0.40061426162719727\n",
      "Epoch:  937    Loss: 0.4005737602710724\n",
      "Epoch:  938    Loss: 0.4005332887172699\n",
      "Epoch:  939    Loss: 0.4004928469657898\n",
      "Epoch:  940    Loss: 0.4004523754119873\n",
      "Epoch:  941    Loss: 0.4004119038581848\n",
      "Epoch:  942    Loss: 0.4003714919090271\n",
      "Epoch:  943    Loss: 0.400331050157547\n",
      "Epoch:  944    Loss: 0.4002906382083893\n",
      "Epoch:  945    Loss: 0.40025022625923157\n",
      "Epoch:  946    Loss: 0.40020984411239624\n",
      "Epoch:  947    Loss: 0.40016940236091614\n",
      "Epoch:  948    Loss: 0.4001290202140808\n",
      "Epoch:  949    Loss: 0.4000885784626007\n",
      "Epoch:  950    Loss: 0.40004825592041016\n",
      "Epoch:  951    Loss: 0.40000781416893005\n",
      "Epoch:  952    Loss: 0.3999674916267395\n",
      "Epoch:  953    Loss: 0.3999271094799042\n",
      "Epoch:  954    Loss: 0.39988669753074646\n",
      "Epoch:  955    Loss: 0.3998463749885559\n",
      "Epoch:  956    Loss: 0.3998059630393982\n",
      "Epoch:  957    Loss: 0.39976561069488525\n",
      "Epoch:  958    Loss: 0.39972519874572754\n",
      "Epoch:  959    Loss: 0.3996847867965698\n",
      "Epoch:  960    Loss: 0.3996444046497345\n",
      "Epoch:  961    Loss: 0.39960405230522156\n",
      "Epoch:  962    Loss: 0.39956367015838623\n",
      "Epoch:  963    Loss: 0.3995232582092285\n",
      "Epoch:  964    Loss: 0.3994828462600708\n",
      "Epoch:  965    Loss: 0.3994424343109131\n",
      "Epoch:  966    Loss: 0.39940205216407776\n",
      "Epoch:  967    Loss: 0.39936167001724243\n",
      "Epoch:  968    Loss: 0.39932116866111755\n",
      "Epoch:  969    Loss: 0.3992807865142822\n",
      "Epoch:  970    Loss: 0.39924031496047974\n",
      "Epoch:  971    Loss: 0.39919984340667725\n",
      "Epoch:  972    Loss: 0.39915940165519714\n",
      "Epoch:  973    Loss: 0.39911890029907227\n",
      "Epoch:  974    Loss: 0.399078369140625\n",
      "Epoch:  975    Loss: 0.3990378677845001\n",
      "Epoch:  976    Loss: 0.39899739623069763\n",
      "Epoch:  977    Loss: 0.39895686507225037\n",
      "Epoch:  978    Loss: 0.39891624450683594\n",
      "Epoch:  979    Loss: 0.39887571334838867\n",
      "Epoch:  980    Loss: 0.39883512258529663\n",
      "Epoch:  981    Loss: 0.3987944722175598\n",
      "Epoch:  982    Loss: 0.398753821849823\n",
      "Epoch:  983    Loss: 0.3987131714820862\n",
      "Epoch:  984    Loss: 0.3986724615097046\n",
      "Epoch:  985    Loss: 0.3986317813396454\n",
      "Epoch:  986    Loss: 0.398591011762619\n",
      "Epoch:  987    Loss: 0.39855024218559265\n",
      "Epoch:  988    Loss: 0.3985094428062439\n",
      "Epoch:  989    Loss: 0.39846861362457275\n",
      "Epoch:  990    Loss: 0.39842772483825684\n",
      "Epoch:  991    Loss: 0.3983868360519409\n",
      "Epoch:  992    Loss: 0.398345947265625\n",
      "Epoch:  993    Loss: 0.39830493927001953\n",
      "Epoch:  994    Loss: 0.39826393127441406\n",
      "Epoch:  995    Loss: 0.3982228636741638\n",
      "Epoch:  996    Loss: 0.3981817662715912\n",
      "Epoch:  997    Loss: 0.3981406092643738\n",
      "Epoch:  998    Loss: 0.3980993926525116\n",
      "Epoch:  999    Loss: 0.3980581760406494\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "xy = np.loadtxt(r'D:\\code\\python\\tmp\\hgd_DeepLearning\\doc\\diabetes.csv.gz', delimiter=',', dtype=np.float32)\n",
    "x_data = torch.from_numpy(xy[:, :-1])\n",
    "y_data = torch.from_numpy(xy[:, [-1]])\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(8, 6)\n",
    "        self.linear2 = torch.nn.Linear(6, 4)\n",
    "        self.linear3 = torch.nn.Linear(4, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.linear1(x))\n",
    "        x = self.sigmoid(self.linear2(x))\n",
    "        x = self.sigmoid(self.linear3(x))\n",
    "        return x\n",
    "        \n",
    "model = Model()\n",
    "criterion = torch.nn.BCELoss(size_average=True)\n",
    "# Adam收敛的更快一些\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "for epoch in range(1000):\n",
    "    # Forward\n",
    "    y_pred = model(x_data)\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    print(\"Epoch: \", epoch, \"   Loss:\", loss.item())\n",
    "\n",
    "    # Backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Update\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset # 抽象类 无法实例化\n",
    "from torch.utils.data import DataLoader # 可以实例化\n",
    "\n",
    "class DiabetesDataset(Dataset):\n",
    "    def __init__(self,filepath):\n",
    "        xy = np.loadtxt(filepath, delimiter=',', dtype=np.float32)\n",
    "        self.len = xy.shape[0]\n",
    "        self.x_data = torch.from_numpy(xy[:, :-1])\n",
    "        self.y_data = torch.from_numpy(xy[:, [-1]])\n",
    "\n",
    "    # 实现两个魔法方法\n",
    "    # 支持index\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "        \n",
    "    # len() \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = DiabetesDataset(r'D:\\code\\python\\tmp\\hgd_DeepLearning\\doc\\diabetes.csv.gz')\n",
    "# num_workers = 2 会存在问题\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda\\envs\\img\\lib\\site-packages\\torch\\nn\\_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(8, 6)\n",
    "        self.linear2 = torch.nn.Linear(6, 4)\n",
    "        self.linear3 = torch.nn.Linear(4, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.linear1(x))\n",
    "        x = self.sigmoid(self.linear2(x))\n",
    "        x = self.sigmoid(self.linear3(x))\n",
    "        return x\n",
    "        \n",
    "model = Model()\n",
    "criterion = torch.nn.BCELoss(size_average=True)\n",
    "# Adam收敛的更快一些\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0     i:  0     Loss:  0.6955980062484741\n",
      "Epoch:  0     i:  1     Loss:  0.6859703063964844\n",
      "Epoch:  0     i:  2     Loss:  0.6792274713516235\n",
      "Epoch:  0     i:  3     Loss:  0.6847602128982544\n",
      "Epoch:  0     i:  4     Loss:  0.6780593395233154\n",
      "Epoch:  0     i:  5     Loss:  0.6645298004150391\n",
      "Epoch:  0     i:  6     Loss:  0.6715691089630127\n",
      "Epoch:  0     i:  7     Loss:  0.6624765396118164\n",
      "Epoch:  0     i:  8     Loss:  0.6766192317008972\n",
      "Epoch:  0     i:  9     Loss:  0.6264051795005798\n",
      "Epoch:  0     i:  10     Loss:  0.6427335739135742\n",
      "Epoch:  0     i:  11     Loss:  0.6515899896621704\n",
      "Epoch:  0     i:  12     Loss:  0.6361820101737976\n",
      "Epoch:  0     i:  13     Loss:  0.6478661894798279\n",
      "Epoch:  0     i:  14     Loss:  0.748672366142273\n",
      "Epoch:  0     i:  15     Loss:  0.6134616136550903\n",
      "Epoch:  0     i:  16     Loss:  0.6751768589019775\n",
      "Epoch:  0     i:  17     Loss:  0.6584152579307556\n",
      "Epoch:  0     i:  18     Loss:  0.6252596378326416\n",
      "Epoch:  0     i:  19     Loss:  0.5884789228439331\n",
      "Epoch:  0     i:  20     Loss:  0.642044723033905\n",
      "Epoch:  0     i:  21     Loss:  0.6601534485816956\n",
      "Epoch:  0     i:  22     Loss:  0.679514467716217\n",
      "Epoch:  0     i:  23     Loss:  0.5879243016242981\n",
      "Epoch:  1     i:  0     Loss:  0.6601347923278809\n",
      "Epoch:  1     i:  1     Loss:  0.6194335222244263\n",
      "Epoch:  1     i:  2     Loss:  0.6398895978927612\n",
      "Epoch:  1     i:  3     Loss:  0.5756750702857971\n",
      "Epoch:  1     i:  4     Loss:  0.5533531904220581\n",
      "Epoch:  1     i:  5     Loss:  0.5954041481018066\n",
      "Epoch:  1     i:  6     Loss:  0.573194146156311\n",
      "Epoch:  1     i:  7     Loss:  0.5931628942489624\n",
      "Epoch:  1     i:  8     Loss:  0.6663863658905029\n",
      "Epoch:  1     i:  9     Loss:  0.6397649049758911\n",
      "Epoch:  1     i:  10     Loss:  0.6873654723167419\n",
      "Epoch:  1     i:  11     Loss:  0.7211304903030396\n",
      "Epoch:  1     i:  12     Loss:  0.7389369606971741\n",
      "Epoch:  1     i:  13     Loss:  0.6922890543937683\n",
      "Epoch:  1     i:  14     Loss:  0.6883206963539124\n",
      "Epoch:  1     i:  15     Loss:  0.714035153388977\n",
      "Epoch:  1     i:  16     Loss:  0.6635078191757202\n",
      "Epoch:  1     i:  17     Loss:  0.6187621355056763\n",
      "Epoch:  1     i:  18     Loss:  0.6137772798538208\n",
      "Epoch:  1     i:  19     Loss:  0.5274155139923096\n",
      "Epoch:  1     i:  20     Loss:  0.7697076797485352\n",
      "Epoch:  1     i:  21     Loss:  0.6215602159500122\n",
      "Epoch:  1     i:  22     Loss:  0.6197620034217834\n",
      "Epoch:  1     i:  23     Loss:  0.6685447692871094\n",
      "Epoch:  2     i:  0     Loss:  0.6605694890022278\n",
      "Epoch:  2     i:  1     Loss:  0.6186665892601013\n",
      "Epoch:  2     i:  2     Loss:  0.6742067337036133\n",
      "Epoch:  2     i:  3     Loss:  0.7174628973007202\n",
      "Epoch:  2     i:  4     Loss:  0.7333556413650513\n",
      "Epoch:  2     i:  5     Loss:  0.5626814961433411\n",
      "Epoch:  2     i:  6     Loss:  0.7640998363494873\n",
      "Epoch:  2     i:  7     Loss:  0.614827036857605\n",
      "Epoch:  2     i:  8     Loss:  0.6364251375198364\n",
      "Epoch:  2     i:  9     Loss:  0.7067369222640991\n",
      "Epoch:  2     i:  10     Loss:  0.618546187877655\n",
      "Epoch:  2     i:  11     Loss:  0.6027693748474121\n",
      "Epoch:  2     i:  12     Loss:  0.6521604061126709\n",
      "Epoch:  2     i:  13     Loss:  0.5896115899085999\n",
      "Epoch:  2     i:  14     Loss:  0.6507387161254883\n",
      "Epoch:  2     i:  15     Loss:  0.6195343136787415\n",
      "Epoch:  2     i:  16     Loss:  0.6678679585456848\n",
      "Epoch:  2     i:  17     Loss:  0.6740242838859558\n",
      "Epoch:  2     i:  18     Loss:  0.55910724401474\n",
      "Epoch:  2     i:  19     Loss:  0.6051545739173889\n",
      "Epoch:  2     i:  20     Loss:  0.6215692758560181\n",
      "Epoch:  2     i:  21     Loss:  0.603564977645874\n",
      "Epoch:  2     i:  22     Loss:  0.6355013847351074\n",
      "Epoch:  2     i:  23     Loss:  0.5372849702835083\n",
      "Epoch:  3     i:  0     Loss:  0.4831317663192749\n",
      "Epoch:  3     i:  1     Loss:  0.6525291204452515\n",
      "Epoch:  3     i:  2     Loss:  0.6706905961036682\n",
      "Epoch:  3     i:  3     Loss:  0.6491811871528625\n",
      "Epoch:  3     i:  4     Loss:  0.5480455756187439\n",
      "Epoch:  3     i:  5     Loss:  0.5449561476707458\n",
      "Epoch:  3     i:  6     Loss:  0.6560592651367188\n",
      "Epoch:  3     i:  7     Loss:  0.6578206419944763\n",
      "Epoch:  3     i:  8     Loss:  0.6122151613235474\n",
      "Epoch:  3     i:  9     Loss:  0.5366363525390625\n",
      "Epoch:  3     i:  10     Loss:  0.6793954372406006\n",
      "Epoch:  3     i:  11     Loss:  0.6782616376876831\n",
      "Epoch:  3     i:  12     Loss:  0.6079548597335815\n",
      "Epoch:  3     i:  13     Loss:  0.6080396175384521\n",
      "Epoch:  3     i:  14     Loss:  0.6881909370422363\n",
      "Epoch:  3     i:  15     Loss:  0.6131280064582825\n",
      "Epoch:  3     i:  16     Loss:  0.63189697265625\n",
      "Epoch:  3     i:  17     Loss:  0.7756681442260742\n",
      "Epoch:  3     i:  18     Loss:  0.7510311603546143\n",
      "Epoch:  3     i:  19     Loss:  0.6732607483863831\n",
      "Epoch:  3     i:  20     Loss:  0.6533817648887634\n",
      "Epoch:  3     i:  21     Loss:  0.6329079866409302\n",
      "Epoch:  3     i:  22     Loss:  0.6650663018226624\n",
      "Epoch:  3     i:  23     Loss:  0.5684059262275696\n",
      "Epoch:  4     i:  0     Loss:  0.6679550409317017\n",
      "Epoch:  4     i:  1     Loss:  0.6094029545783997\n",
      "Epoch:  4     i:  2     Loss:  0.6575774550437927\n",
      "Epoch:  4     i:  3     Loss:  0.6688540577888489\n",
      "Epoch:  4     i:  4     Loss:  0.6919854283332825\n",
      "Epoch:  4     i:  5     Loss:  0.6204509139060974\n",
      "Epoch:  4     i:  6     Loss:  0.6080353260040283\n",
      "Epoch:  4     i:  7     Loss:  0.6397730708122253\n",
      "Epoch:  4     i:  8     Loss:  0.6204240918159485\n",
      "Epoch:  4     i:  9     Loss:  0.6014438271522522\n",
      "Epoch:  4     i:  10     Loss:  0.6734566688537598\n",
      "Epoch:  4     i:  11     Loss:  0.6031999588012695\n",
      "Epoch:  4     i:  12     Loss:  0.543891191482544\n",
      "Epoch:  4     i:  13     Loss:  0.6690688729286194\n",
      "Epoch:  4     i:  14     Loss:  0.6381781101226807\n",
      "Epoch:  4     i:  15     Loss:  0.6778911352157593\n",
      "Epoch:  4     i:  16     Loss:  0.6053986549377441\n",
      "Epoch:  4     i:  17     Loss:  0.6160503625869751\n",
      "Epoch:  4     i:  18     Loss:  0.5945034623146057\n",
      "Epoch:  4     i:  19     Loss:  0.5895185470581055\n",
      "Epoch:  4     i:  20     Loss:  0.6128342151641846\n",
      "Epoch:  4     i:  21     Loss:  0.625739574432373\n",
      "Epoch:  4     i:  22     Loss:  0.5445026755332947\n",
      "Epoch:  4     i:  23     Loss:  0.6010838747024536\n",
      "Epoch:  5     i:  0     Loss:  0.6062391400337219\n",
      "Epoch:  5     i:  1     Loss:  0.6168037056922913\n",
      "Epoch:  5     i:  2     Loss:  0.6466083526611328\n",
      "Epoch:  5     i:  3     Loss:  0.631892204284668\n",
      "Epoch:  5     i:  4     Loss:  0.6028162240982056\n",
      "Epoch:  5     i:  5     Loss:  0.6608256697654724\n",
      "Epoch:  5     i:  6     Loss:  0.6121000051498413\n",
      "Epoch:  5     i:  7     Loss:  0.5344985723495483\n",
      "Epoch:  5     i:  8     Loss:  0.6414749026298523\n",
      "Epoch:  5     i:  9     Loss:  0.6958150863647461\n",
      "Epoch:  5     i:  10     Loss:  0.6064656972885132\n",
      "Epoch:  5     i:  11     Loss:  0.5536758303642273\n",
      "Epoch:  5     i:  12     Loss:  0.6585840582847595\n",
      "Epoch:  5     i:  13     Loss:  0.6281853318214417\n",
      "Epoch:  5     i:  14     Loss:  0.644850492477417\n",
      "Epoch:  5     i:  15     Loss:  0.577873945236206\n",
      "Epoch:  5     i:  16     Loss:  0.6834582686424255\n",
      "Epoch:  5     i:  17     Loss:  0.5487830638885498\n",
      "Epoch:  5     i:  18     Loss:  0.5859659314155579\n",
      "Epoch:  5     i:  19     Loss:  0.6214368343353271\n",
      "Epoch:  5     i:  20     Loss:  0.5571905374526978\n",
      "Epoch:  5     i:  21     Loss:  0.5567785501480103\n",
      "Epoch:  5     i:  22     Loss:  0.5264427065849304\n",
      "Epoch:  5     i:  23     Loss:  0.5811406373977661\n",
      "Epoch:  6     i:  0     Loss:  0.5721116662025452\n",
      "Epoch:  6     i:  1     Loss:  0.5259611010551453\n",
      "Epoch:  6     i:  2     Loss:  0.6898043155670166\n",
      "Epoch:  6     i:  3     Loss:  0.600421667098999\n",
      "Epoch:  6     i:  4     Loss:  0.6409250497817993\n",
      "Epoch:  6     i:  5     Loss:  0.5814456939697266\n",
      "Epoch:  6     i:  6     Loss:  0.5898410081863403\n",
      "Epoch:  6     i:  7     Loss:  0.5571649074554443\n",
      "Epoch:  6     i:  8     Loss:  0.6754176616668701\n",
      "Epoch:  6     i:  9     Loss:  0.5403947234153748\n",
      "Epoch:  6     i:  10     Loss:  0.508825957775116\n",
      "Epoch:  6     i:  11     Loss:  0.5270358324050903\n",
      "Epoch:  6     i:  12     Loss:  0.6112755537033081\n",
      "Epoch:  6     i:  13     Loss:  0.597663402557373\n",
      "Epoch:  6     i:  14     Loss:  0.5445022583007812\n",
      "Epoch:  6     i:  15     Loss:  0.5157989263534546\n",
      "Epoch:  6     i:  16     Loss:  0.6599777936935425\n",
      "Epoch:  6     i:  17     Loss:  0.6193692684173584\n",
      "Epoch:  6     i:  18     Loss:  0.6118661165237427\n",
      "Epoch:  6     i:  19     Loss:  0.5694538354873657\n",
      "Epoch:  6     i:  20     Loss:  0.48082706332206726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  6     i:  21     Loss:  0.6062271595001221\n",
      "Epoch:  6     i:  22     Loss:  0.6316049098968506\n",
      "Epoch:  6     i:  23     Loss:  0.5865544676780701\n",
      "Epoch:  7     i:  0     Loss:  0.6133700013160706\n",
      "Epoch:  7     i:  1     Loss:  0.5651460886001587\n",
      "Epoch:  7     i:  2     Loss:  0.5828322768211365\n",
      "Epoch:  7     i:  3     Loss:  0.5400227904319763\n",
      "Epoch:  7     i:  4     Loss:  0.511715292930603\n",
      "Epoch:  7     i:  5     Loss:  0.57471764087677\n",
      "Epoch:  7     i:  6     Loss:  0.5049719214439392\n",
      "Epoch:  7     i:  7     Loss:  0.6068808436393738\n",
      "Epoch:  7     i:  8     Loss:  0.5559129118919373\n",
      "Epoch:  7     i:  9     Loss:  0.5994631052017212\n",
      "Epoch:  7     i:  10     Loss:  0.5837875008583069\n",
      "Epoch:  7     i:  11     Loss:  0.5173128247261047\n",
      "Epoch:  7     i:  12     Loss:  0.6555840373039246\n",
      "Epoch:  7     i:  13     Loss:  0.5261305570602417\n",
      "Epoch:  7     i:  14     Loss:  0.59306401014328\n",
      "Epoch:  7     i:  15     Loss:  0.5713154077529907\n",
      "Epoch:  7     i:  16     Loss:  0.5410550236701965\n",
      "Epoch:  7     i:  17     Loss:  0.47608333826065063\n",
      "Epoch:  7     i:  18     Loss:  0.5368456840515137\n",
      "Epoch:  7     i:  19     Loss:  0.6116538643836975\n",
      "Epoch:  7     i:  20     Loss:  0.4315773546695709\n",
      "Epoch:  7     i:  21     Loss:  0.5507908463478088\n",
      "Epoch:  7     i:  22     Loss:  0.6035981178283691\n",
      "Epoch:  7     i:  23     Loss:  0.47531116008758545\n",
      "Epoch:  8     i:  0     Loss:  0.5454689860343933\n",
      "Epoch:  8     i:  1     Loss:  0.49921178817749023\n",
      "Epoch:  8     i:  2     Loss:  0.48633623123168945\n",
      "Epoch:  8     i:  3     Loss:  0.5297508835792542\n",
      "Epoch:  8     i:  4     Loss:  0.6487376689910889\n",
      "Epoch:  8     i:  5     Loss:  0.5287466645240784\n",
      "Epoch:  8     i:  6     Loss:  0.49852144718170166\n",
      "Epoch:  8     i:  7     Loss:  0.5472629070281982\n",
      "Epoch:  8     i:  8     Loss:  0.5296083688735962\n",
      "Epoch:  8     i:  9     Loss:  0.4804561138153076\n",
      "Epoch:  8     i:  10     Loss:  0.6417133212089539\n",
      "Epoch:  8     i:  11     Loss:  0.5410452485084534\n",
      "Epoch:  8     i:  12     Loss:  0.4010372757911682\n",
      "Epoch:  8     i:  13     Loss:  0.4185201823711395\n",
      "Epoch:  8     i:  14     Loss:  0.6265965700149536\n",
      "Epoch:  8     i:  15     Loss:  0.5709788799285889\n",
      "Epoch:  8     i:  16     Loss:  0.48209142684936523\n",
      "Epoch:  8     i:  17     Loss:  0.549545407295227\n",
      "Epoch:  8     i:  18     Loss:  0.47239062190055847\n",
      "Epoch:  8     i:  19     Loss:  0.6010587215423584\n",
      "Epoch:  8     i:  20     Loss:  0.6095790863037109\n",
      "Epoch:  8     i:  21     Loss:  0.6034771203994751\n",
      "Epoch:  8     i:  22     Loss:  0.47828155755996704\n",
      "Epoch:  8     i:  23     Loss:  0.41677308082580566\n",
      "Epoch:  9     i:  0     Loss:  0.4971832036972046\n",
      "Epoch:  9     i:  1     Loss:  0.5378947257995605\n",
      "Epoch:  9     i:  2     Loss:  0.4450984597206116\n",
      "Epoch:  9     i:  3     Loss:  0.4771161675453186\n",
      "Epoch:  9     i:  4     Loss:  0.4701865017414093\n",
      "Epoch:  9     i:  5     Loss:  0.48691731691360474\n",
      "Epoch:  9     i:  6     Loss:  0.5506909489631653\n",
      "Epoch:  9     i:  7     Loss:  0.5453710556030273\n",
      "Epoch:  9     i:  8     Loss:  0.5021703243255615\n",
      "Epoch:  9     i:  9     Loss:  0.5433946251869202\n",
      "Epoch:  9     i:  10     Loss:  0.5045517086982727\n",
      "Epoch:  9     i:  11     Loss:  0.536962628364563\n",
      "Epoch:  9     i:  12     Loss:  0.5647055506706238\n",
      "Epoch:  9     i:  13     Loss:  0.45723840594291687\n",
      "Epoch:  9     i:  14     Loss:  0.5338935852050781\n",
      "Epoch:  9     i:  15     Loss:  0.6113303899765015\n",
      "Epoch:  9     i:  16     Loss:  0.5509074926376343\n",
      "Epoch:  9     i:  17     Loss:  0.5439284443855286\n",
      "Epoch:  9     i:  18     Loss:  0.4993104338645935\n",
      "Epoch:  9     i:  19     Loss:  0.3454176187515259\n",
      "Epoch:  9     i:  20     Loss:  0.4881877601146698\n",
      "Epoch:  9     i:  21     Loss:  0.44878390431404114\n",
      "Epoch:  9     i:  22     Loss:  0.5329604148864746\n",
      "Epoch:  9     i:  23     Loss:  0.6244713068008423\n",
      "Epoch:  10     i:  0     Loss:  0.4511534571647644\n",
      "Epoch:  10     i:  1     Loss:  0.4640824496746063\n",
      "Epoch:  10     i:  2     Loss:  0.3557376265525818\n",
      "Epoch:  10     i:  3     Loss:  0.4127819538116455\n",
      "Epoch:  10     i:  4     Loss:  0.4208196699619293\n",
      "Epoch:  10     i:  5     Loss:  0.6048258543014526\n",
      "Epoch:  10     i:  6     Loss:  0.41580930352211\n",
      "Epoch:  10     i:  7     Loss:  0.45690590143203735\n",
      "Epoch:  10     i:  8     Loss:  0.5517767071723938\n",
      "Epoch:  10     i:  9     Loss:  0.661617636680603\n",
      "Epoch:  10     i:  10     Loss:  0.47979047894477844\n",
      "Epoch:  10     i:  11     Loss:  0.45904624462127686\n",
      "Epoch:  10     i:  12     Loss:  0.545325756072998\n",
      "Epoch:  10     i:  13     Loss:  0.47379666566848755\n",
      "Epoch:  10     i:  14     Loss:  0.525686502456665\n",
      "Epoch:  10     i:  15     Loss:  0.5692930221557617\n",
      "Epoch:  10     i:  16     Loss:  0.5276160836219788\n",
      "Epoch:  10     i:  17     Loss:  0.4334790110588074\n",
      "Epoch:  10     i:  18     Loss:  0.4633270800113678\n",
      "Epoch:  10     i:  19     Loss:  0.4573906362056732\n",
      "Epoch:  10     i:  20     Loss:  0.483789324760437\n",
      "Epoch:  10     i:  21     Loss:  0.7487683892250061\n",
      "Epoch:  10     i:  22     Loss:  0.4589678645133972\n",
      "Epoch:  10     i:  23     Loss:  0.546572208404541\n",
      "Epoch:  11     i:  0     Loss:  0.4211125373840332\n",
      "Epoch:  11     i:  1     Loss:  0.5231866240501404\n",
      "Epoch:  11     i:  2     Loss:  0.42660316824913025\n",
      "Epoch:  11     i:  3     Loss:  0.47070467472076416\n",
      "Epoch:  11     i:  4     Loss:  0.4621918797492981\n",
      "Epoch:  11     i:  5     Loss:  0.5995224118232727\n",
      "Epoch:  11     i:  6     Loss:  0.4595514237880707\n",
      "Epoch:  11     i:  7     Loss:  0.5615323185920715\n",
      "Epoch:  11     i:  8     Loss:  0.4339192509651184\n",
      "Epoch:  11     i:  9     Loss:  0.45076143741607666\n",
      "Epoch:  11     i:  10     Loss:  0.4540119469165802\n",
      "Epoch:  11     i:  11     Loss:  0.5045779347419739\n",
      "Epoch:  11     i:  12     Loss:  0.5895580053329468\n",
      "Epoch:  11     i:  13     Loss:  0.5925547480583191\n",
      "Epoch:  11     i:  14     Loss:  0.613520622253418\n",
      "Epoch:  11     i:  15     Loss:  0.4003893733024597\n",
      "Epoch:  11     i:  16     Loss:  0.5018865466117859\n",
      "Epoch:  11     i:  17     Loss:  0.5191465616226196\n",
      "Epoch:  11     i:  18     Loss:  0.4919125437736511\n",
      "Epoch:  11     i:  19     Loss:  0.41534432768821716\n",
      "Epoch:  11     i:  20     Loss:  0.4552263617515564\n",
      "Epoch:  11     i:  21     Loss:  0.5335511565208435\n",
      "Epoch:  11     i:  22     Loss:  0.5175074338912964\n",
      "Epoch:  11     i:  23     Loss:  0.33798322081565857\n",
      "Epoch:  12     i:  0     Loss:  0.49929457902908325\n",
      "Epoch:  12     i:  1     Loss:  0.5355935096740723\n",
      "Epoch:  12     i:  2     Loss:  0.3692936599254608\n",
      "Epoch:  12     i:  3     Loss:  0.48717448115348816\n",
      "Epoch:  12     i:  4     Loss:  0.4225695729255676\n",
      "Epoch:  12     i:  5     Loss:  0.38587602972984314\n",
      "Epoch:  12     i:  6     Loss:  0.5119117498397827\n",
      "Epoch:  12     i:  7     Loss:  0.5137601494789124\n",
      "Epoch:  12     i:  8     Loss:  0.6621348261833191\n",
      "Epoch:  12     i:  9     Loss:  0.48412060737609863\n",
      "Epoch:  12     i:  10     Loss:  0.37393975257873535\n",
      "Epoch:  12     i:  11     Loss:  0.45145905017852783\n",
      "Epoch:  12     i:  12     Loss:  0.5082533359527588\n",
      "Epoch:  12     i:  13     Loss:  0.5327985882759094\n",
      "Epoch:  12     i:  14     Loss:  0.47150281071662903\n",
      "Epoch:  12     i:  15     Loss:  0.4602509140968323\n",
      "Epoch:  12     i:  16     Loss:  0.45894452929496765\n",
      "Epoch:  12     i:  17     Loss:  0.5284472703933716\n",
      "Epoch:  12     i:  18     Loss:  0.4446931481361389\n",
      "Epoch:  12     i:  19     Loss:  0.5707398056983948\n",
      "Epoch:  12     i:  20     Loss:  0.4978407025337219\n",
      "Epoch:  12     i:  21     Loss:  0.49886199831962585\n",
      "Epoch:  12     i:  22     Loss:  0.5675560235977173\n",
      "Epoch:  12     i:  23     Loss:  0.4139458239078522\n",
      "Epoch:  13     i:  0     Loss:  0.4602762758731842\n",
      "Epoch:  13     i:  1     Loss:  0.5256990790367126\n",
      "Epoch:  13     i:  2     Loss:  0.6756295561790466\n",
      "Epoch:  13     i:  3     Loss:  0.46720433235168457\n",
      "Epoch:  13     i:  4     Loss:  0.5632376670837402\n",
      "Epoch:  13     i:  5     Loss:  0.4163059890270233\n",
      "Epoch:  13     i:  6     Loss:  0.601772129535675\n",
      "Epoch:  13     i:  7     Loss:  0.5898313522338867\n",
      "Epoch:  13     i:  8     Loss:  0.36108434200286865\n",
      "Epoch:  13     i:  9     Loss:  0.22375112771987915\n",
      "Epoch:  13     i:  10     Loss:  0.4953702688217163\n",
      "Epoch:  13     i:  11     Loss:  0.504721462726593\n",
      "Epoch:  13     i:  12     Loss:  0.40743669867515564\n",
      "Epoch:  13     i:  13     Loss:  0.5373827219009399\n",
      "Epoch:  13     i:  14     Loss:  0.3993614912033081\n",
      "Epoch:  13     i:  15     Loss:  0.6004780530929565\n",
      "Epoch:  13     i:  16     Loss:  0.35357120633125305\n",
      "Epoch:  13     i:  17     Loss:  0.4022452235221863\n",
      "Epoch:  13     i:  18     Loss:  0.54877108335495\n",
      "Epoch:  13     i:  19     Loss:  0.49085429310798645\n",
      "Epoch:  13     i:  20     Loss:  0.43784600496292114\n",
      "Epoch:  13     i:  21     Loss:  0.5888301134109497\n",
      "Epoch:  13     i:  22     Loss:  0.4864005744457245\n",
      "Epoch:  13     i:  23     Loss:  0.4295381009578705\n",
      "Epoch:  14     i:  0     Loss:  0.4204644560813904\n",
      "Epoch:  14     i:  1     Loss:  0.4754234850406647\n",
      "Epoch:  14     i:  2     Loss:  0.5147309303283691\n",
      "Epoch:  14     i:  3     Loss:  0.44651761651039124\n",
      "Epoch:  14     i:  4     Loss:  0.48432815074920654\n",
      "Epoch:  14     i:  5     Loss:  0.4652257561683655\n",
      "Epoch:  14     i:  6     Loss:  0.4114612340927124\n",
      "Epoch:  14     i:  7     Loss:  0.5737991333007812\n",
      "Epoch:  14     i:  8     Loss:  0.4299490451812744\n",
      "Epoch:  14     i:  9     Loss:  0.412833571434021\n",
      "Epoch:  14     i:  10     Loss:  0.6479914784431458\n",
      "Epoch:  14     i:  11     Loss:  0.4742903709411621\n",
      "Epoch:  14     i:  12     Loss:  0.5702303051948547\n",
      "Epoch:  14     i:  13     Loss:  0.6712407469749451\n",
      "Epoch:  14     i:  14     Loss:  0.5339081883430481\n",
      "Epoch:  14     i:  15     Loss:  0.5587494373321533\n",
      "Epoch:  14     i:  16     Loss:  0.42040735483169556\n",
      "Epoch:  14     i:  17     Loss:  0.2865764796733856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  14     i:  18     Loss:  0.46511930227279663\n",
      "Epoch:  14     i:  19     Loss:  0.29452672600746155\n",
      "Epoch:  14     i:  20     Loss:  0.45616966485977173\n",
      "Epoch:  14     i:  21     Loss:  0.5967050790786743\n",
      "Epoch:  14     i:  22     Loss:  0.4703010618686676\n",
      "Epoch:  14     i:  23     Loss:  0.45003294944763184\n",
      "Epoch:  15     i:  0     Loss:  0.4638771414756775\n",
      "Epoch:  15     i:  1     Loss:  0.4980944097042084\n",
      "Epoch:  15     i:  2     Loss:  0.4370790123939514\n",
      "Epoch:  15     i:  3     Loss:  0.49099814891815186\n",
      "Epoch:  15     i:  4     Loss:  0.36506232619285583\n",
      "Epoch:  15     i:  5     Loss:  0.674645721912384\n",
      "Epoch:  15     i:  6     Loss:  0.5374890565872192\n",
      "Epoch:  15     i:  7     Loss:  0.31711918115615845\n",
      "Epoch:  15     i:  8     Loss:  0.3933059871196747\n",
      "Epoch:  15     i:  9     Loss:  0.47240787744522095\n",
      "Epoch:  15     i:  10     Loss:  0.4446042776107788\n",
      "Epoch:  15     i:  11     Loss:  0.5599479079246521\n",
      "Epoch:  15     i:  12     Loss:  0.41760221123695374\n",
      "Epoch:  15     i:  13     Loss:  0.43946677446365356\n",
      "Epoch:  15     i:  14     Loss:  0.4429229199886322\n",
      "Epoch:  15     i:  15     Loss:  0.5971715450286865\n",
      "Epoch:  15     i:  16     Loss:  0.3818311095237732\n",
      "Epoch:  15     i:  17     Loss:  0.5306223630905151\n",
      "Epoch:  15     i:  18     Loss:  0.611047625541687\n",
      "Epoch:  15     i:  19     Loss:  0.49121952056884766\n",
      "Epoch:  15     i:  20     Loss:  0.4205883741378784\n",
      "Epoch:  15     i:  21     Loss:  0.5443413257598877\n",
      "Epoch:  15     i:  22     Loss:  0.5315064787864685\n",
      "Epoch:  15     i:  23     Loss:  0.4891628623008728\n",
      "Epoch:  16     i:  0     Loss:  0.5677297115325928\n",
      "Epoch:  16     i:  1     Loss:  0.5269827842712402\n",
      "Epoch:  16     i:  2     Loss:  0.4909352660179138\n",
      "Epoch:  16     i:  3     Loss:  0.4083159565925598\n",
      "Epoch:  16     i:  4     Loss:  0.32855960726737976\n",
      "Epoch:  16     i:  5     Loss:  0.6225600838661194\n",
      "Epoch:  16     i:  6     Loss:  0.44186511635780334\n",
      "Epoch:  16     i:  7     Loss:  0.6043902039527893\n",
      "Epoch:  16     i:  8     Loss:  0.5888087153434753\n",
      "Epoch:  16     i:  9     Loss:  0.3821095824241638\n",
      "Epoch:  16     i:  10     Loss:  0.3973349928855896\n",
      "Epoch:  16     i:  11     Loss:  0.38248172402381897\n",
      "Epoch:  16     i:  12     Loss:  0.2995207905769348\n",
      "Epoch:  16     i:  13     Loss:  0.42283201217651367\n",
      "Epoch:  16     i:  14     Loss:  0.5620172023773193\n",
      "Epoch:  16     i:  15     Loss:  0.4634788930416107\n",
      "Epoch:  16     i:  16     Loss:  0.46748846769332886\n",
      "Epoch:  16     i:  17     Loss:  0.5188830494880676\n",
      "Epoch:  16     i:  18     Loss:  0.3751208186149597\n",
      "Epoch:  16     i:  19     Loss:  0.44280385971069336\n",
      "Epoch:  16     i:  20     Loss:  0.43144121766090393\n",
      "Epoch:  16     i:  21     Loss:  0.6628036499023438\n",
      "Epoch:  16     i:  22     Loss:  0.6337973475456238\n",
      "Epoch:  16     i:  23     Loss:  0.4752741754055023\n",
      "Epoch:  17     i:  0     Loss:  0.4310867190361023\n",
      "Epoch:  17     i:  1     Loss:  0.4182063341140747\n",
      "Epoch:  17     i:  2     Loss:  0.4829859137535095\n",
      "Epoch:  17     i:  3     Loss:  0.505860447883606\n",
      "Epoch:  17     i:  4     Loss:  0.4871555268764496\n",
      "Epoch:  17     i:  5     Loss:  0.40840017795562744\n",
      "Epoch:  17     i:  6     Loss:  0.4230755567550659\n",
      "Epoch:  17     i:  7     Loss:  0.49103623628616333\n",
      "Epoch:  17     i:  8     Loss:  0.5549243092536926\n",
      "Epoch:  17     i:  9     Loss:  0.46016955375671387\n",
      "Epoch:  17     i:  10     Loss:  0.5709601640701294\n",
      "Epoch:  17     i:  11     Loss:  0.4132172465324402\n",
      "Epoch:  17     i:  12     Loss:  0.5171895623207092\n",
      "Epoch:  17     i:  13     Loss:  0.5501663088798523\n",
      "Epoch:  17     i:  14     Loss:  0.44286060333251953\n",
      "Epoch:  17     i:  15     Loss:  0.5461191534996033\n",
      "Epoch:  17     i:  16     Loss:  0.4831811189651489\n",
      "Epoch:  17     i:  17     Loss:  0.5693514943122864\n",
      "Epoch:  17     i:  18     Loss:  0.3885473310947418\n",
      "Epoch:  17     i:  19     Loss:  0.5771898031234741\n",
      "Epoch:  17     i:  20     Loss:  0.4939171075820923\n",
      "Epoch:  17     i:  21     Loss:  0.41801708936691284\n",
      "Epoch:  17     i:  22     Loss:  0.35477718710899353\n",
      "Epoch:  17     i:  23     Loss:  0.4416332244873047\n",
      "Epoch:  18     i:  0     Loss:  0.40512707829475403\n",
      "Epoch:  18     i:  1     Loss:  0.5230153203010559\n",
      "Epoch:  18     i:  2     Loss:  0.4755544066429138\n",
      "Epoch:  18     i:  3     Loss:  0.46438825130462646\n",
      "Epoch:  18     i:  4     Loss:  0.4450218677520752\n",
      "Epoch:  18     i:  5     Loss:  0.5543311238288879\n",
      "Epoch:  18     i:  6     Loss:  0.5109102725982666\n",
      "Epoch:  18     i:  7     Loss:  0.5230421423912048\n",
      "Epoch:  18     i:  8     Loss:  0.6812477111816406\n",
      "Epoch:  18     i:  9     Loss:  0.3184894919395447\n",
      "Epoch:  18     i:  10     Loss:  0.45674753189086914\n",
      "Epoch:  18     i:  11     Loss:  0.5695223808288574\n",
      "Epoch:  18     i:  12     Loss:  0.4546642005443573\n",
      "Epoch:  18     i:  13     Loss:  0.5053584575653076\n",
      "Epoch:  18     i:  14     Loss:  0.6503992676734924\n",
      "Epoch:  18     i:  15     Loss:  0.3295530080795288\n",
      "Epoch:  18     i:  16     Loss:  0.4647130072116852\n",
      "Epoch:  18     i:  17     Loss:  0.5306676030158997\n",
      "Epoch:  18     i:  18     Loss:  0.27431216835975647\n",
      "Epoch:  18     i:  19     Loss:  0.4166325330734253\n",
      "Epoch:  18     i:  20     Loss:  0.5222029089927673\n",
      "Epoch:  18     i:  21     Loss:  0.48383235931396484\n",
      "Epoch:  18     i:  22     Loss:  0.459788054227829\n",
      "Epoch:  18     i:  23     Loss:  0.40254199504852295\n",
      "Epoch:  19     i:  0     Loss:  0.39609745144844055\n",
      "Epoch:  19     i:  1     Loss:  0.4858550429344177\n",
      "Epoch:  19     i:  2     Loss:  0.5075845718383789\n",
      "Epoch:  19     i:  3     Loss:  0.6030455827713013\n",
      "Epoch:  19     i:  4     Loss:  0.4034833610057831\n",
      "Epoch:  19     i:  5     Loss:  0.5444076657295227\n",
      "Epoch:  19     i:  6     Loss:  0.44122937321662903\n",
      "Epoch:  19     i:  7     Loss:  0.37882834672927856\n",
      "Epoch:  19     i:  8     Loss:  0.3433944880962372\n",
      "Epoch:  19     i:  9     Loss:  0.4298463463783264\n",
      "Epoch:  19     i:  10     Loss:  0.5129539966583252\n",
      "Epoch:  19     i:  11     Loss:  0.5572543144226074\n",
      "Epoch:  19     i:  12     Loss:  0.4939813017845154\n",
      "Epoch:  19     i:  13     Loss:  0.6010740995407104\n",
      "Epoch:  19     i:  14     Loss:  0.59830641746521\n",
      "Epoch:  19     i:  15     Loss:  0.4423479437828064\n",
      "Epoch:  19     i:  16     Loss:  0.4715494215488434\n",
      "Epoch:  19     i:  17     Loss:  0.3163937032222748\n",
      "Epoch:  19     i:  18     Loss:  0.42516225576400757\n",
      "Epoch:  19     i:  19     Loss:  0.47109851241111755\n",
      "Epoch:  19     i:  20     Loss:  0.49310430884361267\n",
      "Epoch:  19     i:  21     Loss:  0.5683525800704956\n",
      "Epoch:  19     i:  22     Loss:  0.39494508504867554\n",
      "Epoch:  19     i:  23     Loss:  0.5052319169044495\n",
      "Epoch:  20     i:  0     Loss:  0.47562941908836365\n",
      "Epoch:  20     i:  1     Loss:  0.40389958024024963\n",
      "Epoch:  20     i:  2     Loss:  0.33246180415153503\n",
      "Epoch:  20     i:  3     Loss:  0.43582087755203247\n",
      "Epoch:  20     i:  4     Loss:  0.5766862034797668\n",
      "Epoch:  20     i:  5     Loss:  0.6579025387763977\n",
      "Epoch:  20     i:  6     Loss:  0.523292064666748\n",
      "Epoch:  20     i:  7     Loss:  0.3704184293746948\n",
      "Epoch:  20     i:  8     Loss:  0.45642706751823425\n",
      "Epoch:  20     i:  9     Loss:  0.553469181060791\n",
      "Epoch:  20     i:  10     Loss:  0.40964239835739136\n",
      "Epoch:  20     i:  11     Loss:  0.36285194754600525\n",
      "Epoch:  20     i:  12     Loss:  0.5512886047363281\n",
      "Epoch:  20     i:  13     Loss:  0.50314861536026\n",
      "Epoch:  20     i:  14     Loss:  0.6119474172592163\n",
      "Epoch:  20     i:  15     Loss:  0.4998599886894226\n",
      "Epoch:  20     i:  16     Loss:  0.4706723988056183\n",
      "Epoch:  20     i:  17     Loss:  0.4967659115791321\n",
      "Epoch:  20     i:  18     Loss:  0.3515191078186035\n",
      "Epoch:  20     i:  19     Loss:  0.4486701190471649\n",
      "Epoch:  20     i:  20     Loss:  0.4972171485424042\n",
      "Epoch:  20     i:  21     Loss:  0.4252673387527466\n",
      "Epoch:  20     i:  22     Loss:  0.4775315523147583\n",
      "Epoch:  20     i:  23     Loss:  0.4727575182914734\n",
      "Epoch:  21     i:  0     Loss:  0.4840959906578064\n",
      "Epoch:  21     i:  1     Loss:  0.5861111283302307\n",
      "Epoch:  21     i:  2     Loss:  0.4714362919330597\n",
      "Epoch:  21     i:  3     Loss:  0.37004852294921875\n",
      "Epoch:  21     i:  4     Loss:  0.46478337049484253\n",
      "Epoch:  21     i:  5     Loss:  0.5150437355041504\n",
      "Epoch:  21     i:  6     Loss:  0.3841460943222046\n",
      "Epoch:  21     i:  7     Loss:  0.49187320470809937\n",
      "Epoch:  21     i:  8     Loss:  0.46967652440071106\n",
      "Epoch:  21     i:  9     Loss:  0.5544843077659607\n",
      "Epoch:  21     i:  10     Loss:  0.44617804884910583\n",
      "Epoch:  21     i:  11     Loss:  0.4695170223712921\n",
      "Epoch:  21     i:  12     Loss:  0.39087268710136414\n",
      "Epoch:  21     i:  13     Loss:  0.38541379570961\n",
      "Epoch:  21     i:  14     Loss:  0.37923023104667664\n",
      "Epoch:  21     i:  15     Loss:  0.583164393901825\n",
      "Epoch:  21     i:  16     Loss:  0.33227333426475525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  21     i:  17     Loss:  0.45737552642822266\n",
      "Epoch:  21     i:  18     Loss:  0.5426940321922302\n",
      "Epoch:  21     i:  19     Loss:  0.549801230430603\n",
      "Epoch:  21     i:  20     Loss:  0.6649199724197388\n",
      "Epoch:  21     i:  21     Loss:  0.4491305649280548\n",
      "Epoch:  21     i:  22     Loss:  0.49691659212112427\n",
      "Epoch:  21     i:  23     Loss:  0.3948206603527069\n",
      "Epoch:  22     i:  0     Loss:  0.4530907869338989\n",
      "Epoch:  22     i:  1     Loss:  0.4718628227710724\n",
      "Epoch:  22     i:  2     Loss:  0.5481391549110413\n",
      "Epoch:  22     i:  3     Loss:  0.5055069327354431\n",
      "Epoch:  22     i:  4     Loss:  0.3841715455055237\n",
      "Epoch:  22     i:  5     Loss:  0.4525192975997925\n",
      "Epoch:  22     i:  6     Loss:  0.41432029008865356\n",
      "Epoch:  22     i:  7     Loss:  0.44085440039634705\n",
      "Epoch:  22     i:  8     Loss:  0.48609480261802673\n",
      "Epoch:  22     i:  9     Loss:  0.39360928535461426\n",
      "Epoch:  22     i:  10     Loss:  0.451335608959198\n",
      "Epoch:  22     i:  11     Loss:  0.4653457701206207\n",
      "Epoch:  22     i:  12     Loss:  0.33347874879837036\n",
      "Epoch:  22     i:  13     Loss:  0.48260635137557983\n",
      "Epoch:  22     i:  14     Loss:  0.46423789858818054\n",
      "Epoch:  22     i:  15     Loss:  0.5572993755340576\n",
      "Epoch:  22     i:  16     Loss:  0.4979643225669861\n",
      "Epoch:  22     i:  17     Loss:  0.3658842444419861\n",
      "Epoch:  22     i:  18     Loss:  0.7314778566360474\n",
      "Epoch:  22     i:  19     Loss:  0.4036039113998413\n",
      "Epoch:  22     i:  20     Loss:  0.4297059178352356\n",
      "Epoch:  22     i:  21     Loss:  0.4215087294578552\n",
      "Epoch:  22     i:  22     Loss:  0.4952603280544281\n",
      "Epoch:  22     i:  23     Loss:  0.7595592141151428\n",
      "Epoch:  23     i:  0     Loss:  0.41380780935287476\n",
      "Epoch:  23     i:  1     Loss:  0.5009225606918335\n",
      "Epoch:  23     i:  2     Loss:  0.6048654913902283\n",
      "Epoch:  23     i:  3     Loss:  0.4399547874927521\n",
      "Epoch:  23     i:  4     Loss:  0.5464591979980469\n",
      "Epoch:  23     i:  5     Loss:  0.5402535796165466\n",
      "Epoch:  23     i:  6     Loss:  0.5196354389190674\n",
      "Epoch:  23     i:  7     Loss:  0.4288759231567383\n",
      "Epoch:  23     i:  8     Loss:  0.5505748391151428\n",
      "Epoch:  23     i:  9     Loss:  0.4164084792137146\n",
      "Epoch:  23     i:  10     Loss:  0.4288741946220398\n",
      "Epoch:  23     i:  11     Loss:  0.4234510064125061\n",
      "Epoch:  23     i:  12     Loss:  0.5093486309051514\n",
      "Epoch:  23     i:  13     Loss:  0.48173829913139343\n",
      "Epoch:  23     i:  14     Loss:  0.430794358253479\n",
      "Epoch:  23     i:  15     Loss:  0.47779789566993713\n",
      "Epoch:  23     i:  16     Loss:  0.47363361716270447\n",
      "Epoch:  23     i:  17     Loss:  0.5841408967971802\n",
      "Epoch:  23     i:  18     Loss:  0.38620537519454956\n",
      "Epoch:  23     i:  19     Loss:  0.36570948362350464\n",
      "Epoch:  23     i:  20     Loss:  0.44392138719558716\n",
      "Epoch:  23     i:  21     Loss:  0.44807037711143494\n",
      "Epoch:  23     i:  22     Loss:  0.4173728823661804\n",
      "Epoch:  23     i:  23     Loss:  0.46316656470298767\n",
      "Epoch:  24     i:  0     Loss:  0.47404950857162476\n",
      "Epoch:  24     i:  1     Loss:  0.47818416357040405\n",
      "Epoch:  24     i:  2     Loss:  0.40921634435653687\n",
      "Epoch:  24     i:  3     Loss:  0.4271259903907776\n",
      "Epoch:  24     i:  4     Loss:  0.3933388590812683\n",
      "Epoch:  24     i:  5     Loss:  0.41287702322006226\n",
      "Epoch:  24     i:  6     Loss:  0.3954252004623413\n",
      "Epoch:  24     i:  7     Loss:  0.5608792901039124\n",
      "Epoch:  24     i:  8     Loss:  0.45529818534851074\n",
      "Epoch:  24     i:  9     Loss:  0.434738427400589\n",
      "Epoch:  24     i:  10     Loss:  0.49346980452537537\n",
      "Epoch:  24     i:  11     Loss:  0.5165402889251709\n",
      "Epoch:  24     i:  12     Loss:  0.42793387174606323\n",
      "Epoch:  24     i:  13     Loss:  0.5471281409263611\n",
      "Epoch:  24     i:  14     Loss:  0.37946781516075134\n",
      "Epoch:  24     i:  15     Loss:  0.6558591723442078\n",
      "Epoch:  24     i:  16     Loss:  0.4636140465736389\n",
      "Epoch:  24     i:  17     Loss:  0.3828253149986267\n",
      "Epoch:  24     i:  18     Loss:  0.48130369186401367\n",
      "Epoch:  24     i:  19     Loss:  0.5446256399154663\n",
      "Epoch:  24     i:  20     Loss:  0.47904881834983826\n",
      "Epoch:  24     i:  21     Loss:  0.5685019493103027\n",
      "Epoch:  24     i:  22     Loss:  0.4777904748916626\n",
      "Epoch:  24     i:  23     Loss:  0.40561583638191223\n",
      "Epoch:  25     i:  0     Loss:  0.49311816692352295\n",
      "Epoch:  25     i:  1     Loss:  0.36568397283554077\n",
      "Epoch:  25     i:  2     Loss:  0.5456250905990601\n",
      "Epoch:  25     i:  3     Loss:  0.44083672761917114\n",
      "Epoch:  25     i:  4     Loss:  0.5926532745361328\n",
      "Epoch:  25     i:  5     Loss:  0.5019926428794861\n",
      "Epoch:  25     i:  6     Loss:  0.3701282739639282\n",
      "Epoch:  25     i:  7     Loss:  0.5292271971702576\n",
      "Epoch:  25     i:  8     Loss:  0.44809210300445557\n",
      "Epoch:  25     i:  9     Loss:  0.6105815768241882\n",
      "Epoch:  25     i:  10     Loss:  0.40510615706443787\n",
      "Epoch:  25     i:  11     Loss:  0.5824998617172241\n",
      "Epoch:  25     i:  12     Loss:  0.5779852271080017\n",
      "Epoch:  25     i:  13     Loss:  0.4569382071495056\n",
      "Epoch:  25     i:  14     Loss:  0.27588850259780884\n",
      "Epoch:  25     i:  15     Loss:  0.5844241976737976\n",
      "Epoch:  25     i:  16     Loss:  0.37680569291114807\n",
      "Epoch:  25     i:  17     Loss:  0.4744361340999603\n",
      "Epoch:  25     i:  18     Loss:  0.42701223492622375\n",
      "Epoch:  25     i:  19     Loss:  0.4189714193344116\n",
      "Epoch:  25     i:  20     Loss:  0.4037027955055237\n",
      "Epoch:  25     i:  21     Loss:  0.42192548513412476\n",
      "Epoch:  25     i:  22     Loss:  0.4128590226173401\n",
      "Epoch:  25     i:  23     Loss:  0.5996436476707458\n",
      "Epoch:  26     i:  0     Loss:  0.5579192638397217\n",
      "Epoch:  26     i:  1     Loss:  0.5431183576583862\n",
      "Epoch:  26     i:  2     Loss:  0.538568377494812\n",
      "Epoch:  26     i:  3     Loss:  0.5066351890563965\n",
      "Epoch:  26     i:  4     Loss:  0.5096978545188904\n",
      "Epoch:  26     i:  5     Loss:  0.3402675986289978\n",
      "Epoch:  26     i:  6     Loss:  0.5043761730194092\n",
      "Epoch:  26     i:  7     Loss:  0.5208488702774048\n",
      "Epoch:  26     i:  8     Loss:  0.5335572957992554\n",
      "Epoch:  26     i:  9     Loss:  0.3112395405769348\n",
      "Epoch:  26     i:  10     Loss:  0.42779120802879333\n",
      "Epoch:  26     i:  11     Loss:  0.39478039741516113\n",
      "Epoch:  26     i:  12     Loss:  0.35411015152931213\n",
      "Epoch:  26     i:  13     Loss:  0.5682893395423889\n",
      "Epoch:  26     i:  14     Loss:  0.6159535646438599\n",
      "Epoch:  26     i:  15     Loss:  0.4754902422428131\n",
      "Epoch:  26     i:  16     Loss:  0.4464699625968933\n",
      "Epoch:  26     i:  17     Loss:  0.3663622736930847\n",
      "Epoch:  26     i:  18     Loss:  0.4163331389427185\n",
      "Epoch:  26     i:  19     Loss:  0.4488306939601898\n",
      "Epoch:  26     i:  20     Loss:  0.341692715883255\n",
      "Epoch:  26     i:  21     Loss:  0.5131480097770691\n",
      "Epoch:  26     i:  22     Loss:  0.5418312549591064\n",
      "Epoch:  26     i:  23     Loss:  0.6527429223060608\n",
      "Epoch:  27     i:  0     Loss:  0.4413142800331116\n",
      "Epoch:  27     i:  1     Loss:  0.6503869295120239\n",
      "Epoch:  27     i:  2     Loss:  0.5533404350280762\n",
      "Epoch:  27     i:  3     Loss:  0.4584167003631592\n",
      "Epoch:  27     i:  4     Loss:  0.3724592626094818\n",
      "Epoch:  27     i:  5     Loss:  0.46483415365219116\n",
      "Epoch:  27     i:  6     Loss:  0.4778057634830475\n",
      "Epoch:  27     i:  7     Loss:  0.30555450916290283\n",
      "Epoch:  27     i:  8     Loss:  0.3398986756801605\n",
      "Epoch:  27     i:  9     Loss:  0.48543938994407654\n",
      "Epoch:  27     i:  10     Loss:  0.3613647222518921\n",
      "Epoch:  27     i:  11     Loss:  0.6190420985221863\n",
      "Epoch:  27     i:  12     Loss:  0.42940443754196167\n",
      "Epoch:  27     i:  13     Loss:  0.5714138150215149\n",
      "Epoch:  27     i:  14     Loss:  0.4427492618560791\n",
      "Epoch:  27     i:  15     Loss:  0.5556704998016357\n",
      "Epoch:  27     i:  16     Loss:  0.4891868829727173\n",
      "Epoch:  27     i:  17     Loss:  0.45153823494911194\n",
      "Epoch:  27     i:  18     Loss:  0.5393658876419067\n",
      "Epoch:  27     i:  19     Loss:  0.31165096163749695\n",
      "Epoch:  27     i:  20     Loss:  0.5487600564956665\n",
      "Epoch:  27     i:  21     Loss:  0.4734628200531006\n",
      "Epoch:  27     i:  22     Loss:  0.3962114453315735\n",
      "Epoch:  27     i:  23     Loss:  0.5402700304985046\n",
      "Epoch:  28     i:  0     Loss:  0.5092881917953491\n",
      "Epoch:  28     i:  1     Loss:  0.45662662386894226\n",
      "Epoch:  28     i:  2     Loss:  0.3406151533126831\n",
      "Epoch:  28     i:  3     Loss:  0.5605794191360474\n",
      "Epoch:  28     i:  4     Loss:  0.3311086893081665\n",
      "Epoch:  28     i:  5     Loss:  0.49094274640083313\n",
      "Epoch:  28     i:  6     Loss:  0.41563868522644043\n",
      "Epoch:  28     i:  7     Loss:  0.5273590683937073\n",
      "Epoch:  28     i:  8     Loss:  0.46748247742652893\n",
      "Epoch:  28     i:  9     Loss:  0.5458183884620667\n",
      "Epoch:  28     i:  10     Loss:  0.4960700273513794\n",
      "Epoch:  28     i:  11     Loss:  0.5524189472198486\n",
      "Epoch:  28     i:  12     Loss:  0.3929905295372009\n",
      "Epoch:  28     i:  13     Loss:  0.5081640481948853\n",
      "Epoch:  28     i:  14     Loss:  0.4078074097633362\n",
      "Epoch:  28     i:  15     Loss:  0.5298115611076355\n",
      "Epoch:  28     i:  16     Loss:  0.6425763368606567\n",
      "Epoch:  28     i:  17     Loss:  0.5008805394172668\n",
      "Epoch:  28     i:  18     Loss:  0.4737224578857422\n",
      "Epoch:  28     i:  19     Loss:  0.43333685398101807\n",
      "Epoch:  28     i:  20     Loss:  0.5063199400901794\n",
      "Epoch:  28     i:  21     Loss:  0.4581170976161957\n",
      "Epoch:  28     i:  22     Loss:  0.33274245262145996\n",
      "Epoch:  28     i:  23     Loss:  0.3491075336933136\n",
      "Epoch:  29     i:  0     Loss:  0.610709547996521\n",
      "Epoch:  29     i:  1     Loss:  0.31465327739715576\n",
      "Epoch:  29     i:  2     Loss:  0.4764464199542999\n",
      "Epoch:  29     i:  3     Loss:  0.47026726603507996\n",
      "Epoch:  29     i:  4     Loss:  0.4492413103580475\n",
      "Epoch:  29     i:  5     Loss:  0.3981539309024811\n",
      "Epoch:  29     i:  6     Loss:  0.3928556442260742\n",
      "Epoch:  29     i:  7     Loss:  0.3600587546825409\n",
      "Epoch:  29     i:  8     Loss:  0.5680562853813171\n",
      "Epoch:  29     i:  9     Loss:  0.4931982755661011\n",
      "Epoch:  29     i:  10     Loss:  0.48159274458885193\n",
      "Epoch:  29     i:  11     Loss:  0.48404020071029663\n",
      "Epoch:  29     i:  12     Loss:  0.6735592484474182\n",
      "Epoch:  29     i:  13     Loss:  0.5316165685653687\n",
      "Epoch:  29     i:  14     Loss:  0.43274274468421936\n",
      "Epoch:  29     i:  15     Loss:  0.5921137928962708\n",
      "Epoch:  29     i:  16     Loss:  0.5001887083053589\n",
      "Epoch:  29     i:  17     Loss:  0.36992594599723816\n",
      "Epoch:  29     i:  18     Loss:  0.3320857882499695\n",
      "Epoch:  29     i:  19     Loss:  0.36271876096725464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  29     i:  20     Loss:  0.4852597713470459\n",
      "Epoch:  29     i:  21     Loss:  0.41917458176612854\n",
      "Epoch:  29     i:  22     Loss:  0.6013367176055908\n",
      "Epoch:  29     i:  23     Loss:  0.43886277079582214\n",
      "Epoch:  30     i:  0     Loss:  0.5565832853317261\n",
      "Epoch:  30     i:  1     Loss:  0.4358191192150116\n",
      "Epoch:  30     i:  2     Loss:  0.606031596660614\n",
      "Epoch:  30     i:  3     Loss:  0.5887839794158936\n",
      "Epoch:  30     i:  4     Loss:  0.5935682654380798\n",
      "Epoch:  30     i:  5     Loss:  0.5034874677658081\n",
      "Epoch:  30     i:  6     Loss:  0.3369221091270447\n",
      "Epoch:  30     i:  7     Loss:  0.4402788579463959\n",
      "Epoch:  30     i:  8     Loss:  0.38132259249687195\n",
      "Epoch:  30     i:  9     Loss:  0.5229153037071228\n",
      "Epoch:  30     i:  10     Loss:  0.44299423694610596\n",
      "Epoch:  30     i:  11     Loss:  0.3706256151199341\n",
      "Epoch:  30     i:  12     Loss:  0.28960660099983215\n",
      "Epoch:  30     i:  13     Loss:  0.39754924178123474\n",
      "Epoch:  30     i:  14     Loss:  0.39808207750320435\n",
      "Epoch:  30     i:  15     Loss:  0.5107300281524658\n",
      "Epoch:  30     i:  16     Loss:  0.4592527151107788\n",
      "Epoch:  30     i:  17     Loss:  0.559714138507843\n",
      "Epoch:  30     i:  18     Loss:  0.4694162607192993\n",
      "Epoch:  30     i:  19     Loss:  0.5080212354660034\n",
      "Epoch:  30     i:  20     Loss:  0.3241918683052063\n",
      "Epoch:  30     i:  21     Loss:  0.6012414693832397\n",
      "Epoch:  30     i:  22     Loss:  0.4599800109863281\n",
      "Epoch:  30     i:  23     Loss:  0.5237988233566284\n",
      "Epoch:  31     i:  0     Loss:  0.34987175464630127\n",
      "Epoch:  31     i:  1     Loss:  0.4780115783214569\n",
      "Epoch:  31     i:  2     Loss:  0.3984861373901367\n",
      "Epoch:  31     i:  3     Loss:  0.6642911434173584\n",
      "Epoch:  31     i:  4     Loss:  0.4684644043445587\n",
      "Epoch:  31     i:  5     Loss:  0.5062317848205566\n",
      "Epoch:  31     i:  6     Loss:  0.41716429591178894\n",
      "Epoch:  31     i:  7     Loss:  0.6036758422851562\n",
      "Epoch:  31     i:  8     Loss:  0.31929895281791687\n",
      "Epoch:  31     i:  9     Loss:  0.46345120668411255\n",
      "Epoch:  31     i:  10     Loss:  0.7408062219619751\n",
      "Epoch:  31     i:  11     Loss:  0.50953608751297\n",
      "Epoch:  31     i:  12     Loss:  0.3425752818584442\n",
      "Epoch:  31     i:  13     Loss:  0.5940215587615967\n",
      "Epoch:  31     i:  14     Loss:  0.4498037099838257\n",
      "Epoch:  31     i:  15     Loss:  0.3952162563800812\n",
      "Epoch:  31     i:  16     Loss:  0.4778600335121155\n",
      "Epoch:  31     i:  17     Loss:  0.3524022698402405\n",
      "Epoch:  31     i:  18     Loss:  0.36424338817596436\n",
      "Epoch:  31     i:  19     Loss:  0.5004637837409973\n",
      "Epoch:  31     i:  20     Loss:  0.5475438833236694\n",
      "Epoch:  31     i:  21     Loss:  0.4747040867805481\n",
      "Epoch:  31     i:  22     Loss:  0.3661884069442749\n",
      "Epoch:  31     i:  23     Loss:  0.4494388699531555\n",
      "Epoch:  32     i:  0     Loss:  0.49051740765571594\n",
      "Epoch:  32     i:  1     Loss:  0.5536862015724182\n",
      "Epoch:  32     i:  2     Loss:  0.5412536859512329\n",
      "Epoch:  32     i:  3     Loss:  0.4210030436515808\n",
      "Epoch:  32     i:  4     Loss:  0.3786633014678955\n",
      "Epoch:  32     i:  5     Loss:  0.42107969522476196\n",
      "Epoch:  32     i:  6     Loss:  0.6058445572853088\n",
      "Epoch:  32     i:  7     Loss:  0.4157485365867615\n",
      "Epoch:  32     i:  8     Loss:  0.41401100158691406\n",
      "Epoch:  32     i:  9     Loss:  0.35965725779533386\n",
      "Epoch:  32     i:  10     Loss:  0.4861624836921692\n",
      "Epoch:  32     i:  11     Loss:  0.526930034160614\n",
      "Epoch:  32     i:  12     Loss:  0.5653483867645264\n",
      "Epoch:  32     i:  13     Loss:  0.4429537057876587\n",
      "Epoch:  32     i:  14     Loss:  0.48182809352874756\n",
      "Epoch:  32     i:  15     Loss:  0.4620784521102905\n",
      "Epoch:  32     i:  16     Loss:  0.5419126152992249\n",
      "Epoch:  32     i:  17     Loss:  0.39884084463119507\n",
      "Epoch:  32     i:  18     Loss:  0.3559100329875946\n",
      "Epoch:  32     i:  19     Loss:  0.49083971977233887\n",
      "Epoch:  32     i:  20     Loss:  0.4271584153175354\n",
      "Epoch:  32     i:  21     Loss:  0.3717115819454193\n",
      "Epoch:  32     i:  22     Loss:  0.5707405805587769\n",
      "Epoch:  32     i:  23     Loss:  0.47900277376174927\n",
      "Epoch:  33     i:  0     Loss:  0.46385976672172546\n",
      "Epoch:  33     i:  1     Loss:  0.4624221920967102\n",
      "Epoch:  33     i:  2     Loss:  0.48634690046310425\n",
      "Epoch:  33     i:  3     Loss:  0.5151830911636353\n",
      "Epoch:  33     i:  4     Loss:  0.39355072379112244\n",
      "Epoch:  33     i:  5     Loss:  0.4152437150478363\n",
      "Epoch:  33     i:  6     Loss:  0.5711254477500916\n",
      "Epoch:  33     i:  7     Loss:  0.3217964172363281\n",
      "Epoch:  33     i:  8     Loss:  0.4954197406768799\n",
      "Epoch:  33     i:  9     Loss:  0.35465025901794434\n",
      "Epoch:  33     i:  10     Loss:  0.4390670359134674\n",
      "Epoch:  33     i:  11     Loss:  0.5241143107414246\n",
      "Epoch:  33     i:  12     Loss:  0.4514039158821106\n",
      "Epoch:  33     i:  13     Loss:  0.4578378200531006\n",
      "Epoch:  33     i:  14     Loss:  0.376548707485199\n",
      "Epoch:  33     i:  15     Loss:  0.6977177262306213\n",
      "Epoch:  33     i:  16     Loss:  0.5025135278701782\n",
      "Epoch:  33     i:  17     Loss:  0.60321044921875\n",
      "Epoch:  33     i:  18     Loss:  0.484693706035614\n",
      "Epoch:  33     i:  19     Loss:  0.3975060284137726\n",
      "Epoch:  33     i:  20     Loss:  0.52862548828125\n",
      "Epoch:  33     i:  21     Loss:  0.4496409595012665\n",
      "Epoch:  33     i:  22     Loss:  0.3562812805175781\n",
      "Epoch:  33     i:  23     Loss:  0.45770537853240967\n",
      "Epoch:  34     i:  0     Loss:  0.4849676787853241\n",
      "Epoch:  34     i:  1     Loss:  0.3497682809829712\n",
      "Epoch:  34     i:  2     Loss:  0.5602490901947021\n",
      "Epoch:  34     i:  3     Loss:  0.6033101081848145\n",
      "Epoch:  34     i:  4     Loss:  0.577923059463501\n",
      "Epoch:  34     i:  5     Loss:  0.4839005470275879\n",
      "Epoch:  34     i:  6     Loss:  0.35116246342658997\n",
      "Epoch:  34     i:  7     Loss:  0.46284979581832886\n",
      "Epoch:  34     i:  8     Loss:  0.5323346257209778\n",
      "Epoch:  34     i:  9     Loss:  0.4861198961734772\n",
      "Epoch:  34     i:  10     Loss:  0.4481593370437622\n",
      "Epoch:  34     i:  11     Loss:  0.4756019115447998\n",
      "Epoch:  34     i:  12     Loss:  0.38378554582595825\n",
      "Epoch:  34     i:  13     Loss:  0.37285560369491577\n",
      "Epoch:  34     i:  14     Loss:  0.3235352635383606\n",
      "Epoch:  34     i:  15     Loss:  0.43264245986938477\n",
      "Epoch:  34     i:  16     Loss:  0.44443854689598083\n",
      "Epoch:  34     i:  17     Loss:  0.6152427792549133\n",
      "Epoch:  34     i:  18     Loss:  0.3797338902950287\n",
      "Epoch:  34     i:  19     Loss:  0.34152674674987793\n",
      "Epoch:  34     i:  20     Loss:  0.6413736939430237\n",
      "Epoch:  34     i:  21     Loss:  0.3976682722568512\n",
      "Epoch:  34     i:  22     Loss:  0.5171679854393005\n",
      "Epoch:  34     i:  23     Loss:  0.5322025418281555\n",
      "Epoch:  35     i:  0     Loss:  0.5689364075660706\n",
      "Epoch:  35     i:  1     Loss:  0.4081951677799225\n",
      "Epoch:  35     i:  2     Loss:  0.509516716003418\n",
      "Epoch:  35     i:  3     Loss:  0.6058025360107422\n",
      "Epoch:  35     i:  4     Loss:  0.3085917532444\n",
      "Epoch:  35     i:  5     Loss:  0.4982811510562897\n",
      "Epoch:  35     i:  6     Loss:  0.5221709609031677\n",
      "Epoch:  35     i:  7     Loss:  0.3259029984474182\n",
      "Epoch:  35     i:  8     Loss:  0.36245378851890564\n",
      "Epoch:  35     i:  9     Loss:  0.4282550811767578\n",
      "Epoch:  35     i:  10     Loss:  0.509492814540863\n",
      "Epoch:  35     i:  11     Loss:  0.49220994114875793\n",
      "Epoch:  35     i:  12     Loss:  0.4929068386554718\n",
      "Epoch:  35     i:  13     Loss:  0.47506943345069885\n",
      "Epoch:  35     i:  14     Loss:  0.5960394144058228\n",
      "Epoch:  35     i:  15     Loss:  0.44159114360809326\n",
      "Epoch:  35     i:  16     Loss:  0.31714266538619995\n",
      "Epoch:  35     i:  17     Loss:  0.4964804947376251\n",
      "Epoch:  35     i:  18     Loss:  0.515241265296936\n",
      "Epoch:  35     i:  19     Loss:  0.5827462673187256\n",
      "Epoch:  35     i:  20     Loss:  0.5083075165748596\n",
      "Epoch:  35     i:  21     Loss:  0.45628416538238525\n",
      "Epoch:  35     i:  22     Loss:  0.316830575466156\n",
      "Epoch:  35     i:  23     Loss:  0.5080057978630066\n",
      "Epoch:  36     i:  0     Loss:  0.3227863311767578\n",
      "Epoch:  36     i:  1     Loss:  0.4935286343097687\n",
      "Epoch:  36     i:  2     Loss:  0.3897392153739929\n",
      "Epoch:  36     i:  3     Loss:  0.5381144285202026\n",
      "Epoch:  36     i:  4     Loss:  0.38486239314079285\n",
      "Epoch:  36     i:  5     Loss:  0.6164159774780273\n",
      "Epoch:  36     i:  6     Loss:  0.524828314781189\n",
      "Epoch:  36     i:  7     Loss:  0.45817846059799194\n",
      "Epoch:  36     i:  8     Loss:  0.3600529730319977\n",
      "Epoch:  36     i:  9     Loss:  0.4964492917060852\n",
      "Epoch:  36     i:  10     Loss:  0.32758015394210815\n",
      "Epoch:  36     i:  11     Loss:  0.47850269079208374\n",
      "Epoch:  36     i:  12     Loss:  0.3939008414745331\n",
      "Epoch:  36     i:  13     Loss:  0.591037392616272\n",
      "Epoch:  36     i:  14     Loss:  0.29335281252861023\n",
      "Epoch:  36     i:  15     Loss:  0.5481404066085815\n",
      "Epoch:  36     i:  16     Loss:  0.4915914833545685\n",
      "Epoch:  36     i:  17     Loss:  0.4182532727718353\n",
      "Epoch:  36     i:  18     Loss:  0.492347776889801\n",
      "Epoch:  36     i:  19     Loss:  0.5228241682052612\n",
      "Epoch:  36     i:  20     Loss:  0.5984339118003845\n",
      "Epoch:  36     i:  21     Loss:  0.564102053642273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  36     i:  22     Loss:  0.4207136034965515\n",
      "Epoch:  36     i:  23     Loss:  0.4522465467453003\n",
      "Epoch:  37     i:  0     Loss:  0.6404629945755005\n",
      "Epoch:  37     i:  1     Loss:  0.4837111532688141\n",
      "Epoch:  37     i:  2     Loss:  0.406627893447876\n",
      "Epoch:  37     i:  3     Loss:  0.5427221059799194\n",
      "Epoch:  37     i:  4     Loss:  0.4512418508529663\n",
      "Epoch:  37     i:  5     Loss:  0.49208685755729675\n",
      "Epoch:  37     i:  6     Loss:  0.31013306975364685\n",
      "Epoch:  37     i:  7     Loss:  0.4945322275161743\n",
      "Epoch:  37     i:  8     Loss:  0.39054742455482483\n",
      "Epoch:  37     i:  9     Loss:  0.4669707119464874\n",
      "Epoch:  37     i:  10     Loss:  0.4791635274887085\n",
      "Epoch:  37     i:  11     Loss:  0.41289758682250977\n",
      "Epoch:  37     i:  12     Loss:  0.4644995927810669\n",
      "Epoch:  37     i:  13     Loss:  0.6189916729927063\n",
      "Epoch:  37     i:  14     Loss:  0.4639893174171448\n",
      "Epoch:  37     i:  15     Loss:  0.46610450744628906\n",
      "Epoch:  37     i:  16     Loss:  0.40928566455841064\n",
      "Epoch:  37     i:  17     Loss:  0.4696332812309265\n",
      "Epoch:  37     i:  18     Loss:  0.4983430802822113\n",
      "Epoch:  37     i:  19     Loss:  0.5152130126953125\n",
      "Epoch:  37     i:  20     Loss:  0.43097996711730957\n",
      "Epoch:  37     i:  21     Loss:  0.3975154161453247\n",
      "Epoch:  37     i:  22     Loss:  0.44072872400283813\n",
      "Epoch:  37     i:  23     Loss:  0.3948097229003906\n",
      "Epoch:  38     i:  0     Loss:  0.32944726943969727\n",
      "Epoch:  38     i:  1     Loss:  0.5356777310371399\n",
      "Epoch:  38     i:  2     Loss:  0.5265599489212036\n",
      "Epoch:  38     i:  3     Loss:  0.5807274580001831\n",
      "Epoch:  38     i:  4     Loss:  0.46279269456863403\n",
      "Epoch:  38     i:  5     Loss:  0.4248237907886505\n",
      "Epoch:  38     i:  6     Loss:  0.41475391387939453\n",
      "Epoch:  38     i:  7     Loss:  0.487954318523407\n",
      "Epoch:  38     i:  8     Loss:  0.581025242805481\n",
      "Epoch:  38     i:  9     Loss:  0.4487343728542328\n",
      "Epoch:  38     i:  10     Loss:  0.40438035130500793\n",
      "Epoch:  38     i:  11     Loss:  0.5817645788192749\n",
      "Epoch:  38     i:  12     Loss:  0.3275698721408844\n",
      "Epoch:  38     i:  13     Loss:  0.39973196387290955\n",
      "Epoch:  38     i:  14     Loss:  0.6307721734046936\n",
      "Epoch:  38     i:  15     Loss:  0.5151111483573914\n",
      "Epoch:  38     i:  16     Loss:  0.5961052179336548\n",
      "Epoch:  38     i:  17     Loss:  0.3599641025066376\n",
      "Epoch:  38     i:  18     Loss:  0.38619622588157654\n",
      "Epoch:  38     i:  19     Loss:  0.4976375699043274\n",
      "Epoch:  38     i:  20     Loss:  0.48999282717704773\n",
      "Epoch:  38     i:  21     Loss:  0.2898149788379669\n",
      "Epoch:  38     i:  22     Loss:  0.44123902916908264\n",
      "Epoch:  38     i:  23     Loss:  0.44551095366477966\n",
      "Epoch:  39     i:  0     Loss:  0.3744463324546814\n",
      "Epoch:  39     i:  1     Loss:  0.5542334318161011\n",
      "Epoch:  39     i:  2     Loss:  0.5614523887634277\n",
      "Epoch:  39     i:  3     Loss:  0.5883191823959351\n",
      "Epoch:  39     i:  4     Loss:  0.6225190758705139\n",
      "Epoch:  39     i:  5     Loss:  0.5406445264816284\n",
      "Epoch:  39     i:  6     Loss:  0.4297962784767151\n",
      "Epoch:  39     i:  7     Loss:  0.41016149520874023\n",
      "Epoch:  39     i:  8     Loss:  0.5109532475471497\n",
      "Epoch:  39     i:  9     Loss:  0.3757558763027191\n",
      "Epoch:  39     i:  10     Loss:  0.40749186277389526\n",
      "Epoch:  39     i:  11     Loss:  0.3743211627006531\n",
      "Epoch:  39     i:  12     Loss:  0.4413788914680481\n",
      "Epoch:  39     i:  13     Loss:  0.49559468030929565\n",
      "Epoch:  39     i:  14     Loss:  0.3464440703392029\n",
      "Epoch:  39     i:  15     Loss:  0.4393567740917206\n",
      "Epoch:  39     i:  16     Loss:  0.3816017508506775\n",
      "Epoch:  39     i:  17     Loss:  0.2899710536003113\n",
      "Epoch:  39     i:  18     Loss:  0.47075751423835754\n",
      "Epoch:  39     i:  19     Loss:  0.4975079298019409\n",
      "Epoch:  39     i:  20     Loss:  0.41684791445732117\n",
      "Epoch:  39     i:  21     Loss:  0.6481934189796448\n",
      "Epoch:  39     i:  22     Loss:  0.3799150586128235\n",
      "Epoch:  39     i:  23     Loss:  0.6392844915390015\n",
      "Epoch:  40     i:  0     Loss:  0.42150524258613586\n",
      "Epoch:  40     i:  1     Loss:  0.6554580330848694\n",
      "Epoch:  40     i:  2     Loss:  0.5289942622184753\n",
      "Epoch:  40     i:  3     Loss:  0.3652663230895996\n",
      "Epoch:  40     i:  4     Loss:  0.4043925702571869\n",
      "Epoch:  40     i:  5     Loss:  0.2814585864543915\n",
      "Epoch:  40     i:  6     Loss:  0.38968077301979065\n",
      "Epoch:  40     i:  7     Loss:  0.646897554397583\n",
      "Epoch:  40     i:  8     Loss:  0.42329877614974976\n",
      "Epoch:  40     i:  9     Loss:  0.48054224252700806\n",
      "Epoch:  40     i:  10     Loss:  0.46143972873687744\n",
      "Epoch:  40     i:  11     Loss:  0.5233290791511536\n",
      "Epoch:  40     i:  12     Loss:  0.4009111821651459\n",
      "Epoch:  40     i:  13     Loss:  0.5272163152694702\n",
      "Epoch:  40     i:  14     Loss:  0.4140999913215637\n",
      "Epoch:  40     i:  15     Loss:  0.4013776183128357\n",
      "Epoch:  40     i:  16     Loss:  0.4819270074367523\n",
      "Epoch:  40     i:  17     Loss:  0.4927055239677429\n",
      "Epoch:  40     i:  18     Loss:  0.5018774271011353\n",
      "Epoch:  40     i:  19     Loss:  0.38501405715942383\n",
      "Epoch:  40     i:  20     Loss:  0.4602765142917633\n",
      "Epoch:  40     i:  21     Loss:  0.45037341117858887\n",
      "Epoch:  40     i:  22     Loss:  0.4947575330734253\n",
      "Epoch:  40     i:  23     Loss:  0.5360590219497681\n",
      "Epoch:  41     i:  0     Loss:  0.4012000560760498\n",
      "Epoch:  41     i:  1     Loss:  0.38859185576438904\n",
      "Epoch:  41     i:  2     Loss:  0.435413122177124\n",
      "Epoch:  41     i:  3     Loss:  0.3355155885219574\n",
      "Epoch:  41     i:  4     Loss:  0.49898970127105713\n",
      "Epoch:  41     i:  5     Loss:  0.3299058675765991\n",
      "Epoch:  41     i:  6     Loss:  0.5232635736465454\n",
      "Epoch:  41     i:  7     Loss:  0.46231576800346375\n",
      "Epoch:  41     i:  8     Loss:  0.5335524678230286\n",
      "Epoch:  41     i:  9     Loss:  0.36367058753967285\n",
      "Epoch:  41     i:  10     Loss:  0.488111674785614\n",
      "Epoch:  41     i:  11     Loss:  0.7018483877182007\n",
      "Epoch:  41     i:  12     Loss:  0.4332095980644226\n",
      "Epoch:  41     i:  13     Loss:  0.44381532073020935\n",
      "Epoch:  41     i:  14     Loss:  0.44974395632743835\n",
      "Epoch:  41     i:  15     Loss:  0.5162494778633118\n",
      "Epoch:  41     i:  16     Loss:  0.48383641242980957\n",
      "Epoch:  41     i:  17     Loss:  0.4554884433746338\n",
      "Epoch:  41     i:  18     Loss:  0.3925425410270691\n",
      "Epoch:  41     i:  19     Loss:  0.47905898094177246\n",
      "Epoch:  41     i:  20     Loss:  0.6607083082199097\n",
      "Epoch:  41     i:  21     Loss:  0.559607744216919\n",
      "Epoch:  41     i:  22     Loss:  0.3444550931453705\n",
      "Epoch:  41     i:  23     Loss:  0.42627406120300293\n",
      "Epoch:  42     i:  0     Loss:  0.5182793736457825\n",
      "Epoch:  42     i:  1     Loss:  0.38020163774490356\n",
      "Epoch:  42     i:  2     Loss:  0.6272168755531311\n",
      "Epoch:  42     i:  3     Loss:  0.3125186562538147\n",
      "Epoch:  42     i:  4     Loss:  0.4762040376663208\n",
      "Epoch:  42     i:  5     Loss:  0.30336084961891174\n",
      "Epoch:  42     i:  6     Loss:  0.41283220052719116\n",
      "Epoch:  42     i:  7     Loss:  0.5556996464729309\n",
      "Epoch:  42     i:  8     Loss:  0.5089396238327026\n",
      "Epoch:  42     i:  9     Loss:  0.6278723478317261\n",
      "Epoch:  42     i:  10     Loss:  0.5019820332527161\n",
      "Epoch:  42     i:  11     Loss:  0.34965306520462036\n",
      "Epoch:  42     i:  12     Loss:  0.532626748085022\n",
      "Epoch:  42     i:  13     Loss:  0.6162601113319397\n",
      "Epoch:  42     i:  14     Loss:  0.39098650217056274\n",
      "Epoch:  42     i:  15     Loss:  0.5533115863800049\n",
      "Epoch:  42     i:  16     Loss:  0.6131769418716431\n",
      "Epoch:  42     i:  17     Loss:  0.42854925990104675\n",
      "Epoch:  42     i:  18     Loss:  0.4254547953605652\n",
      "Epoch:  42     i:  19     Loss:  0.35176533460617065\n",
      "Epoch:  42     i:  20     Loss:  0.4210679233074188\n",
      "Epoch:  42     i:  21     Loss:  0.3989346921443939\n",
      "Epoch:  42     i:  22     Loss:  0.3059227764606476\n",
      "Epoch:  42     i:  23     Loss:  0.44005095958709717\n",
      "Epoch:  43     i:  0     Loss:  0.6228942275047302\n",
      "Epoch:  43     i:  1     Loss:  0.36619728803634644\n",
      "Epoch:  43     i:  2     Loss:  0.4850817918777466\n",
      "Epoch:  43     i:  3     Loss:  0.306397408246994\n",
      "Epoch:  43     i:  4     Loss:  0.5330305099487305\n",
      "Epoch:  43     i:  5     Loss:  0.48013487458229065\n",
      "Epoch:  43     i:  6     Loss:  0.4622766673564911\n",
      "Epoch:  43     i:  7     Loss:  0.5380727052688599\n",
      "Epoch:  43     i:  8     Loss:  0.4566742479801178\n",
      "Epoch:  43     i:  9     Loss:  0.5920400619506836\n",
      "Epoch:  43     i:  10     Loss:  0.38453415036201477\n",
      "Epoch:  43     i:  11     Loss:  0.4429612159729004\n",
      "Epoch:  43     i:  12     Loss:  0.2560346722602844\n",
      "Epoch:  43     i:  13     Loss:  0.3996387720108032\n",
      "Epoch:  43     i:  14     Loss:  0.49434518814086914\n",
      "Epoch:  43     i:  15     Loss:  0.47164955735206604\n",
      "Epoch:  43     i:  16     Loss:  0.5174930691719055\n",
      "Epoch:  43     i:  17     Loss:  0.5559481978416443\n",
      "Epoch:  43     i:  18     Loss:  0.5104622840881348\n",
      "Epoch:  43     i:  19     Loss:  0.4150352478027344\n",
      "Epoch:  43     i:  20     Loss:  0.40804848074913025\n",
      "Epoch:  43     i:  21     Loss:  0.4848225712776184\n",
      "Epoch:  43     i:  22     Loss:  0.40924423933029175\n",
      "Epoch:  43     i:  23     Loss:  0.48243236541748047\n",
      "Epoch:  44     i:  0     Loss:  0.4419565796852112\n",
      "Epoch:  44     i:  1     Loss:  0.5190414786338806\n",
      "Epoch:  44     i:  2     Loss:  0.333018034696579\n",
      "Epoch:  44     i:  3     Loss:  0.4235067367553711\n",
      "Epoch:  44     i:  4     Loss:  0.4012698531150818\n",
      "Epoch:  44     i:  5     Loss:  0.721084475517273\n",
      "Epoch:  44     i:  6     Loss:  0.48320630192756653\n",
      "Epoch:  44     i:  7     Loss:  0.5132688283920288\n",
      "Epoch:  44     i:  8     Loss:  0.30644047260284424\n",
      "Epoch:  44     i:  9     Loss:  0.5464402437210083\n",
      "Epoch:  44     i:  10     Loss:  0.499722421169281\n",
      "Epoch:  44     i:  11     Loss:  0.6738312840461731\n",
      "Epoch:  44     i:  12     Loss:  0.42446979880332947\n",
      "Epoch:  44     i:  13     Loss:  0.44103720784187317\n",
      "Epoch:  44     i:  14     Loss:  0.5285122394561768\n",
      "Epoch:  44     i:  15     Loss:  0.3243039548397064\n",
      "Epoch:  44     i:  16     Loss:  0.4701904356479645\n",
      "Epoch:  44     i:  17     Loss:  0.547416627407074\n",
      "Epoch:  44     i:  18     Loss:  0.4875076413154602\n",
      "Epoch:  44     i:  19     Loss:  0.47545525431632996\n",
      "Epoch:  44     i:  20     Loss:  0.4467748701572418\n",
      "Epoch:  44     i:  21     Loss:  0.2552727460861206\n",
      "Epoch:  44     i:  22     Loss:  0.44577258825302124\n",
      "Epoch:  44     i:  23     Loss:  0.3781577944755554\n",
      "Epoch:  45     i:  0     Loss:  0.3927473723888397\n",
      "Epoch:  45     i:  1     Loss:  0.5806565880775452\n",
      "Epoch:  45     i:  2     Loss:  0.4428615868091583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  45     i:  3     Loss:  0.4114346206188202\n",
      "Epoch:  45     i:  4     Loss:  0.46497851610183716\n",
      "Epoch:  45     i:  5     Loss:  0.5297031998634338\n",
      "Epoch:  45     i:  6     Loss:  0.2556438148021698\n",
      "Epoch:  45     i:  7     Loss:  0.55900639295578\n",
      "Epoch:  45     i:  8     Loss:  0.5662081241607666\n",
      "Epoch:  45     i:  9     Loss:  0.617591381072998\n",
      "Epoch:  45     i:  10     Loss:  0.39855921268463135\n",
      "Epoch:  45     i:  11     Loss:  0.4714507460594177\n",
      "Epoch:  45     i:  12     Loss:  0.5617696642875671\n",
      "Epoch:  45     i:  13     Loss:  0.3289419114589691\n",
      "Epoch:  45     i:  14     Loss:  0.32555294036865234\n",
      "Epoch:  45     i:  15     Loss:  0.458429753780365\n",
      "Epoch:  45     i:  16     Loss:  0.4950810968875885\n",
      "Epoch:  45     i:  17     Loss:  0.4609019458293915\n",
      "Epoch:  45     i:  18     Loss:  0.5199463367462158\n",
      "Epoch:  45     i:  19     Loss:  0.6640368700027466\n",
      "Epoch:  45     i:  20     Loss:  0.41206055879592896\n",
      "Epoch:  45     i:  21     Loss:  0.3383113145828247\n",
      "Epoch:  45     i:  22     Loss:  0.4064653217792511\n",
      "Epoch:  45     i:  23     Loss:  0.40933459997177124\n",
      "Epoch:  46     i:  0     Loss:  0.4481063485145569\n",
      "Epoch:  46     i:  1     Loss:  0.4955962002277374\n",
      "Epoch:  46     i:  2     Loss:  0.3635987639427185\n",
      "Epoch:  46     i:  3     Loss:  0.42944949865341187\n",
      "Epoch:  46     i:  4     Loss:  0.48832467198371887\n",
      "Epoch:  46     i:  5     Loss:  0.38567331433296204\n",
      "Epoch:  46     i:  6     Loss:  0.4653898775577545\n",
      "Epoch:  46     i:  7     Loss:  0.5071201920509338\n",
      "Epoch:  46     i:  8     Loss:  0.4633338451385498\n",
      "Epoch:  46     i:  9     Loss:  0.6373817324638367\n",
      "Epoch:  46     i:  10     Loss:  0.5297664999961853\n",
      "Epoch:  46     i:  11     Loss:  0.34681007266044617\n",
      "Epoch:  46     i:  12     Loss:  0.5469010472297668\n",
      "Epoch:  46     i:  13     Loss:  0.41042912006378174\n",
      "Epoch:  46     i:  14     Loss:  0.39389434456825256\n",
      "Epoch:  46     i:  15     Loss:  0.38390839099884033\n",
      "Epoch:  46     i:  16     Loss:  0.33050450682640076\n",
      "Epoch:  46     i:  17     Loss:  0.40178054571151733\n",
      "Epoch:  46     i:  18     Loss:  0.5968270301818848\n",
      "Epoch:  46     i:  19     Loss:  0.6285724639892578\n",
      "Epoch:  46     i:  20     Loss:  0.4646519720554352\n",
      "Epoch:  46     i:  21     Loss:  0.48000606894493103\n",
      "Epoch:  46     i:  22     Loss:  0.4202757179737091\n",
      "Epoch:  46     i:  23     Loss:  0.3880552351474762\n",
      "Epoch:  47     i:  0     Loss:  0.49015846848487854\n",
      "Epoch:  47     i:  1     Loss:  0.3940383493900299\n",
      "Epoch:  47     i:  2     Loss:  0.33338993787765503\n",
      "Epoch:  47     i:  3     Loss:  0.578559160232544\n",
      "Epoch:  47     i:  4     Loss:  0.35563334822654724\n",
      "Epoch:  47     i:  5     Loss:  0.393804669380188\n",
      "Epoch:  47     i:  6     Loss:  0.5624247193336487\n",
      "Epoch:  47     i:  7     Loss:  0.5820825099945068\n",
      "Epoch:  47     i:  8     Loss:  0.4286368191242218\n",
      "Epoch:  47     i:  9     Loss:  0.46439892053604126\n",
      "Epoch:  47     i:  10     Loss:  0.3428846001625061\n",
      "Epoch:  47     i:  11     Loss:  0.3201889395713806\n",
      "Epoch:  47     i:  12     Loss:  0.5031057596206665\n",
      "Epoch:  47     i:  13     Loss:  0.4374862015247345\n",
      "Epoch:  47     i:  14     Loss:  0.43661752343177795\n",
      "Epoch:  47     i:  15     Loss:  0.509091854095459\n",
      "Epoch:  47     i:  16     Loss:  0.35267549753189087\n",
      "Epoch:  47     i:  17     Loss:  0.5443114042282104\n",
      "Epoch:  47     i:  18     Loss:  0.3209410309791565\n",
      "Epoch:  47     i:  19     Loss:  0.5705175399780273\n",
      "Epoch:  47     i:  20     Loss:  0.4682515263557434\n",
      "Epoch:  47     i:  21     Loss:  0.5047853589057922\n",
      "Epoch:  47     i:  22     Loss:  0.4090367555618286\n",
      "Epoch:  47     i:  23     Loss:  0.8258119821548462\n",
      "Epoch:  48     i:  0     Loss:  0.384998619556427\n",
      "Epoch:  48     i:  1     Loss:  0.6252416372299194\n",
      "Epoch:  48     i:  2     Loss:  0.4618224799633026\n",
      "Epoch:  48     i:  3     Loss:  0.49181464314460754\n",
      "Epoch:  48     i:  4     Loss:  0.36421406269073486\n",
      "Epoch:  48     i:  5     Loss:  0.40990114212036133\n",
      "Epoch:  48     i:  6     Loss:  0.4221923351287842\n",
      "Epoch:  48     i:  7     Loss:  0.4854266941547394\n",
      "Epoch:  48     i:  8     Loss:  0.41924622654914856\n",
      "Epoch:  48     i:  9     Loss:  0.42565083503723145\n",
      "Epoch:  48     i:  10     Loss:  0.5615547895431519\n",
      "Epoch:  48     i:  11     Loss:  0.48598453402519226\n",
      "Epoch:  48     i:  12     Loss:  0.5595921874046326\n",
      "Epoch:  48     i:  13     Loss:  0.3155977725982666\n",
      "Epoch:  48     i:  14     Loss:  0.42692118883132935\n",
      "Epoch:  48     i:  15     Loss:  0.4671374559402466\n",
      "Epoch:  48     i:  16     Loss:  0.33718445897102356\n",
      "Epoch:  48     i:  17     Loss:  0.5297001600265503\n",
      "Epoch:  48     i:  18     Loss:  0.5126910209655762\n",
      "Epoch:  48     i:  19     Loss:  0.4721956253051758\n",
      "Epoch:  48     i:  20     Loss:  0.4394347369670868\n",
      "Epoch:  48     i:  21     Loss:  0.3890923261642456\n",
      "Epoch:  48     i:  22     Loss:  0.5162797570228577\n",
      "Epoch:  48     i:  23     Loss:  0.5236216187477112\n",
      "Epoch:  49     i:  0     Loss:  0.5796219706535339\n",
      "Epoch:  49     i:  1     Loss:  0.3350462019443512\n",
      "Epoch:  49     i:  2     Loss:  0.30758190155029297\n",
      "Epoch:  49     i:  3     Loss:  0.5575084686279297\n",
      "Epoch:  49     i:  4     Loss:  0.4251546263694763\n",
      "Epoch:  49     i:  5     Loss:  0.5054871439933777\n",
      "Epoch:  49     i:  6     Loss:  0.4556606113910675\n",
      "Epoch:  49     i:  7     Loss:  0.5152947306632996\n",
      "Epoch:  49     i:  8     Loss:  0.2976875305175781\n",
      "Epoch:  49     i:  9     Loss:  0.5122736096382141\n",
      "Epoch:  49     i:  10     Loss:  0.45999565720558167\n",
      "Epoch:  49     i:  11     Loss:  0.34850242733955383\n",
      "Epoch:  49     i:  12     Loss:  0.3494427800178528\n",
      "Epoch:  49     i:  13     Loss:  0.3380443751811981\n",
      "Epoch:  49     i:  14     Loss:  0.578571081161499\n",
      "Epoch:  49     i:  15     Loss:  0.46879178285598755\n",
      "Epoch:  49     i:  16     Loss:  0.40075016021728516\n",
      "Epoch:  49     i:  17     Loss:  0.416777640581131\n",
      "Epoch:  49     i:  18     Loss:  0.5001235008239746\n",
      "Epoch:  49     i:  19     Loss:  0.6230898499488831\n",
      "Epoch:  49     i:  20     Loss:  0.6125415563583374\n",
      "Epoch:  49     i:  21     Loss:  0.5269924402236938\n",
      "Epoch:  49     i:  22     Loss:  0.2999195456504822\n",
      "Epoch:  49     i:  23     Loss:  0.6164548993110657\n",
      "Epoch:  50     i:  0     Loss:  0.404423326253891\n",
      "Epoch:  50     i:  1     Loss:  0.32487767934799194\n",
      "Epoch:  50     i:  2     Loss:  0.31335288286209106\n",
      "Epoch:  50     i:  3     Loss:  0.40900373458862305\n",
      "Epoch:  50     i:  4     Loss:  0.5586350560188293\n",
      "Epoch:  50     i:  5     Loss:  0.3204966187477112\n",
      "Epoch:  50     i:  6     Loss:  0.5312228202819824\n",
      "Epoch:  50     i:  7     Loss:  0.4254644215106964\n",
      "Epoch:  50     i:  8     Loss:  0.40010735392570496\n",
      "Epoch:  50     i:  9     Loss:  0.5394676327705383\n",
      "Epoch:  50     i:  10     Loss:  0.4329380393028259\n",
      "Epoch:  50     i:  11     Loss:  0.4795891046524048\n",
      "Epoch:  50     i:  12     Loss:  0.5914076566696167\n",
      "Epoch:  50     i:  13     Loss:  0.44650959968566895\n",
      "Epoch:  50     i:  14     Loss:  0.6143290996551514\n",
      "Epoch:  50     i:  15     Loss:  0.4936831295490265\n",
      "Epoch:  50     i:  16     Loss:  0.42011258006095886\n",
      "Epoch:  50     i:  17     Loss:  0.4668956696987152\n",
      "Epoch:  50     i:  18     Loss:  0.44529426097869873\n",
      "Epoch:  50     i:  19     Loss:  0.5511339902877808\n",
      "Epoch:  50     i:  20     Loss:  0.44703662395477295\n",
      "Epoch:  50     i:  21     Loss:  0.47442689538002014\n",
      "Epoch:  50     i:  22     Loss:  0.44789332151412964\n",
      "Epoch:  50     i:  23     Loss:  0.4665478765964508\n",
      "Epoch:  51     i:  0     Loss:  0.46852800250053406\n",
      "Epoch:  51     i:  1     Loss:  0.3391093611717224\n",
      "Epoch:  51     i:  2     Loss:  0.5362426042556763\n",
      "Epoch:  51     i:  3     Loss:  0.298528254032135\n",
      "Epoch:  51     i:  4     Loss:  0.6070512533187866\n",
      "Epoch:  51     i:  5     Loss:  0.4806199371814728\n",
      "Epoch:  51     i:  6     Loss:  0.4095383584499359\n",
      "Epoch:  51     i:  7     Loss:  0.5032052993774414\n",
      "Epoch:  51     i:  8     Loss:  0.5961302518844604\n",
      "Epoch:  51     i:  9     Loss:  0.466002494096756\n",
      "Epoch:  51     i:  10     Loss:  0.4994514584541321\n",
      "Epoch:  51     i:  11     Loss:  0.4856148660182953\n",
      "Epoch:  51     i:  12     Loss:  0.4152217209339142\n",
      "Epoch:  51     i:  13     Loss:  0.4650558531284332\n",
      "Epoch:  51     i:  14     Loss:  0.49338656663894653\n",
      "Epoch:  51     i:  15     Loss:  0.5664166212081909\n",
      "Epoch:  51     i:  16     Loss:  0.3340352773666382\n",
      "Epoch:  51     i:  17     Loss:  0.2809900641441345\n",
      "Epoch:  51     i:  18     Loss:  0.4297848641872406\n",
      "Epoch:  51     i:  19     Loss:  0.4234212040901184\n",
      "Epoch:  51     i:  20     Loss:  0.6167032122612\n",
      "Epoch:  51     i:  21     Loss:  0.37875738739967346\n",
      "Epoch:  51     i:  22     Loss:  0.37964075803756714\n",
      "Epoch:  51     i:  23     Loss:  0.47071373462677\n",
      "Epoch:  52     i:  0     Loss:  0.48920974135398865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  52     i:  1     Loss:  0.5480443835258484\n",
      "Epoch:  52     i:  2     Loss:  0.5680370926856995\n",
      "Epoch:  52     i:  3     Loss:  0.4429502487182617\n",
      "Epoch:  52     i:  4     Loss:  0.3537723422050476\n",
      "Epoch:  52     i:  5     Loss:  0.4795737862586975\n",
      "Epoch:  52     i:  6     Loss:  0.5263274312019348\n",
      "Epoch:  52     i:  7     Loss:  0.5332344770431519\n",
      "Epoch:  52     i:  8     Loss:  0.4795440435409546\n",
      "Epoch:  52     i:  9     Loss:  0.45116347074508667\n",
      "Epoch:  52     i:  10     Loss:  0.4437745213508606\n",
      "Epoch:  52     i:  11     Loss:  0.4098247289657593\n",
      "Epoch:  52     i:  12     Loss:  0.3878423571586609\n",
      "Epoch:  52     i:  13     Loss:  0.5022895932197571\n",
      "Epoch:  52     i:  14     Loss:  0.32378000020980835\n",
      "Epoch:  52     i:  15     Loss:  0.4271570146083832\n",
      "Epoch:  52     i:  16     Loss:  0.5178225040435791\n",
      "Epoch:  52     i:  17     Loss:  0.3910221755504608\n",
      "Epoch:  52     i:  18     Loss:  0.5552371144294739\n",
      "Epoch:  52     i:  19     Loss:  0.528892457485199\n",
      "Epoch:  52     i:  20     Loss:  0.4204198122024536\n",
      "Epoch:  52     i:  21     Loss:  0.3939760625362396\n",
      "Epoch:  52     i:  22     Loss:  0.3555671274662018\n",
      "Epoch:  52     i:  23     Loss:  0.45304396748542786\n",
      "Epoch:  53     i:  0     Loss:  0.4378358721733093\n",
      "Epoch:  53     i:  1     Loss:  0.41984814405441284\n",
      "Epoch:  53     i:  2     Loss:  0.33834508061408997\n",
      "Epoch:  53     i:  3     Loss:  0.4614203870296478\n",
      "Epoch:  53     i:  4     Loss:  0.4832930862903595\n",
      "Epoch:  53     i:  5     Loss:  0.2869451940059662\n",
      "Epoch:  53     i:  6     Loss:  0.377782940864563\n",
      "Epoch:  53     i:  7     Loss:  0.6150962710380554\n",
      "Epoch:  53     i:  8     Loss:  0.5080311894416809\n",
      "Epoch:  53     i:  9     Loss:  0.24263779819011688\n",
      "Epoch:  53     i:  10     Loss:  0.3919821083545685\n",
      "Epoch:  53     i:  11     Loss:  0.48020073771476746\n",
      "Epoch:  53     i:  12     Loss:  0.46135154366493225\n",
      "Epoch:  53     i:  13     Loss:  0.3990207314491272\n",
      "Epoch:  53     i:  14     Loss:  0.5130563378334045\n",
      "Epoch:  53     i:  15     Loss:  0.4730846881866455\n",
      "Epoch:  53     i:  16     Loss:  0.5393381118774414\n",
      "Epoch:  53     i:  17     Loss:  0.4676392674446106\n",
      "Epoch:  53     i:  18     Loss:  0.5418652296066284\n",
      "Epoch:  53     i:  19     Loss:  0.5422664284706116\n",
      "Epoch:  53     i:  20     Loss:  0.5870139002799988\n",
      "Epoch:  53     i:  21     Loss:  0.48743709921836853\n",
      "Epoch:  53     i:  22     Loss:  0.46556586027145386\n",
      "Epoch:  53     i:  23     Loss:  0.5500593185424805\n",
      "Epoch:  54     i:  0     Loss:  0.500393271446228\n",
      "Epoch:  54     i:  1     Loss:  0.37897929549217224\n",
      "Epoch:  54     i:  2     Loss:  0.49728038907051086\n",
      "Epoch:  54     i:  3     Loss:  0.5310193300247192\n",
      "Epoch:  54     i:  4     Loss:  0.5046436190605164\n",
      "Epoch:  54     i:  5     Loss:  0.5156071186065674\n",
      "Epoch:  54     i:  6     Loss:  0.4474096894264221\n",
      "Epoch:  54     i:  7     Loss:  0.525861382484436\n",
      "Epoch:  54     i:  8     Loss:  0.4164092540740967\n",
      "Epoch:  54     i:  9     Loss:  0.5053678154945374\n",
      "Epoch:  54     i:  10     Loss:  0.3909296691417694\n",
      "Epoch:  54     i:  11     Loss:  0.3880060911178589\n",
      "Epoch:  54     i:  12     Loss:  0.5523061156272888\n",
      "Epoch:  54     i:  13     Loss:  0.436454176902771\n",
      "Epoch:  54     i:  14     Loss:  0.36594924330711365\n",
      "Epoch:  54     i:  15     Loss:  0.3204668462276459\n",
      "Epoch:  54     i:  16     Loss:  0.5095453262329102\n",
      "Epoch:  54     i:  17     Loss:  0.3676142692565918\n",
      "Epoch:  54     i:  18     Loss:  0.47182077169418335\n",
      "Epoch:  54     i:  19     Loss:  0.5103819370269775\n",
      "Epoch:  54     i:  20     Loss:  0.68319171667099\n",
      "Epoch:  54     i:  21     Loss:  0.36051642894744873\n",
      "Epoch:  54     i:  22     Loss:  0.39083346724510193\n",
      "Epoch:  54     i:  23     Loss:  0.5006819367408752\n",
      "Epoch:  55     i:  0     Loss:  0.5947427749633789\n",
      "Epoch:  55     i:  1     Loss:  0.5488208532333374\n",
      "Epoch:  55     i:  2     Loss:  0.39944398403167725\n",
      "Epoch:  55     i:  3     Loss:  0.4131297469139099\n",
      "Epoch:  55     i:  4     Loss:  0.35485148429870605\n",
      "Epoch:  55     i:  5     Loss:  0.4223483204841614\n",
      "Epoch:  55     i:  6     Loss:  0.6030304431915283\n",
      "Epoch:  55     i:  7     Loss:  0.4597245752811432\n",
      "Epoch:  55     i:  8     Loss:  0.4372228980064392\n",
      "Epoch:  55     i:  9     Loss:  0.4569924771785736\n",
      "Epoch:  55     i:  10     Loss:  0.4437747001647949\n",
      "Epoch:  55     i:  11     Loss:  0.3776887357234955\n",
      "Epoch:  55     i:  12     Loss:  0.42573976516723633\n",
      "Epoch:  55     i:  13     Loss:  0.34453630447387695\n",
      "Epoch:  55     i:  14     Loss:  0.5083838105201721\n",
      "Epoch:  55     i:  15     Loss:  0.6166426539421082\n",
      "Epoch:  55     i:  16     Loss:  0.4477275609970093\n",
      "Epoch:  55     i:  17     Loss:  0.42283502221107483\n",
      "Epoch:  55     i:  18     Loss:  0.5615458488464355\n",
      "Epoch:  55     i:  19     Loss:  0.37335774302482605\n",
      "Epoch:  55     i:  20     Loss:  0.46605339646339417\n",
      "Epoch:  55     i:  21     Loss:  0.5232370495796204\n",
      "Epoch:  55     i:  22     Loss:  0.39372026920318604\n",
      "Epoch:  55     i:  23     Loss:  0.30909666419029236\n",
      "Epoch:  56     i:  0     Loss:  0.23767422139644623\n",
      "Epoch:  56     i:  1     Loss:  0.513062596321106\n",
      "Epoch:  56     i:  2     Loss:  0.361859530210495\n",
      "Epoch:  56     i:  3     Loss:  0.44922804832458496\n",
      "Epoch:  56     i:  4     Loss:  0.35209921002388\n",
      "Epoch:  56     i:  5     Loss:  0.42952921986579895\n",
      "Epoch:  56     i:  6     Loss:  0.594996452331543\n",
      "Epoch:  56     i:  7     Loss:  0.4398181140422821\n",
      "Epoch:  56     i:  8     Loss:  0.3807624578475952\n",
      "Epoch:  56     i:  9     Loss:  0.6818252801895142\n",
      "Epoch:  56     i:  10     Loss:  0.6225358843803406\n",
      "Epoch:  56     i:  11     Loss:  0.5056769847869873\n",
      "Epoch:  56     i:  12     Loss:  0.456611305475235\n",
      "Epoch:  56     i:  13     Loss:  0.3683303892612457\n",
      "Epoch:  56     i:  14     Loss:  0.30471423268318176\n",
      "Epoch:  56     i:  15     Loss:  0.506260871887207\n",
      "Epoch:  56     i:  16     Loss:  0.617720901966095\n",
      "Epoch:  56     i:  17     Loss:  0.4958968162536621\n",
      "Epoch:  56     i:  18     Loss:  0.4461015462875366\n",
      "Epoch:  56     i:  19     Loss:  0.568181037902832\n",
      "Epoch:  56     i:  20     Loss:  0.44819772243499756\n",
      "Epoch:  56     i:  21     Loss:  0.5013012886047363\n",
      "Epoch:  56     i:  22     Loss:  0.2861727774143219\n",
      "Epoch:  56     i:  23     Loss:  0.3965528607368469\n",
      "Epoch:  57     i:  0     Loss:  0.33907637000083923\n",
      "Epoch:  57     i:  1     Loss:  0.4910006523132324\n",
      "Epoch:  57     i:  2     Loss:  0.4347016215324402\n",
      "Epoch:  57     i:  3     Loss:  0.5042368173599243\n",
      "Epoch:  57     i:  4     Loss:  0.4138036370277405\n",
      "Epoch:  57     i:  5     Loss:  0.5494952201843262\n",
      "Epoch:  57     i:  6     Loss:  0.3812912404537201\n",
      "Epoch:  57     i:  7     Loss:  0.4313274025917053\n",
      "Epoch:  57     i:  8     Loss:  0.4325713515281677\n",
      "Epoch:  57     i:  9     Loss:  0.32652363181114197\n",
      "Epoch:  57     i:  10     Loss:  0.5523534417152405\n",
      "Epoch:  57     i:  11     Loss:  0.5529557466506958\n",
      "Epoch:  57     i:  12     Loss:  0.4622565507888794\n",
      "Epoch:  57     i:  13     Loss:  0.39450711011886597\n",
      "Epoch:  57     i:  14     Loss:  0.5628509521484375\n",
      "Epoch:  57     i:  15     Loss:  0.29895922541618347\n",
      "Epoch:  57     i:  16     Loss:  0.3667846620082855\n",
      "Epoch:  57     i:  17     Loss:  0.3862471282482147\n",
      "Epoch:  57     i:  18     Loss:  0.3940117061138153\n",
      "Epoch:  57     i:  19     Loss:  0.5874502658843994\n",
      "Epoch:  57     i:  20     Loss:  0.343371719121933\n",
      "Epoch:  57     i:  21     Loss:  0.512540876865387\n",
      "Epoch:  57     i:  22     Loss:  0.6426470875740051\n",
      "Epoch:  57     i:  23     Loss:  0.6359732747077942\n",
      "Epoch:  58     i:  0     Loss:  0.4252263009548187\n",
      "Epoch:  58     i:  1     Loss:  0.4323582947254181\n",
      "Epoch:  58     i:  2     Loss:  0.42560380697250366\n",
      "Epoch:  58     i:  3     Loss:  0.5665302872657776\n",
      "Epoch:  58     i:  4     Loss:  0.3779211938381195\n",
      "Epoch:  58     i:  5     Loss:  0.547926127910614\n",
      "Epoch:  58     i:  6     Loss:  0.456361323595047\n",
      "Epoch:  58     i:  7     Loss:  0.4660848379135132\n",
      "Epoch:  58     i:  8     Loss:  0.4614746570587158\n",
      "Epoch:  58     i:  9     Loss:  0.420467346906662\n",
      "Epoch:  58     i:  10     Loss:  0.5346806049346924\n",
      "Epoch:  58     i:  11     Loss:  0.48837342858314514\n",
      "Epoch:  58     i:  12     Loss:  0.46768733859062195\n",
      "Epoch:  58     i:  13     Loss:  0.5192360877990723\n",
      "Epoch:  58     i:  14     Loss:  0.38343995809555054\n",
      "Epoch:  58     i:  15     Loss:  0.6604083180427551\n",
      "Epoch:  58     i:  16     Loss:  0.5085325837135315\n",
      "Epoch:  58     i:  17     Loss:  0.33915895223617554\n",
      "Epoch:  58     i:  18     Loss:  0.5332331657409668\n",
      "Epoch:  58     i:  19     Loss:  0.5066213011741638\n",
      "Epoch:  58     i:  20     Loss:  0.40883493423461914\n",
      "Epoch:  58     i:  21     Loss:  0.339577317237854\n",
      "Epoch:  58     i:  22     Loss:  0.3393470048904419\n",
      "Epoch:  58     i:  23     Loss:  0.2677123248577118\n",
      "Epoch:  59     i:  0     Loss:  0.3993477523326874\n",
      "Epoch:  59     i:  1     Loss:  0.3740558922290802\n",
      "Epoch:  59     i:  2     Loss:  0.49871331453323364\n",
      "Epoch:  59     i:  3     Loss:  0.4945293664932251\n",
      "Epoch:  59     i:  4     Loss:  0.3585593104362488\n",
      "Epoch:  59     i:  5     Loss:  0.38479360938072205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  59     i:  6     Loss:  0.4826638996601105\n",
      "Epoch:  59     i:  7     Loss:  0.5183608531951904\n",
      "Epoch:  59     i:  8     Loss:  0.4810776710510254\n",
      "Epoch:  59     i:  9     Loss:  0.43188896775245667\n",
      "Epoch:  59     i:  10     Loss:  0.35343676805496216\n",
      "Epoch:  59     i:  11     Loss:  0.5053390264511108\n",
      "Epoch:  59     i:  12     Loss:  0.6043766736984253\n",
      "Epoch:  59     i:  13     Loss:  0.6167581081390381\n",
      "Epoch:  59     i:  14     Loss:  0.4132566750049591\n",
      "Epoch:  59     i:  15     Loss:  0.3849583566188812\n",
      "Epoch:  59     i:  16     Loss:  0.41991496086120605\n",
      "Epoch:  59     i:  17     Loss:  0.5327919125556946\n",
      "Epoch:  59     i:  18     Loss:  0.4868224859237671\n",
      "Epoch:  59     i:  19     Loss:  0.3407634496688843\n",
      "Epoch:  59     i:  20     Loss:  0.44851258397102356\n",
      "Epoch:  59     i:  21     Loss:  0.6200722455978394\n",
      "Epoch:  59     i:  22     Loss:  0.2643631100654602\n",
      "Epoch:  59     i:  23     Loss:  0.4952104687690735\n",
      "Epoch:  60     i:  0     Loss:  0.531448245048523\n",
      "Epoch:  60     i:  1     Loss:  0.6074138879776001\n",
      "Epoch:  60     i:  2     Loss:  0.34224504232406616\n",
      "Epoch:  60     i:  3     Loss:  0.3312516510486603\n",
      "Epoch:  60     i:  4     Loss:  0.3290218710899353\n",
      "Epoch:  60     i:  5     Loss:  0.5858137607574463\n",
      "Epoch:  60     i:  6     Loss:  0.5773929357528687\n",
      "Epoch:  60     i:  7     Loss:  0.5067704319953918\n",
      "Epoch:  60     i:  8     Loss:  0.43069136142730713\n",
      "Epoch:  60     i:  9     Loss:  0.32831257581710815\n",
      "Epoch:  60     i:  10     Loss:  0.48005568981170654\n",
      "Epoch:  60     i:  11     Loss:  0.5155656933784485\n",
      "Epoch:  60     i:  12     Loss:  0.3882027864456177\n",
      "Epoch:  60     i:  13     Loss:  0.5395380258560181\n",
      "Epoch:  60     i:  14     Loss:  0.42622077465057373\n",
      "Epoch:  60     i:  15     Loss:  0.29082241654396057\n",
      "Epoch:  60     i:  16     Loss:  0.42951101064682007\n",
      "Epoch:  60     i:  17     Loss:  0.494154155254364\n",
      "Epoch:  60     i:  18     Loss:  0.40254172682762146\n",
      "Epoch:  60     i:  19     Loss:  0.33958616852760315\n",
      "Epoch:  60     i:  20     Loss:  0.5386030673980713\n",
      "Epoch:  60     i:  21     Loss:  0.47579920291900635\n",
      "Epoch:  60     i:  22     Loss:  0.5043749809265137\n",
      "Epoch:  60     i:  23     Loss:  0.5157936811447144\n",
      "Epoch:  61     i:  0     Loss:  0.41234540939331055\n",
      "Epoch:  61     i:  1     Loss:  0.464042991399765\n",
      "Epoch:  61     i:  2     Loss:  0.44640225172042847\n",
      "Epoch:  61     i:  3     Loss:  0.5612926483154297\n",
      "Epoch:  61     i:  4     Loss:  0.5381923913955688\n",
      "Epoch:  61     i:  5     Loss:  0.38302168250083923\n",
      "Epoch:  61     i:  6     Loss:  0.3896467685699463\n",
      "Epoch:  61     i:  7     Loss:  0.47896647453308105\n",
      "Epoch:  61     i:  8     Loss:  0.368663489818573\n",
      "Epoch:  61     i:  9     Loss:  0.5961730480194092\n",
      "Epoch:  61     i:  10     Loss:  0.42583590745925903\n",
      "Epoch:  61     i:  11     Loss:  0.5393418073654175\n",
      "Epoch:  61     i:  12     Loss:  0.4601300358772278\n",
      "Epoch:  61     i:  13     Loss:  0.44206124544143677\n",
      "Epoch:  61     i:  14     Loss:  0.5973700284957886\n",
      "Epoch:  61     i:  15     Loss:  0.35133111476898193\n",
      "Epoch:  61     i:  16     Loss:  0.4517814517021179\n",
      "Epoch:  61     i:  17     Loss:  0.3701137900352478\n",
      "Epoch:  61     i:  18     Loss:  0.32639655470848083\n",
      "Epoch:  61     i:  19     Loss:  0.30133548378944397\n",
      "Epoch:  61     i:  20     Loss:  0.5740420818328857\n",
      "Epoch:  61     i:  21     Loss:  0.3513040542602539\n",
      "Epoch:  61     i:  22     Loss:  0.5961398482322693\n",
      "Epoch:  61     i:  23     Loss:  0.47497737407684326\n",
      "Epoch:  62     i:  0     Loss:  0.42699119448661804\n",
      "Epoch:  62     i:  1     Loss:  0.36918750405311584\n",
      "Epoch:  62     i:  2     Loss:  0.5116037130355835\n",
      "Epoch:  62     i:  3     Loss:  0.4538186192512512\n",
      "Epoch:  62     i:  4     Loss:  0.4238772392272949\n",
      "Epoch:  62     i:  5     Loss:  0.39980024099349976\n",
      "Epoch:  62     i:  6     Loss:  0.6278759837150574\n",
      "Epoch:  62     i:  7     Loss:  0.32697319984436035\n",
      "Epoch:  62     i:  8     Loss:  0.4786949157714844\n",
      "Epoch:  62     i:  9     Loss:  0.3025718331336975\n",
      "Epoch:  62     i:  10     Loss:  0.4857507348060608\n",
      "Epoch:  62     i:  11     Loss:  0.49910420179367065\n",
      "Epoch:  62     i:  12     Loss:  0.5314381718635559\n",
      "Epoch:  62     i:  13     Loss:  0.36801353096961975\n",
      "Epoch:  62     i:  14     Loss:  0.36249473690986633\n",
      "Epoch:  62     i:  15     Loss:  0.47517186403274536\n",
      "Epoch:  62     i:  16     Loss:  0.5196476578712463\n",
      "Epoch:  62     i:  17     Loss:  0.5628989934921265\n",
      "Epoch:  62     i:  18     Loss:  0.5351899266242981\n",
      "Epoch:  62     i:  19     Loss:  0.4039255380630493\n",
      "Epoch:  62     i:  20     Loss:  0.35798466205596924\n",
      "Epoch:  62     i:  21     Loss:  0.3937559723854065\n",
      "Epoch:  62     i:  22     Loss:  0.47505685687065125\n",
      "Epoch:  62     i:  23     Loss:  0.6085075736045837\n",
      "Epoch:  63     i:  0     Loss:  0.5347697734832764\n",
      "Epoch:  63     i:  1     Loss:  0.4216042160987854\n",
      "Epoch:  63     i:  2     Loss:  0.4679676592350006\n",
      "Epoch:  63     i:  3     Loss:  0.3278685212135315\n",
      "Epoch:  63     i:  4     Loss:  0.44697660207748413\n",
      "Epoch:  63     i:  5     Loss:  0.3539275527000427\n",
      "Epoch:  63     i:  6     Loss:  0.5255351066589355\n",
      "Epoch:  63     i:  7     Loss:  0.37622952461242676\n",
      "Epoch:  63     i:  8     Loss:  0.3727530539035797\n",
      "Epoch:  63     i:  9     Loss:  0.4657064378261566\n",
      "Epoch:  63     i:  10     Loss:  0.4720039367675781\n",
      "Epoch:  63     i:  11     Loss:  0.4713969826698303\n",
      "Epoch:  63     i:  12     Loss:  0.5047290325164795\n",
      "Epoch:  63     i:  13     Loss:  0.3162856996059418\n",
      "Epoch:  63     i:  14     Loss:  0.4519754648208618\n",
      "Epoch:  63     i:  15     Loss:  0.5393311977386475\n",
      "Epoch:  63     i:  16     Loss:  0.3278455138206482\n",
      "Epoch:  63     i:  17     Loss:  0.5060018301010132\n",
      "Epoch:  63     i:  18     Loss:  0.5400922894477844\n",
      "Epoch:  63     i:  19     Loss:  0.4451833963394165\n",
      "Epoch:  63     i:  20     Loss:  0.5889627933502197\n",
      "Epoch:  63     i:  21     Loss:  0.2797123193740845\n",
      "Epoch:  63     i:  22     Loss:  0.5582185983657837\n",
      "Epoch:  63     i:  23     Loss:  0.516832172870636\n",
      "Epoch:  64     i:  0     Loss:  0.40216028690338135\n",
      "Epoch:  64     i:  1     Loss:  0.3738751709461212\n",
      "Epoch:  64     i:  2     Loss:  0.3450879156589508\n",
      "Epoch:  64     i:  3     Loss:  0.5438411831855774\n",
      "Epoch:  64     i:  4     Loss:  0.3827279210090637\n",
      "Epoch:  64     i:  5     Loss:  0.38859495520591736\n",
      "Epoch:  64     i:  6     Loss:  0.5050635933876038\n",
      "Epoch:  64     i:  7     Loss:  0.45994293689727783\n",
      "Epoch:  64     i:  8     Loss:  0.4675520658493042\n",
      "Epoch:  64     i:  9     Loss:  0.5004960298538208\n",
      "Epoch:  64     i:  10     Loss:  0.34408465027809143\n",
      "Epoch:  64     i:  11     Loss:  0.3258007764816284\n",
      "Epoch:  64     i:  12     Loss:  0.4818061590194702\n",
      "Epoch:  64     i:  13     Loss:  0.5332902073860168\n",
      "Epoch:  64     i:  14     Loss:  0.5665895342826843\n",
      "Epoch:  64     i:  15     Loss:  0.456703245639801\n",
      "Epoch:  64     i:  16     Loss:  0.5431606769561768\n",
      "Epoch:  64     i:  17     Loss:  0.36902761459350586\n",
      "Epoch:  64     i:  18     Loss:  0.4851992130279541\n",
      "Epoch:  64     i:  19     Loss:  0.47580692172050476\n",
      "Epoch:  64     i:  20     Loss:  0.3088338375091553\n",
      "Epoch:  64     i:  21     Loss:  0.4743554890155792\n",
      "Epoch:  64     i:  22     Loss:  0.5678200125694275\n",
      "Epoch:  64     i:  23     Loss:  0.506155252456665\n",
      "Epoch:  65     i:  0     Loss:  0.5506615042686462\n",
      "Epoch:  65     i:  1     Loss:  0.3329162895679474\n",
      "Epoch:  65     i:  2     Loss:  0.44311121106147766\n",
      "Epoch:  65     i:  3     Loss:  0.38427498936653137\n",
      "Epoch:  65     i:  4     Loss:  0.42386192083358765\n",
      "Epoch:  65     i:  5     Loss:  0.4327056407928467\n",
      "Epoch:  65     i:  6     Loss:  0.5459735989570618\n",
      "Epoch:  65     i:  7     Loss:  0.5350239872932434\n",
      "Epoch:  65     i:  8     Loss:  0.4488388001918793\n",
      "Epoch:  65     i:  9     Loss:  0.4021594524383545\n",
      "Epoch:  65     i:  10     Loss:  0.6685963273048401\n",
      "Epoch:  65     i:  11     Loss:  0.36474454402923584\n",
      "Epoch:  65     i:  12     Loss:  0.6435388326644897\n",
      "Epoch:  65     i:  13     Loss:  0.5011578798294067\n",
      "Epoch:  65     i:  14     Loss:  0.3430759608745575\n",
      "Epoch:  65     i:  15     Loss:  0.41966712474823\n",
      "Epoch:  65     i:  16     Loss:  0.43292415142059326\n",
      "Epoch:  65     i:  17     Loss:  0.4441510736942291\n",
      "Epoch:  65     i:  18     Loss:  0.5781446099281311\n",
      "Epoch:  65     i:  19     Loss:  0.5475108623504639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  65     i:  20     Loss:  0.3383169174194336\n",
      "Epoch:  65     i:  21     Loss:  0.3592738211154938\n",
      "Epoch:  65     i:  22     Loss:  0.31613677740097046\n",
      "Epoch:  65     i:  23     Loss:  0.2605361044406891\n",
      "Epoch:  66     i:  0     Loss:  0.46231573820114136\n",
      "Epoch:  66     i:  1     Loss:  0.3186165988445282\n",
      "Epoch:  66     i:  2     Loss:  0.36025434732437134\n",
      "Epoch:  66     i:  3     Loss:  0.32536184787750244\n",
      "Epoch:  66     i:  4     Loss:  0.5316886901855469\n",
      "Epoch:  66     i:  5     Loss:  0.41937240958213806\n",
      "Epoch:  66     i:  6     Loss:  0.44886064529418945\n",
      "Epoch:  66     i:  7     Loss:  0.4085463285446167\n",
      "Epoch:  66     i:  8     Loss:  0.5249372720718384\n",
      "Epoch:  66     i:  9     Loss:  0.5214969515800476\n",
      "Epoch:  66     i:  10     Loss:  0.7115475535392761\n",
      "Epoch:  66     i:  11     Loss:  0.35289913415908813\n",
      "Epoch:  66     i:  12     Loss:  0.6522987484931946\n",
      "Epoch:  66     i:  13     Loss:  0.5295670628547668\n",
      "Epoch:  66     i:  14     Loss:  0.4559049904346466\n",
      "Epoch:  66     i:  15     Loss:  0.4307281970977783\n",
      "Epoch:  66     i:  16     Loss:  0.40394702553749084\n",
      "Epoch:  66     i:  17     Loss:  0.42946892976760864\n",
      "Epoch:  66     i:  18     Loss:  0.42176759243011475\n",
      "Epoch:  66     i:  19     Loss:  0.32906919717788696\n",
      "Epoch:  66     i:  20     Loss:  0.4060031771659851\n",
      "Epoch:  66     i:  21     Loss:  0.4530405104160309\n",
      "Epoch:  66     i:  22     Loss:  0.40951624512672424\n",
      "Epoch:  66     i:  23     Loss:  0.4368986487388611\n",
      "Epoch:  67     i:  0     Loss:  0.5615782141685486\n",
      "Epoch:  67     i:  1     Loss:  0.303049772977829\n",
      "Epoch:  67     i:  2     Loss:  0.47972923517227173\n",
      "Epoch:  67     i:  3     Loss:  0.3390393853187561\n",
      "Epoch:  67     i:  4     Loss:  0.3817683458328247\n",
      "Epoch:  67     i:  5     Loss:  0.3211086094379425\n",
      "Epoch:  67     i:  6     Loss:  0.4885597825050354\n",
      "Epoch:  67     i:  7     Loss:  0.5215598344802856\n",
      "Epoch:  67     i:  8     Loss:  0.3376041650772095\n",
      "Epoch:  67     i:  9     Loss:  0.49961116909980774\n",
      "Epoch:  67     i:  10     Loss:  0.4030533730983734\n",
      "Epoch:  67     i:  11     Loss:  0.39815646409988403\n",
      "Epoch:  67     i:  12     Loss:  0.45963114500045776\n",
      "Epoch:  67     i:  13     Loss:  0.6125047206878662\n",
      "Epoch:  67     i:  14     Loss:  0.43273159861564636\n",
      "Epoch:  67     i:  15     Loss:  0.37916073203086853\n",
      "Epoch:  67     i:  16     Loss:  0.5059998631477356\n",
      "Epoch:  67     i:  17     Loss:  0.5110817551612854\n",
      "Epoch:  67     i:  18     Loss:  0.645001232624054\n",
      "Epoch:  67     i:  19     Loss:  0.6498192548751831\n",
      "Epoch:  67     i:  20     Loss:  0.26185375452041626\n",
      "Epoch:  67     i:  21     Loss:  0.4550391137599945\n",
      "Epoch:  67     i:  22     Loss:  0.38072165846824646\n",
      "Epoch:  67     i:  23     Loss:  0.42527201771736145\n",
      "Epoch:  68     i:  0     Loss:  0.39800897240638733\n",
      "Epoch:  68     i:  1     Loss:  0.677361786365509\n",
      "Epoch:  68     i:  2     Loss:  0.5120008587837219\n",
      "Epoch:  68     i:  3     Loss:  0.5084786415100098\n",
      "Epoch:  68     i:  4     Loss:  0.38453027606010437\n",
      "Epoch:  68     i:  5     Loss:  0.4553997218608856\n",
      "Epoch:  68     i:  6     Loss:  0.40174978971481323\n",
      "Epoch:  68     i:  7     Loss:  0.30331453680992126\n",
      "Epoch:  68     i:  8     Loss:  0.5118258595466614\n",
      "Epoch:  68     i:  9     Loss:  0.3999066948890686\n",
      "Epoch:  68     i:  10     Loss:  0.5142427086830139\n",
      "Epoch:  68     i:  11     Loss:  0.39034613966941833\n",
      "Epoch:  68     i:  12     Loss:  0.4327050745487213\n",
      "Epoch:  68     i:  13     Loss:  0.5617867112159729\n",
      "Epoch:  68     i:  14     Loss:  0.314250648021698\n",
      "Epoch:  68     i:  15     Loss:  0.3623027801513672\n",
      "Epoch:  68     i:  16     Loss:  0.4534672200679779\n",
      "Epoch:  68     i:  17     Loss:  0.37004080414772034\n",
      "Epoch:  68     i:  18     Loss:  0.5666627883911133\n",
      "Epoch:  68     i:  19     Loss:  0.49614977836608887\n",
      "Epoch:  68     i:  20     Loss:  0.2948009967803955\n",
      "Epoch:  68     i:  21     Loss:  0.5691768527030945\n",
      "Epoch:  68     i:  22     Loss:  0.43005499243736267\n",
      "Epoch:  68     i:  23     Loss:  0.49987930059432983\n",
      "Epoch:  69     i:  0     Loss:  0.43354177474975586\n",
      "Epoch:  69     i:  1     Loss:  0.5191025733947754\n",
      "Epoch:  69     i:  2     Loss:  0.3980720043182373\n",
      "Epoch:  69     i:  3     Loss:  0.3833460807800293\n",
      "Epoch:  69     i:  4     Loss:  0.4735058844089508\n",
      "Epoch:  69     i:  5     Loss:  0.6305957436561584\n",
      "Epoch:  69     i:  6     Loss:  0.4154297709465027\n",
      "Epoch:  69     i:  7     Loss:  0.3577861487865448\n",
      "Epoch:  69     i:  8     Loss:  0.528396487236023\n",
      "Epoch:  69     i:  9     Loss:  0.40724697709083557\n",
      "Epoch:  69     i:  10     Loss:  0.4842180907726288\n",
      "Epoch:  69     i:  11     Loss:  0.41117048263549805\n",
      "Epoch:  69     i:  12     Loss:  0.3094836473464966\n",
      "Epoch:  69     i:  13     Loss:  0.4632723927497864\n",
      "Epoch:  69     i:  14     Loss:  0.4354318082332611\n",
      "Epoch:  69     i:  15     Loss:  0.43488359451293945\n",
      "Epoch:  69     i:  16     Loss:  0.46380528807640076\n",
      "Epoch:  69     i:  17     Loss:  0.4702552556991577\n",
      "Epoch:  69     i:  18     Loss:  0.332036554813385\n",
      "Epoch:  69     i:  19     Loss:  0.448763370513916\n",
      "Epoch:  69     i:  20     Loss:  0.5085110664367676\n",
      "Epoch:  69     i:  21     Loss:  0.5799074172973633\n",
      "Epoch:  69     i:  22     Loss:  0.5571962594985962\n",
      "Epoch:  69     i:  23     Loss:  0.2687612473964691\n",
      "Epoch:  70     i:  0     Loss:  0.4956202805042267\n",
      "Epoch:  70     i:  1     Loss:  0.34521251916885376\n",
      "Epoch:  70     i:  2     Loss:  0.6402367949485779\n",
      "Epoch:  70     i:  3     Loss:  0.44240182638168335\n",
      "Epoch:  70     i:  4     Loss:  0.586772620677948\n",
      "Epoch:  70     i:  5     Loss:  0.5986331105232239\n",
      "Epoch:  70     i:  6     Loss:  0.4177841246128082\n",
      "Epoch:  70     i:  7     Loss:  0.4927625358104706\n",
      "Epoch:  70     i:  8     Loss:  0.6194684505462646\n",
      "Epoch:  70     i:  9     Loss:  0.3110838830471039\n",
      "Epoch:  70     i:  10     Loss:  0.3352000117301941\n",
      "Epoch:  70     i:  11     Loss:  0.4085788130760193\n",
      "Epoch:  70     i:  12     Loss:  0.39166268706321716\n",
      "Epoch:  70     i:  13     Loss:  0.40858256816864014\n",
      "Epoch:  70     i:  14     Loss:  0.3645153343677521\n",
      "Epoch:  70     i:  15     Loss:  0.336096853017807\n",
      "Epoch:  70     i:  16     Loss:  0.5035879015922546\n",
      "Epoch:  70     i:  17     Loss:  0.30279770493507385\n",
      "Epoch:  70     i:  18     Loss:  0.43252354860305786\n",
      "Epoch:  70     i:  19     Loss:  0.3782247006893158\n",
      "Epoch:  70     i:  20     Loss:  0.494638592004776\n",
      "Epoch:  70     i:  21     Loss:  0.38431963324546814\n",
      "Epoch:  70     i:  22     Loss:  0.5557211637496948\n",
      "Epoch:  70     i:  23     Loss:  0.4971825182437897\n",
      "Epoch:  71     i:  0     Loss:  0.544603705406189\n",
      "Epoch:  71     i:  1     Loss:  0.3997958302497864\n",
      "Epoch:  71     i:  2     Loss:  0.4640694260597229\n",
      "Epoch:  71     i:  3     Loss:  0.32615581154823303\n",
      "Epoch:  71     i:  4     Loss:  0.39126479625701904\n",
      "Epoch:  71     i:  5     Loss:  0.3286808729171753\n",
      "Epoch:  71     i:  6     Loss:  0.5424017906188965\n",
      "Epoch:  71     i:  7     Loss:  0.5636730194091797\n",
      "Epoch:  71     i:  8     Loss:  0.42119595408439636\n",
      "Epoch:  71     i:  9     Loss:  0.5283253788948059\n",
      "Epoch:  71     i:  10     Loss:  0.4361225962638855\n",
      "Epoch:  71     i:  11     Loss:  0.45344260334968567\n",
      "Epoch:  71     i:  12     Loss:  0.5213077664375305\n",
      "Epoch:  71     i:  13     Loss:  0.6337326169013977\n",
      "Epoch:  71     i:  14     Loss:  0.3997798562049866\n",
      "Epoch:  71     i:  15     Loss:  0.4568883180618286\n",
      "Epoch:  71     i:  16     Loss:  0.32646727561950684\n",
      "Epoch:  71     i:  17     Loss:  0.3226703405380249\n",
      "Epoch:  71     i:  18     Loss:  0.4290692210197449\n",
      "Epoch:  71     i:  19     Loss:  0.44108662009239197\n",
      "Epoch:  71     i:  20     Loss:  0.6109293699264526\n",
      "Epoch:  71     i:  21     Loss:  0.3670327067375183\n",
      "Epoch:  71     i:  22     Loss:  0.5047074556350708\n",
      "Epoch:  71     i:  23     Loss:  0.36903584003448486\n",
      "Epoch:  72     i:  0     Loss:  0.3605901896953583\n",
      "Epoch:  72     i:  1     Loss:  0.3905518352985382\n",
      "Epoch:  72     i:  2     Loss:  0.43410515785217285\n",
      "Epoch:  72     i:  3     Loss:  0.43050891160964966\n",
      "Epoch:  72     i:  4     Loss:  0.3003266751766205\n",
      "Epoch:  72     i:  5     Loss:  0.3431767523288727\n",
      "Epoch:  72     i:  6     Loss:  0.4909779131412506\n",
      "Epoch:  72     i:  7     Loss:  0.30168014764785767\n",
      "Epoch:  72     i:  8     Loss:  0.4873313903808594\n",
      "Epoch:  72     i:  9     Loss:  0.6465345025062561\n",
      "Epoch:  72     i:  10     Loss:  0.4626154899597168\n",
      "Epoch:  72     i:  11     Loss:  0.4993954598903656\n",
      "Epoch:  72     i:  12     Loss:  0.47952601313591003\n",
      "Epoch:  72     i:  13     Loss:  0.3555704951286316\n",
      "Epoch:  72     i:  14     Loss:  0.3988550901412964\n",
      "Epoch:  72     i:  15     Loss:  0.5140579342842102\n",
      "Epoch:  72     i:  16     Loss:  0.42839333415031433\n",
      "Epoch:  72     i:  17     Loss:  0.44826093316078186\n",
      "Epoch:  72     i:  18     Loss:  0.4264821708202362\n",
      "Epoch:  72     i:  19     Loss:  0.5326237082481384\n",
      "Epoch:  72     i:  20     Loss:  0.35693156719207764\n",
      "Epoch:  72     i:  21     Loss:  0.4877864122390747\n",
      "Epoch:  72     i:  22     Loss:  0.5597659945487976\n",
      "Epoch:  72     i:  23     Loss:  0.631635844707489\n",
      "Epoch:  73     i:  0     Loss:  0.5305768251419067\n",
      "Epoch:  73     i:  1     Loss:  0.29221078753471375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  73     i:  2     Loss:  0.4596100151538849\n",
      "Epoch:  73     i:  3     Loss:  0.4244232773780823\n",
      "Epoch:  73     i:  4     Loss:  0.41975438594818115\n",
      "Epoch:  73     i:  5     Loss:  0.3752111792564392\n",
      "Epoch:  73     i:  6     Loss:  0.3514478802680969\n",
      "Epoch:  73     i:  7     Loss:  0.5455106496810913\n",
      "Epoch:  73     i:  8     Loss:  0.5760234594345093\n",
      "Epoch:  73     i:  9     Loss:  0.39577293395996094\n",
      "Epoch:  73     i:  10     Loss:  0.47538667917251587\n",
      "Epoch:  73     i:  11     Loss:  0.3595598042011261\n",
      "Epoch:  73     i:  12     Loss:  0.3137723505496979\n",
      "Epoch:  73     i:  13     Loss:  0.5343092083930969\n",
      "Epoch:  73     i:  14     Loss:  0.45267951488494873\n",
      "Epoch:  73     i:  15     Loss:  0.3975672721862793\n",
      "Epoch:  73     i:  16     Loss:  0.49214208126068115\n",
      "Epoch:  73     i:  17     Loss:  0.3953072428703308\n",
      "Epoch:  73     i:  18     Loss:  0.4623630940914154\n",
      "Epoch:  73     i:  19     Loss:  0.538059651851654\n",
      "Epoch:  73     i:  20     Loss:  0.6718968152999878\n",
      "Epoch:  73     i:  21     Loss:  0.39835602045059204\n",
      "Epoch:  73     i:  22     Loss:  0.45720118284225464\n",
      "Epoch:  73     i:  23     Loss:  0.27950501441955566\n",
      "Epoch:  74     i:  0     Loss:  0.4263707399368286\n",
      "Epoch:  74     i:  1     Loss:  0.40446943044662476\n",
      "Epoch:  74     i:  2     Loss:  0.43743547797203064\n",
      "Epoch:  74     i:  3     Loss:  0.39181238412857056\n",
      "Epoch:  74     i:  4     Loss:  0.4226260781288147\n",
      "Epoch:  74     i:  5     Loss:  0.4346199631690979\n",
      "Epoch:  74     i:  6     Loss:  0.4327922463417053\n",
      "Epoch:  74     i:  7     Loss:  0.4674585163593292\n",
      "Epoch:  74     i:  8     Loss:  0.5172590613365173\n",
      "Epoch:  74     i:  9     Loss:  0.4556659460067749\n",
      "Epoch:  74     i:  10     Loss:  0.4973888397216797\n",
      "Epoch:  74     i:  11     Loss:  0.44005250930786133\n",
      "Epoch:  74     i:  12     Loss:  0.22091537714004517\n",
      "Epoch:  74     i:  13     Loss:  0.35315340757369995\n",
      "Epoch:  74     i:  14     Loss:  0.37944650650024414\n",
      "Epoch:  74     i:  15     Loss:  0.6056717038154602\n",
      "Epoch:  74     i:  16     Loss:  0.4144752025604248\n",
      "Epoch:  74     i:  17     Loss:  0.4975112974643707\n",
      "Epoch:  74     i:  18     Loss:  0.4902060329914093\n",
      "Epoch:  74     i:  19     Loss:  0.48210155963897705\n",
      "Epoch:  74     i:  20     Loss:  0.5592417120933533\n",
      "Epoch:  74     i:  21     Loss:  0.5314528346061707\n",
      "Epoch:  74     i:  22     Loss:  0.3659551739692688\n",
      "Epoch:  74     i:  23     Loss:  0.5047922134399414\n",
      "Epoch:  75     i:  0     Loss:  0.671241044998169\n",
      "Epoch:  75     i:  1     Loss:  0.2706350088119507\n",
      "Epoch:  75     i:  2     Loss:  0.5328813791275024\n",
      "Epoch:  75     i:  3     Loss:  0.3595176339149475\n",
      "Epoch:  75     i:  4     Loss:  0.4540182948112488\n",
      "Epoch:  75     i:  5     Loss:  0.5311256051063538\n",
      "Epoch:  75     i:  6     Loss:  0.5107144117355347\n",
      "Epoch:  75     i:  7     Loss:  0.33817875385284424\n",
      "Epoch:  75     i:  8     Loss:  0.825630247592926\n",
      "Epoch:  75     i:  9     Loss:  0.29198360443115234\n",
      "Epoch:  75     i:  10     Loss:  0.4744296669960022\n",
      "Epoch:  75     i:  11     Loss:  0.3687039613723755\n",
      "Epoch:  75     i:  12     Loss:  0.366001158952713\n",
      "Epoch:  75     i:  13     Loss:  0.35008418560028076\n",
      "Epoch:  75     i:  14     Loss:  0.39357584714889526\n",
      "Epoch:  75     i:  15     Loss:  0.4012863337993622\n",
      "Epoch:  75     i:  16     Loss:  0.4243803024291992\n",
      "Epoch:  75     i:  17     Loss:  0.4979519844055176\n",
      "Epoch:  75     i:  18     Loss:  0.505033552646637\n",
      "Epoch:  75     i:  19     Loss:  0.4196445047855377\n",
      "Epoch:  75     i:  20     Loss:  0.3951641917228699\n",
      "Epoch:  75     i:  21     Loss:  0.43373024463653564\n",
      "Epoch:  75     i:  22     Loss:  0.4754551947116852\n",
      "Epoch:  75     i:  23     Loss:  0.4127313792705536\n",
      "Epoch:  76     i:  0     Loss:  0.3720056116580963\n",
      "Epoch:  76     i:  1     Loss:  0.5076859593391418\n",
      "Epoch:  76     i:  2     Loss:  0.33743464946746826\n",
      "Epoch:  76     i:  3     Loss:  0.4633229374885559\n",
      "Epoch:  76     i:  4     Loss:  0.33021417260169983\n",
      "Epoch:  76     i:  5     Loss:  0.37851035594940186\n",
      "Epoch:  76     i:  6     Loss:  0.42972221970558167\n",
      "Epoch:  76     i:  7     Loss:  0.5503087043762207\n",
      "Epoch:  76     i:  8     Loss:  0.43278709053993225\n",
      "Epoch:  76     i:  9     Loss:  0.360889196395874\n",
      "Epoch:  76     i:  10     Loss:  0.40699005126953125\n",
      "Epoch:  76     i:  11     Loss:  0.4221764802932739\n",
      "Epoch:  76     i:  12     Loss:  0.39263230562210083\n",
      "Epoch:  76     i:  13     Loss:  0.5944036245346069\n",
      "Epoch:  76     i:  14     Loss:  0.37633147835731506\n",
      "Epoch:  76     i:  15     Loss:  0.4656669795513153\n",
      "Epoch:  76     i:  16     Loss:  0.49712851643562317\n",
      "Epoch:  76     i:  17     Loss:  0.4640832543373108\n",
      "Epoch:  76     i:  18     Loss:  0.4234314262866974\n",
      "Epoch:  76     i:  19     Loss:  0.4012555480003357\n",
      "Epoch:  76     i:  20     Loss:  0.6017470359802246\n",
      "Epoch:  76     i:  21     Loss:  0.5331441164016724\n",
      "Epoch:  76     i:  22     Loss:  0.458261638879776\n",
      "Epoch:  76     i:  23     Loss:  0.4339585602283478\n",
      "Epoch:  77     i:  0     Loss:  0.5701022744178772\n",
      "Epoch:  77     i:  1     Loss:  0.37886008620262146\n",
      "Epoch:  77     i:  2     Loss:  0.531500518321991\n",
      "Epoch:  77     i:  3     Loss:  0.40740659832954407\n",
      "Epoch:  77     i:  4     Loss:  0.365938276052475\n",
      "Epoch:  77     i:  5     Loss:  0.43518349528312683\n",
      "Epoch:  77     i:  6     Loss:  0.38790351152420044\n",
      "Epoch:  77     i:  7     Loss:  0.4150351881980896\n",
      "Epoch:  77     i:  8     Loss:  0.338204026222229\n",
      "Epoch:  77     i:  9     Loss:  0.46673035621643066\n",
      "Epoch:  77     i:  10     Loss:  0.4237760007381439\n",
      "Epoch:  77     i:  11     Loss:  0.40435442328453064\n",
      "Epoch:  77     i:  12     Loss:  0.3767198324203491\n",
      "Epoch:  77     i:  13     Loss:  0.4965950846672058\n",
      "Epoch:  77     i:  14     Loss:  0.4761095643043518\n",
      "Epoch:  77     i:  15     Loss:  0.4675220251083374\n",
      "Epoch:  77     i:  16     Loss:  0.5975127220153809\n",
      "Epoch:  77     i:  17     Loss:  0.38171109557151794\n",
      "Epoch:  77     i:  18     Loss:  0.4601624011993408\n",
      "Epoch:  77     i:  19     Loss:  0.44808298349380493\n",
      "Epoch:  77     i:  20     Loss:  0.4952853322029114\n",
      "Epoch:  77     i:  21     Loss:  0.4189137816429138\n",
      "Epoch:  77     i:  22     Loss:  0.42719370126724243\n",
      "Epoch:  77     i:  23     Loss:  0.47606074810028076\n",
      "Epoch:  78     i:  0     Loss:  0.545215368270874\n",
      "Epoch:  78     i:  1     Loss:  0.38567787408828735\n",
      "Epoch:  78     i:  2     Loss:  0.35833996534347534\n",
      "Epoch:  78     i:  3     Loss:  0.34422072768211365\n",
      "Epoch:  78     i:  4     Loss:  0.3816731870174408\n",
      "Epoch:  78     i:  5     Loss:  0.49012619256973267\n",
      "Epoch:  78     i:  6     Loss:  0.41586577892303467\n",
      "Epoch:  78     i:  7     Loss:  0.5145947933197021\n",
      "Epoch:  78     i:  8     Loss:  0.3935677707195282\n",
      "Epoch:  78     i:  9     Loss:  0.3564245104789734\n",
      "Epoch:  78     i:  10     Loss:  0.5446869134902954\n",
      "Epoch:  78     i:  11     Loss:  0.48264336585998535\n",
      "Epoch:  78     i:  12     Loss:  0.643928587436676\n",
      "Epoch:  78     i:  13     Loss:  0.45988398790359497\n",
      "Epoch:  78     i:  14     Loss:  0.3085070252418518\n",
      "Epoch:  78     i:  15     Loss:  0.3798726201057434\n",
      "Epoch:  78     i:  16     Loss:  0.4127359390258789\n",
      "Epoch:  78     i:  17     Loss:  0.49794572591781616\n",
      "Epoch:  78     i:  18     Loss:  0.483509361743927\n",
      "Epoch:  78     i:  19     Loss:  0.49120569229125977\n",
      "Epoch:  78     i:  20     Loss:  0.4675717353820801\n",
      "Epoch:  78     i:  21     Loss:  0.3057841658592224\n",
      "Epoch:  78     i:  22     Loss:  0.4845714569091797\n",
      "Epoch:  78     i:  23     Loss:  0.513437032699585\n",
      "Epoch:  79     i:  0     Loss:  0.3636043667793274\n",
      "Epoch:  79     i:  1     Loss:  0.3620174527168274\n",
      "Epoch:  79     i:  2     Loss:  0.4888255298137665\n",
      "Epoch:  79     i:  3     Loss:  0.4236766993999481\n",
      "Epoch:  79     i:  4     Loss:  0.34382569789886475\n",
      "Epoch:  79     i:  5     Loss:  0.4473544955253601\n",
      "Epoch:  79     i:  6     Loss:  0.4650658369064331\n",
      "Epoch:  79     i:  7     Loss:  0.43485140800476074\n",
      "Epoch:  79     i:  8     Loss:  0.45149484276771545\n",
      "Epoch:  79     i:  9     Loss:  0.4026656746864319\n",
      "Epoch:  79     i:  10     Loss:  0.5667852163314819\n",
      "Epoch:  79     i:  11     Loss:  0.4946650564670563\n",
      "Epoch:  79     i:  12     Loss:  0.6340523362159729\n",
      "Epoch:  79     i:  13     Loss:  0.3642250597476959\n",
      "Epoch:  79     i:  14     Loss:  0.50904381275177\n",
      "Epoch:  79     i:  15     Loss:  0.49485886096954346\n",
      "Epoch:  79     i:  16     Loss:  0.39635536074638367\n",
      "Epoch:  79     i:  17     Loss:  0.4210049510002136\n",
      "Epoch:  79     i:  18     Loss:  0.4491901397705078\n",
      "Epoch:  79     i:  19     Loss:  0.46325308084487915\n",
      "Epoch:  79     i:  20     Loss:  0.45405110716819763\n",
      "Epoch:  79     i:  21     Loss:  0.33310720324516296\n",
      "Epoch:  79     i:  22     Loss:  0.43825235962867737\n",
      "Epoch:  79     i:  23     Loss:  0.45301371812820435\n",
      "Epoch:  80     i:  0     Loss:  0.48544612526893616\n",
      "Epoch:  80     i:  1     Loss:  0.42256447672843933\n",
      "Epoch:  80     i:  2     Loss:  0.485698401927948\n",
      "Epoch:  80     i:  3     Loss:  0.42328283190727234\n",
      "Epoch:  80     i:  4     Loss:  0.5703526735305786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  80     i:  5     Loss:  0.4925936758518219\n",
      "Epoch:  80     i:  6     Loss:  0.33115872740745544\n",
      "Epoch:  80     i:  7     Loss:  0.638596773147583\n",
      "Epoch:  80     i:  8     Loss:  0.38189828395843506\n",
      "Epoch:  80     i:  9     Loss:  0.30295228958129883\n",
      "Epoch:  80     i:  10     Loss:  0.35845234990119934\n",
      "Epoch:  80     i:  11     Loss:  0.46949440240859985\n",
      "Epoch:  80     i:  12     Loss:  0.5562240481376648\n",
      "Epoch:  80     i:  13     Loss:  0.473563015460968\n",
      "Epoch:  80     i:  14     Loss:  0.5062768459320068\n",
      "Epoch:  80     i:  15     Loss:  0.5109841823577881\n",
      "Epoch:  80     i:  16     Loss:  0.3078427016735077\n",
      "Epoch:  80     i:  17     Loss:  0.42352917790412903\n",
      "Epoch:  80     i:  18     Loss:  0.3360055685043335\n",
      "Epoch:  80     i:  19     Loss:  0.32543304562568665\n",
      "Epoch:  80     i:  20     Loss:  0.5016796588897705\n",
      "Epoch:  80     i:  21     Loss:  0.39781883358955383\n",
      "Epoch:  80     i:  22     Loss:  0.4537539780139923\n",
      "Epoch:  80     i:  23     Loss:  0.47895896434783936\n",
      "Epoch:  81     i:  0     Loss:  0.5321260094642639\n",
      "Epoch:  81     i:  1     Loss:  0.38992840051651\n",
      "Epoch:  81     i:  2     Loss:  0.566957950592041\n",
      "Epoch:  81     i:  3     Loss:  0.49471423029899597\n",
      "Epoch:  81     i:  4     Loss:  0.3674364984035492\n",
      "Epoch:  81     i:  5     Loss:  0.4118523597717285\n",
      "Epoch:  81     i:  6     Loss:  0.3945504426956177\n",
      "Epoch:  81     i:  7     Loss:  0.4055352807044983\n",
      "Epoch:  81     i:  8     Loss:  0.5978477001190186\n",
      "Epoch:  81     i:  9     Loss:  0.5388506650924683\n",
      "Epoch:  81     i:  10     Loss:  0.48436427116394043\n",
      "Epoch:  81     i:  11     Loss:  0.525661826133728\n",
      "Epoch:  81     i:  12     Loss:  0.299430251121521\n",
      "Epoch:  81     i:  13     Loss:  0.4176953434944153\n",
      "Epoch:  81     i:  14     Loss:  0.43699464201927185\n",
      "Epoch:  81     i:  15     Loss:  0.40181154012680054\n",
      "Epoch:  81     i:  16     Loss:  0.5883313417434692\n",
      "Epoch:  81     i:  17     Loss:  0.49315011501312256\n",
      "Epoch:  81     i:  18     Loss:  0.4202032685279846\n",
      "Epoch:  81     i:  19     Loss:  0.4157693684101105\n",
      "Epoch:  81     i:  20     Loss:  0.25235021114349365\n",
      "Epoch:  81     i:  21     Loss:  0.4093170762062073\n",
      "Epoch:  81     i:  22     Loss:  0.4122900664806366\n",
      "Epoch:  81     i:  23     Loss:  0.3650875389575958\n",
      "Epoch:  82     i:  0     Loss:  0.2865513563156128\n",
      "Epoch:  82     i:  1     Loss:  0.3431249260902405\n",
      "Epoch:  82     i:  2     Loss:  0.5201349854469299\n",
      "Epoch:  82     i:  3     Loss:  0.5127315521240234\n",
      "Epoch:  82     i:  4     Loss:  0.48830679059028625\n",
      "Epoch:  82     i:  5     Loss:  0.5169406533241272\n",
      "Epoch:  82     i:  6     Loss:  0.2552638351917267\n",
      "Epoch:  82     i:  7     Loss:  0.40123552083969116\n",
      "Epoch:  82     i:  8     Loss:  0.3541516959667206\n",
      "Epoch:  82     i:  9     Loss:  0.5446597933769226\n",
      "Epoch:  82     i:  10     Loss:  0.5396367907524109\n",
      "Epoch:  82     i:  11     Loss:  0.4843176603317261\n",
      "Epoch:  82     i:  12     Loss:  0.46513310074806213\n",
      "Epoch:  82     i:  13     Loss:  0.4097555875778198\n",
      "Epoch:  82     i:  14     Loss:  0.31897634267807007\n",
      "Epoch:  82     i:  15     Loss:  0.4535576105117798\n",
      "Epoch:  82     i:  16     Loss:  0.4449004530906677\n",
      "Epoch:  82     i:  17     Loss:  0.5612479448318481\n",
      "Epoch:  82     i:  18     Loss:  0.46700194478034973\n",
      "Epoch:  82     i:  19     Loss:  0.5381107330322266\n",
      "Epoch:  82     i:  20     Loss:  0.5230582356452942\n",
      "Epoch:  82     i:  21     Loss:  0.4409117102622986\n",
      "Epoch:  82     i:  22     Loss:  0.41680172085762024\n",
      "Epoch:  82     i:  23     Loss:  0.31131815910339355\n",
      "Epoch:  83     i:  0     Loss:  0.5224369168281555\n",
      "Epoch:  83     i:  1     Loss:  0.3527507781982422\n",
      "Epoch:  83     i:  2     Loss:  0.5701406002044678\n",
      "Epoch:  83     i:  3     Loss:  0.4982273578643799\n",
      "Epoch:  83     i:  4     Loss:  0.30910900235176086\n",
      "Epoch:  83     i:  5     Loss:  0.40685999393463135\n",
      "Epoch:  83     i:  6     Loss:  0.3438904583454132\n",
      "Epoch:  83     i:  7     Loss:  0.36746105551719666\n",
      "Epoch:  83     i:  8     Loss:  0.344764769077301\n",
      "Epoch:  83     i:  9     Loss:  0.4040255546569824\n",
      "Epoch:  83     i:  10     Loss:  0.40159159898757935\n",
      "Epoch:  83     i:  11     Loss:  0.407513827085495\n",
      "Epoch:  83     i:  12     Loss:  0.57966548204422\n",
      "Epoch:  83     i:  13     Loss:  0.6047360897064209\n",
      "Epoch:  83     i:  14     Loss:  0.43179744482040405\n",
      "Epoch:  83     i:  15     Loss:  0.5415614247322083\n",
      "Epoch:  83     i:  16     Loss:  0.3380029499530792\n",
      "Epoch:  83     i:  17     Loss:  0.4129508137702942\n",
      "Epoch:  83     i:  18     Loss:  0.5109926462173462\n",
      "Epoch:  83     i:  19     Loss:  0.4713130593299866\n",
      "Epoch:  83     i:  20     Loss:  0.43009644746780396\n",
      "Epoch:  83     i:  21     Loss:  0.45257166028022766\n",
      "Epoch:  83     i:  22     Loss:  0.37084054946899414\n",
      "Epoch:  83     i:  23     Loss:  0.6091703772544861\n",
      "Epoch:  84     i:  0     Loss:  0.29349738359451294\n",
      "Epoch:  84     i:  1     Loss:  0.4412717819213867\n",
      "Epoch:  84     i:  2     Loss:  0.4831607937812805\n",
      "Epoch:  84     i:  3     Loss:  0.3635607361793518\n",
      "Epoch:  84     i:  4     Loss:  0.3857923150062561\n",
      "Epoch:  84     i:  5     Loss:  0.5125120878219604\n",
      "Epoch:  84     i:  6     Loss:  0.6291666626930237\n",
      "Epoch:  84     i:  7     Loss:  0.39179080724716187\n",
      "Epoch:  84     i:  8     Loss:  0.41917452216148376\n",
      "Epoch:  84     i:  9     Loss:  0.29527518153190613\n",
      "Epoch:  84     i:  10     Loss:  0.4269276261329651\n",
      "Epoch:  84     i:  11     Loss:  0.48448365926742554\n",
      "Epoch:  84     i:  12     Loss:  0.4034004807472229\n",
      "Epoch:  84     i:  13     Loss:  0.35741662979125977\n",
      "Epoch:  84     i:  14     Loss:  0.43310409784317017\n",
      "Epoch:  84     i:  15     Loss:  0.4685193598270416\n",
      "Epoch:  84     i:  16     Loss:  0.6556846499443054\n",
      "Epoch:  84     i:  17     Loss:  0.46902576088905334\n",
      "Epoch:  84     i:  18     Loss:  0.3986937701702118\n",
      "Epoch:  84     i:  19     Loss:  0.38149505853652954\n",
      "Epoch:  84     i:  20     Loss:  0.4727362394332886\n",
      "Epoch:  84     i:  21     Loss:  0.46029022336006165\n",
      "Epoch:  84     i:  22     Loss:  0.5693173408508301\n",
      "Epoch:  84     i:  23     Loss:  0.4048026502132416\n",
      "Epoch:  85     i:  0     Loss:  0.4454783499240875\n",
      "Epoch:  85     i:  1     Loss:  0.31489700078964233\n",
      "Epoch:  85     i:  2     Loss:  0.4348812997341156\n",
      "Epoch:  85     i:  3     Loss:  0.3938564956188202\n",
      "Epoch:  85     i:  4     Loss:  0.3127416670322418\n",
      "Epoch:  85     i:  5     Loss:  0.41981053352355957\n",
      "Epoch:  85     i:  6     Loss:  0.611568808555603\n",
      "Epoch:  85     i:  7     Loss:  0.6565028429031372\n",
      "Epoch:  85     i:  8     Loss:  0.4070177972316742\n",
      "Epoch:  85     i:  9     Loss:  0.3914787173271179\n",
      "Epoch:  85     i:  10     Loss:  0.46538183093070984\n",
      "Epoch:  85     i:  11     Loss:  0.5682440996170044\n",
      "Epoch:  85     i:  12     Loss:  0.4175426959991455\n",
      "Epoch:  85     i:  13     Loss:  0.3938690423965454\n",
      "Epoch:  85     i:  14     Loss:  0.35108470916748047\n",
      "Epoch:  85     i:  15     Loss:  0.3629347085952759\n",
      "Epoch:  85     i:  16     Loss:  0.34044691920280457\n",
      "Epoch:  85     i:  17     Loss:  0.4976029098033905\n",
      "Epoch:  85     i:  18     Loss:  0.3838934004306793\n",
      "Epoch:  85     i:  19     Loss:  0.636174738407135\n",
      "Epoch:  85     i:  20     Loss:  0.4536255896091461\n",
      "Epoch:  85     i:  21     Loss:  0.4575282633304596\n",
      "Epoch:  85     i:  22     Loss:  0.3661404252052307\n",
      "Epoch:  85     i:  23     Loss:  0.5836185216903687\n",
      "Epoch:  86     i:  0     Loss:  0.40144234895706177\n",
      "Epoch:  86     i:  1     Loss:  0.4867517948150635\n",
      "Epoch:  86     i:  2     Loss:  0.5328042507171631\n",
      "Epoch:  86     i:  3     Loss:  0.4651355445384979\n",
      "Epoch:  86     i:  4     Loss:  0.3546718657016754\n",
      "Epoch:  86     i:  5     Loss:  0.33113253116607666\n",
      "Epoch:  86     i:  6     Loss:  0.5308389663696289\n",
      "Epoch:  86     i:  7     Loss:  0.40667518973350525\n",
      "Epoch:  86     i:  8     Loss:  0.555510938167572\n",
      "Epoch:  86     i:  9     Loss:  0.28610965609550476\n",
      "Epoch:  86     i:  10     Loss:  0.5178588032722473\n",
      "Epoch:  86     i:  11     Loss:  0.4285854697227478\n",
      "Epoch:  86     i:  12     Loss:  0.4210987985134125\n",
      "Epoch:  86     i:  13     Loss:  0.3598144054412842\n",
      "Epoch:  86     i:  14     Loss:  0.4950411319732666\n",
      "Epoch:  86     i:  15     Loss:  0.45964857935905457\n",
      "Epoch:  86     i:  16     Loss:  0.45359861850738525\n",
      "Epoch:  86     i:  17     Loss:  0.3933269679546356\n",
      "Epoch:  86     i:  18     Loss:  0.47424930334091187\n",
      "Epoch:  86     i:  19     Loss:  0.5269114971160889\n",
      "Epoch:  86     i:  20     Loss:  0.4128298759460449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  86     i:  21     Loss:  0.41330793499946594\n",
      "Epoch:  86     i:  22     Loss:  0.5214564800262451\n",
      "Epoch:  86     i:  23     Loss:  0.3175463378429413\n",
      "Epoch:  87     i:  0     Loss:  0.39975979924201965\n",
      "Epoch:  87     i:  1     Loss:  0.3759559988975525\n",
      "Epoch:  87     i:  2     Loss:  0.5697813630104065\n",
      "Epoch:  87     i:  3     Loss:  0.6928274631500244\n",
      "Epoch:  87     i:  4     Loss:  0.4048271179199219\n",
      "Epoch:  87     i:  5     Loss:  0.468578577041626\n",
      "Epoch:  87     i:  6     Loss:  0.612680196762085\n",
      "Epoch:  87     i:  7     Loss:  0.3255947530269623\n",
      "Epoch:  87     i:  8     Loss:  0.4074231684207916\n",
      "Epoch:  87     i:  9     Loss:  0.4074345827102661\n",
      "Epoch:  87     i:  10     Loss:  0.506266176700592\n",
      "Epoch:  87     i:  11     Loss:  0.4706370532512665\n",
      "Epoch:  87     i:  12     Loss:  0.3876360058784485\n",
      "Epoch:  87     i:  13     Loss:  0.3220536708831787\n",
      "Epoch:  87     i:  14     Loss:  0.3698047399520874\n",
      "Epoch:  87     i:  15     Loss:  0.5457125306129456\n",
      "Epoch:  87     i:  16     Loss:  0.38937634229660034\n",
      "Epoch:  87     i:  17     Loss:  0.42317408323287964\n",
      "Epoch:  87     i:  18     Loss:  0.48587602376937866\n",
      "Epoch:  87     i:  19     Loss:  0.3738277554512024\n",
      "Epoch:  87     i:  20     Loss:  0.2925376892089844\n",
      "Epoch:  87     i:  21     Loss:  0.6615939140319824\n",
      "Epoch:  87     i:  22     Loss:  0.37711161375045776\n",
      "Epoch:  87     i:  23     Loss:  0.32715511322021484\n",
      "Epoch:  88     i:  0     Loss:  0.516052782535553\n",
      "Epoch:  88     i:  1     Loss:  0.7091498374938965\n",
      "Epoch:  88     i:  2     Loss:  0.36095601320266724\n",
      "Epoch:  88     i:  3     Loss:  0.4756808578968048\n",
      "Epoch:  88     i:  4     Loss:  0.5009915232658386\n",
      "Epoch:  88     i:  5     Loss:  0.3933270573616028\n",
      "Epoch:  88     i:  6     Loss:  0.4635244607925415\n",
      "Epoch:  88     i:  7     Loss:  0.43338075280189514\n",
      "Epoch:  88     i:  8     Loss:  0.5071993470191956\n",
      "Epoch:  88     i:  9     Loss:  0.4774971008300781\n",
      "Epoch:  88     i:  10     Loss:  0.34054869413375854\n",
      "Epoch:  88     i:  11     Loss:  0.3973630964756012\n",
      "Epoch:  88     i:  12     Loss:  0.3650883436203003\n",
      "Epoch:  88     i:  13     Loss:  0.5190457701683044\n",
      "Epoch:  88     i:  14     Loss:  0.5092046856880188\n",
      "Epoch:  88     i:  15     Loss:  0.5034957528114319\n",
      "Epoch:  88     i:  16     Loss:  0.48160606622695923\n",
      "Epoch:  88     i:  17     Loss:  0.33030015230178833\n",
      "Epoch:  88     i:  18     Loss:  0.40953364968299866\n",
      "Epoch:  88     i:  19     Loss:  0.23224881291389465\n",
      "Epoch:  88     i:  20     Loss:  0.3838828206062317\n",
      "Epoch:  88     i:  21     Loss:  0.46254971623420715\n",
      "Epoch:  88     i:  22     Loss:  0.4353998601436615\n",
      "Epoch:  88     i:  23     Loss:  0.37540295720100403\n",
      "Epoch:  89     i:  0     Loss:  0.5288410782814026\n",
      "Epoch:  89     i:  1     Loss:  0.5569559931755066\n",
      "Epoch:  89     i:  2     Loss:  0.3630959093570709\n",
      "Epoch:  89     i:  3     Loss:  0.46114224195480347\n",
      "Epoch:  89     i:  4     Loss:  0.49263179302215576\n",
      "Epoch:  89     i:  5     Loss:  0.5412573218345642\n",
      "Epoch:  89     i:  6     Loss:  0.412112832069397\n",
      "Epoch:  89     i:  7     Loss:  0.4357525706291199\n",
      "Epoch:  89     i:  8     Loss:  0.47884315252304077\n",
      "Epoch:  89     i:  9     Loss:  0.4679138958454132\n",
      "Epoch:  89     i:  10     Loss:  0.36885589361190796\n",
      "Epoch:  89     i:  11     Loss:  0.4391052722930908\n",
      "Epoch:  89     i:  12     Loss:  0.5661190748214722\n",
      "Epoch:  89     i:  13     Loss:  0.36301684379577637\n",
      "Epoch:  89     i:  14     Loss:  0.33524227142333984\n",
      "Epoch:  89     i:  15     Loss:  0.4395191967487335\n",
      "Epoch:  89     i:  16     Loss:  0.45508086681365967\n",
      "Epoch:  89     i:  17     Loss:  0.4802488088607788\n",
      "Epoch:  89     i:  18     Loss:  0.5264003276824951\n",
      "Epoch:  89     i:  19     Loss:  0.37816280126571655\n",
      "Epoch:  89     i:  20     Loss:  0.37213462591171265\n",
      "Epoch:  89     i:  21     Loss:  0.3148092031478882\n",
      "Epoch:  89     i:  22     Loss:  0.2618066072463989\n",
      "Epoch:  89     i:  23     Loss:  0.5862434506416321\n",
      "Epoch:  90     i:  0     Loss:  0.43826937675476074\n",
      "Epoch:  90     i:  1     Loss:  0.4696797728538513\n",
      "Epoch:  90     i:  2     Loss:  0.3861718475818634\n",
      "Epoch:  90     i:  3     Loss:  0.4065837562084198\n",
      "Epoch:  90     i:  4     Loss:  0.29645591974258423\n",
      "Epoch:  90     i:  5     Loss:  0.3557666540145874\n",
      "Epoch:  90     i:  6     Loss:  0.34918180108070374\n",
      "Epoch:  90     i:  7     Loss:  0.24431203305721283\n",
      "Epoch:  90     i:  8     Loss:  0.6166032552719116\n",
      "Epoch:  90     i:  9     Loss:  0.43081751465797424\n",
      "Epoch:  90     i:  10     Loss:  0.35066044330596924\n",
      "Epoch:  90     i:  11     Loss:  0.5101479291915894\n",
      "Epoch:  90     i:  12     Loss:  0.35728925466537476\n",
      "Epoch:  90     i:  13     Loss:  0.4332681894302368\n",
      "Epoch:  90     i:  14     Loss:  0.6763942241668701\n",
      "Epoch:  90     i:  15     Loss:  0.4621924161911011\n",
      "Epoch:  90     i:  16     Loss:  0.38726896047592163\n",
      "Epoch:  90     i:  17     Loss:  0.3977207541465759\n",
      "Epoch:  90     i:  18     Loss:  0.7406616806983948\n",
      "Epoch:  90     i:  19     Loss:  0.3915501832962036\n",
      "Epoch:  90     i:  20     Loss:  0.4799255430698395\n",
      "Epoch:  90     i:  21     Loss:  0.3519444465637207\n",
      "Epoch:  90     i:  22     Loss:  0.5272954702377319\n",
      "Epoch:  90     i:  23     Loss:  0.6300337314605713\n",
      "Epoch:  91     i:  0     Loss:  0.4078994691371918\n",
      "Epoch:  91     i:  1     Loss:  0.39468905329704285\n",
      "Epoch:  91     i:  2     Loss:  0.4767006039619446\n",
      "Epoch:  91     i:  3     Loss:  0.3850450813770294\n",
      "Epoch:  91     i:  4     Loss:  0.5368567705154419\n",
      "Epoch:  91     i:  5     Loss:  0.45275184512138367\n",
      "Epoch:  91     i:  6     Loss:  0.3462759256362915\n",
      "Epoch:  91     i:  7     Loss:  0.4601260721683502\n",
      "Epoch:  91     i:  8     Loss:  0.3993335962295532\n",
      "Epoch:  91     i:  9     Loss:  0.33914634585380554\n",
      "Epoch:  91     i:  10     Loss:  0.5592936873435974\n",
      "Epoch:  91     i:  11     Loss:  0.3342640995979309\n",
      "Epoch:  91     i:  12     Loss:  0.5420965552330017\n",
      "Epoch:  91     i:  13     Loss:  0.537244439125061\n",
      "Epoch:  91     i:  14     Loss:  0.5438030362129211\n",
      "Epoch:  91     i:  15     Loss:  0.47081342339515686\n",
      "Epoch:  91     i:  16     Loss:  0.551658570766449\n",
      "Epoch:  91     i:  17     Loss:  0.27829140424728394\n",
      "Epoch:  91     i:  18     Loss:  0.5770693421363831\n",
      "Epoch:  91     i:  19     Loss:  0.5045144557952881\n",
      "Epoch:  91     i:  20     Loss:  0.3348028063774109\n",
      "Epoch:  91     i:  21     Loss:  0.4192071855068207\n",
      "Epoch:  91     i:  22     Loss:  0.41442492604255676\n",
      "Epoch:  91     i:  23     Loss:  0.3548191785812378\n",
      "Epoch:  92     i:  0     Loss:  0.4287930727005005\n",
      "Epoch:  92     i:  1     Loss:  0.3076136112213135\n",
      "Epoch:  92     i:  2     Loss:  0.44242000579833984\n",
      "Epoch:  92     i:  3     Loss:  0.26292914152145386\n",
      "Epoch:  92     i:  4     Loss:  0.6952478885650635\n",
      "Epoch:  92     i:  5     Loss:  0.2607322335243225\n",
      "Epoch:  92     i:  6     Loss:  0.6404769420623779\n",
      "Epoch:  92     i:  7     Loss:  0.2995114326477051\n",
      "Epoch:  92     i:  8     Loss:  0.3444675803184509\n",
      "Epoch:  92     i:  9     Loss:  0.5935259461402893\n",
      "Epoch:  92     i:  10     Loss:  0.5562102198600769\n",
      "Epoch:  92     i:  11     Loss:  0.4676275849342346\n",
      "Epoch:  92     i:  12     Loss:  0.35391274094581604\n",
      "Epoch:  92     i:  13     Loss:  0.5320879817008972\n",
      "Epoch:  92     i:  14     Loss:  0.5163118839263916\n",
      "Epoch:  92     i:  15     Loss:  0.43074190616607666\n",
      "Epoch:  92     i:  16     Loss:  0.44942688941955566\n",
      "Epoch:  92     i:  17     Loss:  0.4486467242240906\n",
      "Epoch:  92     i:  18     Loss:  0.4456176459789276\n",
      "Epoch:  92     i:  19     Loss:  0.4223960041999817\n",
      "Epoch:  92     i:  20     Loss:  0.4280112087726593\n",
      "Epoch:  92     i:  21     Loss:  0.5432673692703247\n",
      "Epoch:  92     i:  22     Loss:  0.38549619913101196\n",
      "Epoch:  92     i:  23     Loss:  0.3462885618209839\n",
      "Epoch:  93     i:  0     Loss:  0.5444281101226807\n",
      "Epoch:  93     i:  1     Loss:  0.3753202557563782\n",
      "Epoch:  93     i:  2     Loss:  0.3933936357498169\n",
      "Epoch:  93     i:  3     Loss:  0.46228963136672974\n",
      "Epoch:  93     i:  4     Loss:  0.4941493272781372\n",
      "Epoch:  93     i:  5     Loss:  0.6137478947639465\n",
      "Epoch:  93     i:  6     Loss:  0.5439825057983398\n",
      "Epoch:  93     i:  7     Loss:  0.4269573986530304\n",
      "Epoch:  93     i:  8     Loss:  0.5186178088188171\n",
      "Epoch:  93     i:  9     Loss:  0.3060859739780426\n",
      "Epoch:  93     i:  10     Loss:  0.44379550218582153\n",
      "Epoch:  93     i:  11     Loss:  0.3563239574432373\n",
      "Epoch:  93     i:  12     Loss:  0.3804408609867096\n",
      "Epoch:  93     i:  13     Loss:  0.46949467062950134\n",
      "Epoch:  93     i:  14     Loss:  0.42419198155403137\n",
      "Epoch:  93     i:  15     Loss:  0.3465527296066284\n",
      "Epoch:  93     i:  16     Loss:  0.6592056751251221\n",
      "Epoch:  93     i:  17     Loss:  0.509390652179718\n",
      "Epoch:  93     i:  18     Loss:  0.49435511231422424\n",
      "Epoch:  93     i:  19     Loss:  0.3723604679107666\n",
      "Epoch:  93     i:  20     Loss:  0.36108672618865967\n",
      "Epoch:  93     i:  21     Loss:  0.2421465367078781\n",
      "Epoch:  93     i:  22     Loss:  0.4067741632461548\n",
      "Epoch:  93     i:  23     Loss:  0.3945637047290802\n",
      "Epoch:  94     i:  0     Loss:  0.40605810284614563\n",
      "Epoch:  94     i:  1     Loss:  0.334585964679718\n",
      "Epoch:  94     i:  2     Loss:  0.22552531957626343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  94     i:  3     Loss:  0.3278146982192993\n",
      "Epoch:  94     i:  4     Loss:  0.735690712928772\n",
      "Epoch:  94     i:  5     Loss:  0.32985547184944153\n",
      "Epoch:  94     i:  6     Loss:  0.4139881134033203\n",
      "Epoch:  94     i:  7     Loss:  0.4780561327934265\n",
      "Epoch:  94     i:  8     Loss:  0.43190258741378784\n",
      "Epoch:  94     i:  9     Loss:  0.6304870843887329\n",
      "Epoch:  94     i:  10     Loss:  0.3607372045516968\n",
      "Epoch:  94     i:  11     Loss:  0.4084266126155853\n",
      "Epoch:  94     i:  12     Loss:  0.42012104392051697\n",
      "Epoch:  94     i:  13     Loss:  0.41216662526130676\n",
      "Epoch:  94     i:  14     Loss:  0.39847424626350403\n",
      "Epoch:  94     i:  15     Loss:  0.4806106984615326\n",
      "Epoch:  94     i:  16     Loss:  0.6658540964126587\n",
      "Epoch:  94     i:  17     Loss:  0.49421942234039307\n",
      "Epoch:  94     i:  18     Loss:  0.2891433537006378\n",
      "Epoch:  94     i:  19     Loss:  0.4835352897644043\n",
      "Epoch:  94     i:  20     Loss:  0.526509165763855\n",
      "Epoch:  94     i:  21     Loss:  0.547492504119873\n",
      "Epoch:  94     i:  22     Loss:  0.5229116678237915\n",
      "Epoch:  94     i:  23     Loss:  0.28673556447029114\n",
      "Epoch:  95     i:  0     Loss:  0.39089545607566833\n",
      "Epoch:  95     i:  1     Loss:  0.5151191353797913\n",
      "Epoch:  95     i:  2     Loss:  0.44454705715179443\n",
      "Epoch:  95     i:  3     Loss:  0.4525569975376129\n",
      "Epoch:  95     i:  4     Loss:  0.3129758834838867\n",
      "Epoch:  95     i:  5     Loss:  0.33552250266075134\n",
      "Epoch:  95     i:  6     Loss:  0.4374540448188782\n",
      "Epoch:  95     i:  7     Loss:  0.6565576791763306\n",
      "Epoch:  95     i:  8     Loss:  0.4435306191444397\n",
      "Epoch:  95     i:  9     Loss:  0.44530969858169556\n",
      "Epoch:  95     i:  10     Loss:  0.42821580171585083\n",
      "Epoch:  95     i:  11     Loss:  0.5684006810188293\n",
      "Epoch:  95     i:  12     Loss:  0.38108840584754944\n",
      "Epoch:  95     i:  13     Loss:  0.2650473117828369\n",
      "Epoch:  95     i:  14     Loss:  0.5068907141685486\n",
      "Epoch:  95     i:  15     Loss:  0.6063660383224487\n",
      "Epoch:  95     i:  16     Loss:  0.460432231426239\n",
      "Epoch:  95     i:  17     Loss:  0.4973237216472626\n",
      "Epoch:  95     i:  18     Loss:  0.3654279112815857\n",
      "Epoch:  95     i:  19     Loss:  0.3585774004459381\n",
      "Epoch:  95     i:  20     Loss:  0.4204406142234802\n",
      "Epoch:  95     i:  21     Loss:  0.3540356457233429\n",
      "Epoch:  95     i:  22     Loss:  0.462385356426239\n",
      "Epoch:  95     i:  23     Loss:  0.42395147681236267\n",
      "Epoch:  96     i:  0     Loss:  0.40224766731262207\n",
      "Epoch:  96     i:  1     Loss:  0.4481458365917206\n",
      "Epoch:  96     i:  2     Loss:  0.40992307662963867\n",
      "Epoch:  96     i:  3     Loss:  0.44072461128234863\n",
      "Epoch:  96     i:  4     Loss:  0.49916839599609375\n",
      "Epoch:  96     i:  5     Loss:  0.5274646282196045\n",
      "Epoch:  96     i:  6     Loss:  0.3579770028591156\n",
      "Epoch:  96     i:  7     Loss:  0.6367449760437012\n",
      "Epoch:  96     i:  8     Loss:  0.339169442653656\n",
      "Epoch:  96     i:  9     Loss:  0.4784560799598694\n",
      "Epoch:  96     i:  10     Loss:  0.29222121834754944\n",
      "Epoch:  96     i:  11     Loss:  0.37639474868774414\n",
      "Epoch:  96     i:  12     Loss:  0.526261031627655\n",
      "Epoch:  96     i:  13     Loss:  0.5064883232116699\n",
      "Epoch:  96     i:  14     Loss:  0.48347485065460205\n",
      "Epoch:  96     i:  15     Loss:  0.4035400152206421\n",
      "Epoch:  96     i:  16     Loss:  0.41046297550201416\n",
      "Epoch:  96     i:  17     Loss:  0.26340341567993164\n",
      "Epoch:  96     i:  18     Loss:  0.4094330668449402\n",
      "Epoch:  96     i:  19     Loss:  0.39255693554878235\n",
      "Epoch:  96     i:  20     Loss:  0.614365816116333\n",
      "Epoch:  96     i:  21     Loss:  0.33704107999801636\n",
      "Epoch:  96     i:  22     Loss:  0.5090484619140625\n",
      "Epoch:  96     i:  23     Loss:  0.4831319749355316\n",
      "Epoch:  97     i:  0     Loss:  0.42688482999801636\n",
      "Epoch:  97     i:  1     Loss:  0.4548119604587555\n",
      "Epoch:  97     i:  2     Loss:  0.5160634517669678\n",
      "Epoch:  97     i:  3     Loss:  0.46877235174179077\n",
      "Epoch:  97     i:  4     Loss:  0.39721304178237915\n",
      "Epoch:  97     i:  5     Loss:  0.4579867422580719\n",
      "Epoch:  97     i:  6     Loss:  0.5419020652770996\n",
      "Epoch:  97     i:  7     Loss:  0.4100557565689087\n",
      "Epoch:  97     i:  8     Loss:  0.44059988856315613\n",
      "Epoch:  97     i:  9     Loss:  0.4074890613555908\n",
      "Epoch:  97     i:  10     Loss:  0.6532688140869141\n",
      "Epoch:  97     i:  11     Loss:  0.2670462429523468\n",
      "Epoch:  97     i:  12     Loss:  0.3575296401977539\n",
      "Epoch:  97     i:  13     Loss:  0.47255614399909973\n",
      "Epoch:  97     i:  14     Loss:  0.36179691553115845\n",
      "Epoch:  97     i:  15     Loss:  0.3320777714252472\n",
      "Epoch:  97     i:  16     Loss:  0.41956546902656555\n",
      "Epoch:  97     i:  17     Loss:  0.5704998970031738\n",
      "Epoch:  97     i:  18     Loss:  0.38078638911247253\n",
      "Epoch:  97     i:  19     Loss:  0.32734811305999756\n",
      "Epoch:  97     i:  20     Loss:  0.5806921124458313\n",
      "Epoch:  97     i:  21     Loss:  0.4490242600440979\n",
      "Epoch:  97     i:  22     Loss:  0.4620125889778137\n",
      "Epoch:  97     i:  23     Loss:  0.41879820823669434\n",
      "Epoch:  98     i:  0     Loss:  0.3542798161506653\n",
      "Epoch:  98     i:  1     Loss:  0.428239643573761\n",
      "Epoch:  98     i:  2     Loss:  0.393674373626709\n",
      "Epoch:  98     i:  3     Loss:  0.3484928607940674\n",
      "Epoch:  98     i:  4     Loss:  0.4055229425430298\n",
      "Epoch:  98     i:  5     Loss:  0.4144729971885681\n",
      "Epoch:  98     i:  6     Loss:  0.4038386642932892\n",
      "Epoch:  98     i:  7     Loss:  0.3655846118927002\n",
      "Epoch:  98     i:  8     Loss:  0.5929563045501709\n",
      "Epoch:  98     i:  9     Loss:  0.5166494250297546\n",
      "Epoch:  98     i:  10     Loss:  0.5871986150741577\n",
      "Epoch:  98     i:  11     Loss:  0.6967554688453674\n",
      "Epoch:  98     i:  12     Loss:  0.448829710483551\n",
      "Epoch:  98     i:  13     Loss:  0.44778284430503845\n",
      "Epoch:  98     i:  14     Loss:  0.301545649766922\n",
      "Epoch:  98     i:  15     Loss:  0.4015308618545532\n",
      "Epoch:  98     i:  16     Loss:  0.45611265301704407\n",
      "Epoch:  98     i:  17     Loss:  0.4039995074272156\n",
      "Epoch:  98     i:  18     Loss:  0.3070049583911896\n",
      "Epoch:  98     i:  19     Loss:  0.44579339027404785\n",
      "Epoch:  98     i:  20     Loss:  0.4609586000442505\n",
      "Epoch:  98     i:  21     Loss:  0.47988396883010864\n",
      "Epoch:  98     i:  22     Loss:  0.5032986998558044\n",
      "Epoch:  98     i:  23     Loss:  0.395763635635376\n",
      "Epoch:  99     i:  0     Loss:  0.7801283597946167\n",
      "Epoch:  99     i:  1     Loss:  0.3140784204006195\n",
      "Epoch:  99     i:  2     Loss:  0.5496322512626648\n",
      "Epoch:  99     i:  3     Loss:  0.4529511332511902\n",
      "Epoch:  99     i:  4     Loss:  0.5279648900032043\n",
      "Epoch:  99     i:  5     Loss:  0.46696120500564575\n",
      "Epoch:  99     i:  6     Loss:  0.6299800276756287\n",
      "Epoch:  99     i:  7     Loss:  0.46049371361732483\n",
      "Epoch:  99     i:  8     Loss:  0.4264480173587799\n",
      "Epoch:  99     i:  9     Loss:  0.44661828875541687\n",
      "Epoch:  99     i:  10     Loss:  0.26680102944374084\n",
      "Epoch:  99     i:  11     Loss:  0.46211907267570496\n",
      "Epoch:  99     i:  12     Loss:  0.40376853942871094\n",
      "Epoch:  99     i:  13     Loss:  0.5000340938568115\n",
      "Epoch:  99     i:  14     Loss:  0.3479810655117035\n",
      "Epoch:  99     i:  15     Loss:  0.2950522303581238\n",
      "Epoch:  99     i:  16     Loss:  0.3119376003742218\n",
      "Epoch:  99     i:  17     Loss:  0.2824256420135498\n",
      "Epoch:  99     i:  18     Loss:  0.43857645988464355\n",
      "Epoch:  99     i:  19     Loss:  0.3198981285095215\n",
      "Epoch:  99     i:  20     Loss:  0.49222883582115173\n",
      "Epoch:  99     i:  21     Loss:  0.3933992385864258\n",
      "Epoch:  99     i:  22     Loss:  0.4001320004463196\n",
      "Epoch:  99     i:  23     Loss:  0.6628809571266174\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # 1. Prepare data\n",
    "        inputs, labels = data  \n",
    "        # 2. Forward\n",
    "        y_pred = model(inputs)\n",
    "        loss = criterion(y_pred, labels)\n",
    "        print(\"Epoch: \", epoch, \"    i: \", i, \"    Loss: \",loss.item() )\n",
    "        # 3. Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # 4.Update\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**作业：**\n",
    "kaggle入门项目： titannic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 多分类问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../dataset/mnist/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dced938f5cbe4198b20deb494fecb8d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../dataset/mnist/MNIST\\raw\\train-images-idx3-ubyte.gz to ../dataset/mnist/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../dataset/mnist/MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93381cb3dbd1440b925ee341b1c310a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../dataset/mnist/MNIST\\raw\\train-labels-idx1-ubyte.gz to ../dataset/mnist/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../dataset/mnist/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce0deb6a2d0d45219c70613ef6c65e9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../dataset/mnist/MNIST\\raw\\t10k-images-idx3-ubyte.gz to ../dataset/mnist/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../dataset/mnist/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "552b7d94dcec44b6b1e2c7d71f09ac6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../dataset/mnist/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ../dataset/mnist/MNIST\\raw\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda\\envs\\img\\lib\\site-packages\\torchvision\\datasets\\mnist.py:457: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "\n",
      "[1,   300] loss: 2.253\n",
      "[1,   600] loss: 1.048\n",
      "[1,   900] loss: 0.446\n",
      "accuracy on test set: 89 % \n",
      "[2,   300] loss: 0.321\n",
      "[2,   600] loss: 0.274\n",
      "[2,   900] loss: 0.239\n",
      "accuracy on test set: 94 % \n",
      "[3,   300] loss: 0.192\n",
      "[3,   600] loss: 0.171\n",
      "[3,   900] loss: 0.159\n",
      "accuracy on test set: 95 % \n",
      "[4,   300] loss: 0.131\n",
      "[4,   600] loss: 0.126\n",
      "[4,   900] loss: 0.114\n",
      "accuracy on test set: 96 % \n",
      "[5,   300] loss: 0.103\n",
      "[5,   600] loss: 0.095\n",
      "[5,   900] loss: 0.091\n",
      "accuracy on test set: 96 % \n",
      "[6,   300] loss: 0.075\n",
      "[6,   600] loss: 0.078\n",
      "[6,   900] loss: 0.078\n",
      "accuracy on test set: 96 % \n",
      "[7,   300] loss: 0.061\n",
      "[7,   600] loss: 0.062\n",
      "[7,   900] loss: 0.065\n",
      "accuracy on test set: 97 % \n",
      "[8,   300] loss: 0.046\n",
      "[8,   600] loss: 0.054\n",
      "[8,   900] loss: 0.053\n",
      "accuracy on test set: 97 % \n",
      "[9,   300] loss: 0.043\n",
      "[9,   600] loss: 0.041\n",
      "[9,   900] loss: 0.040\n",
      "accuracy on test set: 97 % \n",
      "[10,   300] loss: 0.033\n",
      "[10,   600] loss: 0.034\n",
      "[10,   900] loss: 0.035\n",
      "accuracy on test set: 97 % \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    " \n",
    "# prepare dataset\n",
    " \n",
    "batch_size = 64\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]) # 归一化,均值和方差\n",
    " \n",
    "train_dataset = datasets.MNIST(root='../dataset/mnist/', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "test_dataset = datasets.MNIST(root='../dataset/mnist/', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n",
    " \n",
    "# design model using class\n",
    " \n",
    " \n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(784, 512)\n",
    "        self.l2 = torch.nn.Linear(512, 256)\n",
    "        self.l3 = torch.nn.Linear(256, 128)\n",
    "        self.l4 = torch.nn.Linear(128, 64)\n",
    "        self.l5 = torch.nn.Linear(64, 10)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)  # -1其实就是自动获取mini_batch\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.relu(self.l3(x))\n",
    "        x = F.relu(self.l4(x))\n",
    "        return self.l5(x)  # 最后一层不做激活，不进行非线性变换\n",
    " \n",
    " \n",
    "model = Net()\n",
    " \n",
    "# construct loss and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    " \n",
    "# training cycle forward, backward, update\n",
    " \n",
    " \n",
    "def train(epoch):\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, data in enumerate(train_loader, 0):\n",
    "        inputs, target = data\n",
    "        optimizer.zero_grad()\n",
    " \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    " \n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 300 == 299:\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch+1, batch_idx+1, running_loss/300))\n",
    "            running_loss = 0.0\n",
    " \n",
    " \n",
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, dim=1) # dim = 1 列是第0个维度，行是第1个维度\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item() # 张量之间的比较运算\n",
    "    print('accuracy on test set: %d %% ' % (100*correct/total))\n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    "    for epoch in range(10):\n",
    "        train(epoch)\n",
    "        test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.卷积神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   300] loss: 0.610\n",
      "[1,   600] loss: 0.178\n",
      "[1,   900] loss: 0.130\n",
      "accuracy on test set: 96 % \n",
      "[2,   300] loss: 0.105\n",
      "[2,   600] loss: 0.096\n",
      "[2,   900] loss: 0.085\n",
      "accuracy on test set: 98 % \n",
      "[3,   300] loss: 0.075\n",
      "[3,   600] loss: 0.071\n",
      "[3,   900] loss: 0.072\n",
      "accuracy on test set: 98 % \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-84370beeca26>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m         \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m         \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-53-84370beeca26>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda\\envs\\img\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-53-84370beeca26>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m# flatten data from (n,1,28,28) to (n, 784)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpooling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpooling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# -1 此处自动算出的是320\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda\\envs\\img\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda\\envs\\img\\lib\\site-packages\\torch\\nn\\modules\\pooling.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    153\u001b[0m         return F.max_pool2d(input, self.kernel_size, self.stride,\n\u001b[0;32m    154\u001b[0m                             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mceil_mode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m                             self.return_indices)\n\u001b[0m\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda\\envs\\img\\lib\\site-packages\\torch\\_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    265\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda\\envs\\img\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36m_max_pool2d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    584\u001b[0m         \u001b[0mstride\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m     return torch.max_pool2d(\n\u001b[1;32m--> 586\u001b[1;33m         input, kernel_size, stride, padding, dilation, ceil_mode)\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m max_pool2d = boolean_dispatch(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim \n",
    "\n",
    "# prepare dataset\n",
    "\n",
    "batch_size = 64\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.137,), (0.381,))])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='../dataset/mnist/', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "test_dataset = datasets.MNIST(root='../dataset/mnist/', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "# design model\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = torch.nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.pooling = torch.nn.MaxPool2d(2)\n",
    "        self.fc = torch.nn.Linear(320, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        # torch.Size([64, 1, 28, 28])\n",
    "        x = F.relu(self.pooling(self.conv1(x)))\n",
    "        # torch.Size([64, 10, 12, 12])\n",
    "        x = F.relu(self.pooling(self.conv2(x)))\n",
    "        # torch.Size([64, 20, 4, 4])\n",
    "        x = x.view(batch_size, -1)\n",
    "        # torch.Size([64, 320])\n",
    "        x = self.fc(x)\n",
    "        # torch.Size([64, 10])\n",
    "\n",
    "        return x\n",
    "\n",
    "class NiuNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NiuNet, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 20, kernel_size=5)\n",
    "        self.conv2 = torch.nn.Conv2d(20, 30, kernel_size=3)\n",
    "        self.conv3 = torch.nn.Conv2d(30, 10, kernel_size=3)\n",
    "        self.pooling = torch.nn.MaxPool2d(2)\n",
    "        self.fc = torch.nn.Linear(10, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = F.relu(self.pooling(self.conv1(x)))\n",
    "        x = F.relu(self.pooling(self.conv2(x)))\n",
    "        x = F.relu(self.pooling(self.conv3(x)))\n",
    "        x = x.view(batch_size, -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "model = Net().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "def train(epoch):\n",
    "    running_loss = 0\n",
    "    for batch_idx, data in enumerate(train_loader, 0):\n",
    "        inputs, target = data\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 300 == 299:\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch+1, batch_idx+1, running_loss/300))\n",
    "            running_loss = 0.0\n",
    "\n",
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, dim=1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(\"accuracy on test set : %d  %% \" % (100 * correct / total))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for epoch in range(10):\n",
    "        train(epoch)\n",
    "        test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "河工大深度学习课程.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
